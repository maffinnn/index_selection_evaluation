{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare to load table\n",
      "Loading table done\n",
      "Reading column names\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "from selection.index_selection_evaluation import DBMSYSTEMS\n",
    "from selection.query_generator import QueryGenerator\n",
    "from selection.table_generator import TableGenerator\n",
    "from selection.what_if_index_creation import WhatIfIndexCreation\n",
    "from selection.cost_evaluation import CostEvaluation\n",
    "from selection.workload import Workload\n",
    "from selection.index import Index, index_merge\n",
    "from selection.candidate_generation import syntactically_relevant_indexes, candidates_per_query\n",
    "from selection.utils import get_utilized_indexes, indexes_by_table\n",
    "\n",
    "        \n",
    "config_file = \"config.json\"\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)\n",
    "dbms_class = DBMSYSTEMS[config[\"database_system\"]]\n",
    "generating_connector = dbms_class(None, autocommit=True)\n",
    "table_generator = TableGenerator(config[\"benchmark_name\"], config[\"scale_factor\"], generating_connector)\n",
    "database_name = table_generator.database_name()\n",
    "database_system = config[\"database_system\"]\n",
    "db_connector = DBMSYSTEMS[database_system](database_name)\n",
    "query_generator = QueryGenerator(\n",
    "    config[\"benchmark_name\"],\n",
    "    config[\"scale_factor\"],\n",
    "    db_connector,\n",
    "    config[\"queries\"],\n",
    "    table_generator.columns,\n",
    ")\n",
    "workload = Workload(query_generator.queries)\n",
    "cost_evaluation = CostEvaluation(db_connector, cost_estimation=\"actual_runtimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dsb data\n",
      "Files generated: ['customer_address.dat', 'income_band.dat', 'date_dim.dat', 'warehouse.dat', 'call_center.dat', 'dbgen_version.dat', 'catalog_sales.dat', 'web_returns.dat', 'promotion.dat', 'web_site.dat', 'store_returns.dat', 'web_sales.dat', 'store_sales.dat', 'customer_demographics.dat', 'store.dat', 'customer.dat', 'catalog_page.dat', 'time_dim.dat', 'inventory.dat', 'household_demographics.dat', 'catalog_returns.dat', 'web_page.dat', 'ship_mode.dat', 'item.dat', 'reason.dat']\n"
     ]
    }
   ],
   "source": [
    "# table_generator._generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_files = ['catalog_page.dat']\n",
    "# import os\n",
    "# from io import StringIO  \n",
    "\n",
    "# print(\"Loading data into the tables\")\n",
    "# for filename in table_files:\n",
    "#     print(\"    Loading file {}\".format(filename))\n",
    "#     table = filename.replace(\".tbl\", \"\").replace(\".dat\", \"\")\n",
    "#     path = table_generator.directory + \"/\" + filename\n",
    "#     print(f\"    Import data of path {path}\")\n",
    "#     with open(path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         for line in lines:\n",
    "#             to_write = line.rstrip('\\n')[:-1]\n",
    "#             f = StringIO(to_write+'\\n')\n",
    "#             db_connector._cursor.copy_from(f, table, sep=\"|\", null=\"\")\n",
    "#     db_connector.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate candidates\n",
      "Potential indexes: 55\n",
      "Potential indexes: 55\n",
      "Potential indexes: 55\n",
      "Potential indexes: 120\n",
      "Potential indexes: 120\n",
      "Potential indexes: 120\n",
      "Potential indexes: 175\n",
      "Potential indexes: 175\n",
      "Potential indexes: 175\n",
      "Potential indexes: 110\n",
      "Potential indexes: 110\n",
      "Potential indexes: 110\n",
      "Potential indexes: 62\n",
      "Potential indexes: 62\n",
      "Potential indexes: 62\n",
      "Potential indexes: 111\n",
      "Potential indexes: 111\n",
      "Potential indexes: 111\n",
      "Potential indexes: 97\n",
      "Potential indexes: 97\n",
      "Potential indexes: 97\n",
      "Potential indexes: 67\n",
      "Potential indexes: 67\n",
      "Potential indexes: 67\n",
      "Potential indexes: 81\n",
      "Potential indexes: 81\n",
      "Potential indexes: 81\n",
      "Potential indexes: 84\n",
      "Potential indexes: 84\n",
      "Potential indexes: 84\n",
      "Potential indexes: 92\n",
      "Potential indexes: 92\n",
      "Potential indexes: 92\n",
      "Potential indexes: 57\n",
      "Potential indexes: 57\n",
      "Potential indexes: 57\n",
      "Potential indexes: 104\n",
      "Potential indexes: 104\n",
      "Potential indexes: 104\n",
      "Potential indexes: 101\n",
      "Potential indexes: 101\n",
      "Potential indexes: 101\n",
      "Potential indexes: 159\n",
      "Potential indexes: 159\n",
      "Potential indexes: 159\n",
      "Potential indexes: 201\n",
      "Potential indexes: 201\n",
      "Potential indexes: 201\n",
      "Potential indexes: 205\n",
      "Potential indexes: 205\n",
      "Potential indexes: 205\n",
      "Potential indexes: 79\n",
      "Potential indexes: 79\n",
      "Potential indexes: 79\n",
      "Potential indexes: 313\n",
      "Potential indexes: 313\n",
      "Potential indexes: 313\n",
      "Potential indexes: 73\n",
      "Potential indexes: 73\n",
      "Potential indexes: 73\n",
      "Potential indexes: 263\n",
      "Potential indexes: 263\n",
      "Potential indexes: 263\n",
      "Potential indexes: 144\n",
      "Potential indexes: 144\n",
      "Potential indexes: 144\n",
      "Potential indexes: 91\n",
      "Potential indexes: 91\n",
      "Potential indexes: 91\n",
      "Potential indexes: 144\n",
      "Potential indexes: 144\n",
      "Potential indexes: 144\n",
      "Potential indexes: 56\n",
      "Potential indexes: 56\n",
      "Potential indexes: 56\n",
      "Potential indexes: 145\n",
      "Potential indexes: 145\n",
      "Potential indexes: 145\n",
      "Potential indexes: 102\n",
      "Potential indexes: 102\n",
      "Potential indexes: 102\n",
      "Potential indexes: 73\n",
      "Potential indexes: 73\n",
      "Potential indexes: 73\n",
      "Potential indexes: 82\n",
      "Potential indexes: 82\n",
      "Potential indexes: 82\n",
      "Potential indexes: 94\n",
      "Potential indexes: 94\n",
      "Potential indexes: 94\n",
      "Potential indexes: 45\n",
      "Potential indexes: 45\n",
      "Potential indexes: 45\n",
      "Potential indexes: 45\n",
      "Potential indexes: 45\n",
      "Potential indexes: 45\n",
      "Potential indexes: 178\n",
      "Potential indexes: 178\n",
      "Potential indexes: 178\n",
      "Potential indexes: 318\n",
      "Potential indexes: 318\n",
      "Potential indexes: 318\n",
      "Potential indexes: 301\n",
      "Potential indexes: 301\n",
      "Potential indexes: 301\n",
      "Potential indexes: 132\n",
      "Potential indexes: 132\n",
      "Potential indexes: 132\n",
      "Potential indexes: 77\n",
      "Potential indexes: 77\n",
      "Potential indexes: 77\n"
     ]
    }
   ],
   "source": [
    "from selection.index_selection_evaluation import IndexSelection\n",
    "import time\n",
    "\n",
    "# index_selection = IndexSelection()\n",
    "# with open(config_file) as f:\n",
    "#     config = json.load(f)\n",
    "# index_selection._setup_config(config)\n",
    "db_connector.drop_indexes()\n",
    "\n",
    "# Set the random seed to obtain deterministic statistics (and cost estimations)\n",
    "# because ANALYZE (and alike) use sampling for large tables\n",
    "# index_selection.db_connector.create_statistics()\n",
    "# index_selection.db_connector.commit()\n",
    "candidates = candidates_per_query(workload,2,candidate_generator=syntactically_relevant_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(workload.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [I(C customer.c_current_addr_sk,C customer.c_current_cdemo_sk)],\n",
       " [I(C income_band.ib_income_band_sk), I(C household_demographics.hd_demo_sk)],\n",
       " [I(C customer.c_last_name,C customer.c_first_name),\n",
       "  I(C customer.c_customer_id,C customer.c_first_name),\n",
       "  I(C customer_address.ca_city)],\n",
       " [I(C customer.c_current_cdemo_sk,C customer.c_current_hdemo_sk),\n",
       "  I(C income_band.ib_upper_bound),\n",
       "  I(C income_band.ib_upper_bound,C income_band.ib_lower_bound),\n",
       "  I(C customer.c_customer_id,C customer.c_first_name)]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def sample_candidates(candidates_per_query, max_config_width):\n",
    "    candidates = [[]]\n",
    "    for width in range(1, max_config_width+1):\n",
    "        possible_candidates = random.sample(candidates_per_query, width)\n",
    "        if width == 1:\n",
    "            candidates.append(possible_candidates)\n",
    "            continue\n",
    "        else:\n",
    "            # check if a config contains same column index\n",
    "            column_check = set()\n",
    "            for candidate in possible_candidates:\n",
    "                if column_check & set(candidate.columns): possible_candidates.remove(candidate)\n",
    "                else: column_check |= set(candidate.columns)\n",
    "            # keep sample until reaches the width\n",
    "            while len(possible_candidates) < width:\n",
    "                candidate = random.sample(candidates_per_query, 1)[0]\n",
    "                if column_check & set(candidate.columns): continue\n",
    "                else: \n",
    "                    column_check |= set(candidate.columns)\n",
    "                    possible_candidates.append(candidate)\n",
    "        candidates.append(possible_candidates)\n",
    "    return candidates\n",
    "        \n",
    "sample_candidates(candidates[0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no: 48: \n",
      "\tnow running the 0th run\n",
      "\tnow running the 1th run\n",
      "\tnow running the 2th run\n",
      "\tnow running the 3th run\n",
      "\tnow running the 0th run\n",
      "\tnow running the 1th run\n",
      "\tnow running the 2th run\n"
     ]
    }
   ],
   "source": [
    "number_of_actual_runs = 4\n",
    "\n",
    "data = []\n",
    "filename = \"../data/DSB/dsb.csv\"\n",
    "iter = 48\n",
    "for  query, candidates_per_query in zip(workload.queries[iter:], candidates[iter:]):\n",
    "    print(f\"iteration no: {iter}: \")\n",
    "    entry = [[query.nr, query.text]]\n",
    "    index_configs_per_query = sample_candidates(candidates_per_query, 4)\n",
    "    formatted_index_configs_per_query = []\n",
    "    average_execution_times_per_index_config, execution_time_list_per_config, query_plans_per_index_config = [], [], []\n",
    "    for index_config in index_configs_per_query:\n",
    "        if len(index_config) == 0: formatted_index_configs_per_query.append([])\n",
    "        elif len(index_config) == 1: formatted_index_configs_per_query.append(index_config[0])\n",
    "        else: formatted_index_configs_per_query.append(tuple(index_config))\n",
    "        cost_evaluation._prepare_cost_calculation(index_config)\n",
    "        execution_time_list = []\n",
    "        for i in range(number_of_actual_runs):\n",
    "            print(f\"\\tnow running the {i}th run\")\n",
    "            actual_execu_time, plan = db_connector.exec_query(query)\n",
    "            execution_time_list.append(actual_execu_time)\n",
    "        average_execution_time = sum(execution_time_list)/len(execution_time_list)\n",
    "        average_execution_times_per_index_config.append(average_execution_time)\n",
    "        execution_time_list_per_config.append(execution_time_list[1:])\n",
    "        query_plans_per_index_config.append(plan)\n",
    "        cost_evaluation.complete_cost_estimation()\n",
    "    entry.append(formatted_index_configs_per_query)\n",
    "    entry.append(average_execution_times_per_index_config)\n",
    "    entry.append(query_plans_per_index_config)\n",
    "    entry.append(execution_time_list_per_config)\n",
    "    data.append(entry)\n",
    "    with open(filename, \"a+\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(entry)\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siyuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
