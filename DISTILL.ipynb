{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def json_dump(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "from selection.index_selection_evaluation import DBMSYSTEMS\n",
    "from selection.query_generator import QueryGenerator\n",
    "from selection.table_generator import TableGenerator\n",
    "from selection.what_if_index_creation import WhatIfIndexCreation\n",
    "from selection.workload import Workload\n",
    "\n",
    "config_file = \"config.json\"\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server on socket \"/tmp/.s.PGSQL.5432\" failed: No such file or directory\n\tIs the server running locally and accepting connections on that socket?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dbms_class \u001b[38;5;241m=\u001b[39m DBMSYSTEMS[config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase_system\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m----> 2\u001b[0m generating_connector \u001b[38;5;241m=\u001b[39m \u001b[43mdbms_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/dbms/postgres_dbms.py:17\u001b[0m, in \u001b[0;36mPostgresDatabaseConnector.__init__\u001b[0;34m(self, db_name, autocommit)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_name:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_random_seed()\n\u001b[1;32m     21\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPostgres connector created: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(db_name))\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/dbms/postgres_dbms.py:26\u001b[0m, in \u001b[0;36mPostgresDatabaseConnector.create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbname=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server on socket \"/tmp/.s.PGSQL.5432\" failed: No such file or directory\n\tIs the server running locally and accepting connections on that socket?\n"
     ]
    }
   ],
   "source": [
    "dbms_class = DBMSYSTEMS[config[\"database_system\"]]\n",
    "generating_connector = dbms_class(None, autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_generator = TableGenerator(\n",
    "    config[\"benchmark_name\"], config[\"scale_factor\"], generating_connector\n",
    ")\n",
    "database_name = table_generator.database_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_system = config[\"database_system\"]\n",
    "db_connector = DBMSYSTEMS[database_system](database_name)\n",
    "what_if = WhatIfIndexCreation(db_connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_connector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query_generator \u001b[38;5;241m=\u001b[39m QueryGenerator(\n\u001b[1;32m      2\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmark_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mdb_connector\u001b[49m,\n\u001b[1;32m      5\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     table_generator\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_connector' is not defined"
     ]
    }
   ],
   "source": [
    "query_generator = QueryGenerator(\n",
    "    config[\"benchmark_name\"],\n",
    "    config[\"scale_factor\"],\n",
    "    db_connector,\n",
    "    config[\"queries\"],\n",
    "    table_generator.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload = Workload(query_generator.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16730\n"
     ]
    }
   ],
   "source": [
    "from selection.candidate_generation import syntactically_relevant_indexes\n",
    "all_syntactically_relevant_indexes, query_index_pairs = set(), []\n",
    "for query in workload.queries:\n",
    "    indexes = syntactically_relevant_indexes(query, len(query.columns))\n",
    "    all_syntactically_relevant_indexes.update(indexes)\n",
    "    query_index_pairs.extend([(query, index) for index in indexes])\n",
    "print(len(query_index_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection.index_selection_evaluation import IndexSelection \n",
    "\n",
    "workload_cost = None\n",
    "index_selection = IndexSelection()\n",
    "index_selection._run_algorithms(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46075564.57\n"
     ]
    }
   ],
   "source": [
    "csv_path= f\"benchmark_results/results_no_index_tpch_19_queries.csv\"\n",
    "no_index_df = pd.read_csv(csv_path, sep=';')\n",
    "workload_cost_no_index = 0\n",
    "for _, v in no_index_df.loc[0, \"q1\": \"q22\"].to_dict().items():\n",
    "    workload_cost_no_index += float(json.loads(v)[\"Cost\"])\n",
    "print(workload_cost_no_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection.cost_evaluation import CostEvaluation\n",
    "cost_evaluation = CostEvaluation(db_connector)\n",
    "\n",
    "# workload_cost_with_index = []\n",
    "# for index in all_syntactically_relevant_indexes:\n",
    "#     cost = cost_evaluation.calculate_cost(workload, set([index]))\n",
    "#     workload_cost_with_index.append(round(cost, 2))\n",
    "#     cost_evaluation._unsimulate_or_drop_index(index)\n",
    "\n",
    "# print(workload_cost_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHYISCAL_TO_LOGICAL_OPERATOR_MAP = {\n",
    "    \"Seq Scan\": \"Scan\",\n",
    "    \"Bitmap Index Scan\": \"Scan\",\n",
    "    \"Bitmap Heap Scan\": \"Scan\",\n",
    "    \"Index Scan\": \"Scan\",\n",
    "    \"Index Only Scan\": \"Scan\",\n",
    "    \"Sort\": \"Sort\",\n",
    "    \"Hash Join\": \"Join\",\n",
    "    \"Merge Join\": \"Join\",\n",
    "    \"Nested Loop\": \"Join\",\n",
    "    \"Aggregate\": \"Aggregate\",\n",
    "    \"Gather Merge\": \"\",\n",
    "    \"Gather\": \"\",\n",
    "    \"BitmapOr\": \"\",\n",
    "    \"Limit\": \"\",\n",
    "    \"Hash\": \"\",\n",
    "}\n",
    "\n",
    "LOGICAL_OPERATORS = [\"Scan\", \"Join\", \"Aggregate\", \"Sort\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedFunction",
     "evalue": "function hypopg_create_index(unknown) does not exist\nLINE 1: select * from hypopg_create_index( 'create index on lineitem...\n                      ^\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedFunction\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m indexes \u001b[38;5;241m=\u001b[39m syntactically_relevant_indexes(query, \u001b[38;5;28mlen\u001b[39m(query\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mwhat_if\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     indexed_query_plan \u001b[38;5;241m=\u001b[39m db_connector\u001b[38;5;241m.\u001b[39mget_plan_with_statistics(query)\n\u001b[1;32m     12\u001b[0m     indexed_query_cost \u001b[38;5;241m=\u001b[39m db_connector\u001b[38;5;241m.\u001b[39mget_cost(query)\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/what_if_index_creation.py:16\u001b[0m, in \u001b[0;36mWhatIfIndexCreation.simulate_index\u001b[0;34m(self, potential_index, store_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, potential_index, store_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpotential_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     index_oid \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m     index_name \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/database_connector.py:58\u001b[0m, in \u001b[0;36mDatabaseConnector.simulate_index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulated_indexes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 58\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_simulation_duration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/dbms/postgres_dbms.py:118\u001b[0m, in \u001b[0;36mPostgresDatabaseConnector._simulate_index\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    112\u001b[0m table_name \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mtable()\n\u001b[1;32m    113\u001b[0m statement \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from hypopg_create_index( \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate index on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mjoined_column_names()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 118\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Dev/fyp/index_selection_evaluation/selection/database_connector.py:22\u001b[0m, in \u001b[0;36mDatabaseConnector.exec_fetch\u001b[0;34m(self, statement, one)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexec_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, statement, one\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m one:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mfetchone()\n",
      "\u001b[0;31mUndefinedFunction\u001b[0m: function hypopg_create_index(unknown) does not exist\nLINE 1: select * from hypopg_create_index( 'create index on lineitem...\n                      ^\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\n"
     ]
    }
   ],
   "source": [
    "query_plans_with_index, query_costs_with_index = {}, {}\n",
    "query_plans_with_index_dump = []\n",
    "for query in workload.queries:\n",
    "    query_plan = db_connector.get_plan_with_statistics(query)\n",
    "    query_plans_with_index[(query, None)] = query_plan\n",
    "    query_costs_with_index[(query, None)] = db_connector.get_cost(query)\n",
    "    query_plans_with_index_dump.append(query_plan)\n",
    "    indexes = syntactically_relevant_indexes(query, len(query.columns))\n",
    "    for index in indexes:\n",
    "        what_if.simulate_index(index)\n",
    "        indexed_query_plan = db_connector.get_plan_with_statistics(query)\n",
    "        indexed_query_cost = db_connector.get_cost(query)\n",
    "        what_if.drop_simulated_index(index)\n",
    "        query_plans_with_index_dump.append(indexed_query_plan)\n",
    "        query_plans_with_index[(query,index)] = indexed_query_plan\n",
    "        query_costs_with_index[(query,index)] = indexed_query_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(\"indexed_query_plans.json\", query_plans_with_index_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cardinatlity statistics\n",
    "for table in table_generator.tables:\n",
    "    row_count = db_connector.table_row_count(table.name)\n",
    "    table.set_row_count(row_count)\n",
    "    for column in table.columns:\n",
    "        card = db_connector.get_column_cardinality(column)\n",
    "        column.set_cardinality(-card * row_count if card < 0 else card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_filtering_property(query_plan):\n",
    "    if \"Filter\" in query_plan.keys():\n",
    "        return query_plan[\"Filter\"]\n",
    "    if \"Hash Cond\" in query_plan.keys():\n",
    "        return query_plan[\"Hash Cond\"]\n",
    "    if \"Join Filter\" in query_plan.keys():\n",
    "        return query_plan[\"Join Filter\"]\n",
    "    return \"\"\n",
    "\n",
    "def has_child_node(query_plan):\n",
    "    return \"Plans\" in query_plan.keys()\n",
    "\n",
    "def is_join_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Join\"\n",
    "\n",
    "def is_sort_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Sort\"\n",
    "\n",
    "def is_aggregate_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Aggregate\"\n",
    "\n",
    "def is_scan_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Scan\"\n",
    "\n",
    "def check_indexed_column_in_condition(index, condition):\n",
    "    for column in index.columns:\n",
    "        if column.name in condition:\n",
    "            return True\n",
    "\n",
    "def get_table_from_plan_node(query_plan):\n",
    "    table = \"\"\n",
    "    if \"Relation Name\" in query_plan.keys():\n",
    "        table = query_plan[\"Relation Name\"]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 1\n",
    "def estimate_index_utility_recursive(index, original_query_plan, indexed_query_plan):\n",
    "    total_cost = 0\n",
    "    if has_child_node(original_query_plan):\n",
    "        for original_child_node, indexed_child_node in zip(indexed_query_plan[\"Plans\"], indexed_query_plan[\"Plans\"]):\n",
    "            total_cost += estimate_index_utility_recursive(index, original_child_node, indexed_child_node)\n",
    "    current_operator = indexed_query_plan[\"Node Type\"]\n",
    "    current_cost = original_query_plan[\"Total Cost\"]\n",
    "    if (condition := has_filtering_property(indexed_query_plan)) != \"\":\n",
    "        if is_join_operator(current_operator):\n",
    "            join_output_rows = indexed_query_plan[\"Plan Rows\"]\n",
    "            left_input_rows = indexed_query_plan[\"Plans\"][0][\"Plan Rows\"]\n",
    "            right_input_rows = indexed_query_plan[\"Plans\"][1][\"Plan Rows\"]\n",
    "            if check_indexed_column_in_condition(index, condition):    \n",
    "                current_cost = (1-np.sqrt(join_output_rows/(left_input_rows*right_input_rows)))*original_query_plan[\"Total Cost\"]\n",
    "        else:\n",
    "            selectivities = [indexed_query_plan[\"Plan Rows\"]/column.table.row_count for column in index.columns if column.name in condition]\n",
    "            average_selectivity = sum(selectivities)/len(selectivities) if len(selectivities) > 0 else 0\n",
    "            current_cost = (1-average_selectivity)*original_query_plan[\"Total Cost\"]\n",
    "    elif is_sort_operator(current_operator):\n",
    "        sort_conditions = indexed_query_plan[\"Sort Key\"]\n",
    "        for sort_condition in sort_conditions:\n",
    "            if check_indexed_column_in_condition(index, sort_condition):\n",
    "                current_cost = indexed_query_plan[\"Total Cost\"]\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        group_conditions = query_plan[\"Group Key\"]\n",
    "        for group_condition in group_conditions:\n",
    "            if check_indexed_column_in_condition(index, group_condition):\n",
    "                current_cost = indexed_query_plan[\"Total Cost\"]\n",
    "    return total_cost+current_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3077099684.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 34\u001b[0;36m\u001b[0m\n\u001b[0;31m    return pd.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# signal 2\n",
    "def extract_shape_of_query_and_index(index, original_query_plan):\n",
    "    query_shape, index_shape = {}, []\n",
    "    _extract_query_shape(query_shape, original_query_plan)\n",
    "    _extract_index_shape(index_shape, index, original_query_plan)\n",
    "    return query_shape, index_shape\n",
    "\n",
    "\n",
    "def _extract_query_shape(query_shape, query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    logical_operator = PHYISCAL_TO_LOGICAL_OPERATOR_MAP[current_operator]\n",
    "    if is_scan_operator(current_operator):\n",
    "        table = get_table_from_plan_node(query_plan)\n",
    "        if table in query_shape.keys():\n",
    "            query_shape[table].append(logical_operator)\n",
    "        else:\n",
    "            query_shape[table] = [logical_operator]\n",
    "        return table\n",
    "    \n",
    "    tables = []    \n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            table = _extract_query_shape(query_shape, child_node)\n",
    "            if table and logical_operator:\n",
    "                tables.append(table)\n",
    "                query_shape[table].append(logical_operator)\n",
    "    return tables[0] if 0<len(tables)<2 else \"\"\n",
    "    \n",
    "    \n",
    "def convert_query_shape_to_df(query_shape):\n",
    "    data = []\n",
    "    table_names = []\n",
    "    for table_name, shape in query_shape.values():\n",
    "        data.append(shape)\n",
    "        table_names.append(table_name)\n",
    "    return pd.DataFrame(data, index = table_names)\n",
    "\n",
    "    \n",
    "def _extract_index_shape(index_shape, index, query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    logical_operator = PHYISCAL_TO_LOGICAL_OPERATOR_MAP[current_operator]\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _extract_index_shape(index_shape, index, child_node)\n",
    "            \n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        if check_indexed_column_in_condition(index, condition):\n",
    "            index_shape.append(logical_operator)\n",
    "    elif is_sort_operator(current_operator):\n",
    "        sort_conditions = query_plan[\"Sort Key\"]\n",
    "        for sort_condition in sort_conditions:\n",
    "            if check_indexed_column_in_condition(index, sort_condition):\n",
    "                index_shape.append(logical_operator)\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        aggregate_conditions = query_plan[\"Group Key\"]\n",
    "        for aggregate_condition in aggregate_conditions:\n",
    "            if check_indexed_column_in_condition(index, aggregate_condition):\n",
    "                index_shape.append(logical_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 3\n",
    "def evaluate_operator_relevance(operator_relevance, index, query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    relevance = 0\n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        selectivities = [query_plan[\"Plan Rows\"]/column.table.row_count for column in index.columns if column.name in condition]\n",
    "        relevance = sum(selectivities)/len(selectivities)\n",
    "    elif is_sort_operator(current_operator) and \"Sort Key\" in query_plan:\n",
    "        densities = []\n",
    "        conditions = query_plan[\"Sort Key\"]\n",
    "        for condition in conditions:\n",
    "            for column in index.columns:\n",
    "                if column.name in condition:\n",
    "                    densities.append(column.cardinality/column.table.row_count)\n",
    "        relevance = sum(densities)/len(densities) if len(densities) > 0 else 0\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        densities = []\n",
    "        conditions = query_plan[\"Group Key\"]\n",
    "        for condition in conditions:\n",
    "            for column in index.columns:\n",
    "                if column.name in condition:\n",
    "                    densities.append(column.cardinality/column.table.row_count)\n",
    "        relevance = sum(densities)/len(densities) if len(densities) > 0 else 0\n",
    "    if current_operator not in operator_relevance: \n",
    "        operator_relevance[current_operator] = []\n",
    "    operator_relevance[current_operator].append(relevance)\n",
    "    \n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            evaluate_operator_relevance[child_node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 4\n",
    "def get_number_of_pages(query_plan):\n",
    "    return query_plan[\"Shared Hit Blocks\"] + query_plan[\"Shared Read Blocks\"] + query_plan[\"Local Hit Blocks\"] + query_plan[\"Local Read Blocks\"]\n",
    "\n",
    "\n",
    "# seems not supported by postgres:\n",
    "# https://stackoverflow.com/questions/20410444/postgres-ignoring-clustered-index-on-date-query\n",
    "def count_clustered_index(table_name):\n",
    "    count = db_connector.count_clustered_indexes(table_name)\n",
    "    return count\n",
    "\n",
    "def check_using_bitmap(query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    use = False\n",
    "    if is_scan_operator(current_operator):\n",
    "        use = \"Bitmap\" in current_operator\n",
    "    \n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            use |= check_using_bitmap(child_node)\n",
    "    return use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Limit', 'Gather', 'Nested Loop', 'BitmapOr', 'Bitmap Index Scan', 'Aggregate', 'Merge Join', 'Hash Join', 'Sort', 'Bitmap Heap Scan', 'Index Only Scan', 'Index Scan', 'Seq Scan', 'Hash', 'Gather Merge']\n"
     ]
    }
   ],
   "source": [
    "physical_operators = set()\n",
    "\n",
    "def collect_physical_operators(physical_operators, query_plan): \n",
    "    physical_operators.add(query_plan[\"Node Type\"])\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            collect_physical_operators(physical_operators, child_node)\n",
    "            \n",
    "for _, query_plan in query_plans_with_index.items():\n",
    "    collect_physical_operators(physical_operators, query_plan) \n",
    "\n",
    "physical_operators = list(physical_operators)  \n",
    "print(physical_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 1.]]\n",
      "[array(['Aggregate', 'Scan', 'Sort'], dtype=object), array(['Aggregate', 'Join', 'Scan', 'Sort'], dtype=object), array(['Join', None], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# create an example dataframe to work with\n",
    "df = pd.DataFrame([\n",
    "    [\"Scan\", \"Join\"],\n",
    "    [\"Scan\", \"Aggregate\", \"Join\"],\n",
    "    [\"Aggregate\", \"Sort\"],\n",
    "    [\"Sort\", \"Join\"],\n",
    "    [\"Aggregate\", \"Scan\"]\n",
    "], columns=[\"operator1\", \"operator2\", \"operator3\"])\n",
    "\n",
    "# create a OneHotEncoder that ignores (0 encodes) unseen categories\n",
    "# and encode the categorical features for the example dataframe\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_encoded = encoder.fit_transform(df)\n",
    "print(X_encoded)\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124556\n",
      "1124556\n",
      "1124556\n",
      "1124556\n",
      "1124556\n",
      "1124556\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "feature_columns = [\"utility\", \"relevance\", \"num_pages\", \"use_itmap\"]\n",
    "features = pd.DataFrame(columns=feature_columns, index=range(len(query_index_pairs)))\n",
    "\n",
    "i = 0\n",
    "for (query, index), indexed_query_plan in query_plans_with_index.items():\n",
    "    if index == None: continue\n",
    "    labels.append(query_costs_with_index[(query, index)])\n",
    "    original_query_plan = query_plans_with_index[(query, None)]\n",
    "    original_query_cost = query_costs_with_index[(query, None)]\n",
    "    utility = estimate_index_utility_recursive(index, original_query_plan, indexed_query_plan)/original_query_cost\n",
    "    query_shape, index_shape = extract_shape_of_query_and_index(index, original_query_plan)\n",
    "    query_shape\n",
    "    operator_relevance = {}\n",
    "    evaluate_operator_relevance(operator_relevance, index, original_query_plan)\n",
    "    num_pages = get_number_of_pages(indexed_query_plan)\n",
    "    use_bitmap = check_using_bitmap(indexed_query_plan)\n",
    "    features.iloc[i][\"utility\"] = utility\n",
    "    features.iloc[i][\"num_pages\"] = num_pages\n",
    "    features.iloc[i][\"use_bitmap\"] = use_bitmap\n",
    "    i+=1\n",
    "\n",
    "# print(features)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = lgb.train(features, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=2, random_state=0).fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(random_state=1, max_iter=500).fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Cost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_query_index_pairs = []\n",
    "configurations = {}\n",
    "for (query, index) in filtered_query_index_pairs:\n",
    "    if query not in configurations:\n",
    "        configurations[query] = set()\n",
    "    configurations[query].add(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameter_selectivity(query_plan):\n",
    "    param_selectivities = []\n",
    "    calculate_parameter_selectivity(param_selectivities, query_plan)\n",
    "    return param_selectivities\n",
    "    \n",
    "    \n",
    "def _calculate_parameter_selectivity(param_selectivities, query_plan):\n",
    "    if has_child_node(original_query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _calculate_parameter_selectivity(child_node)\n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        for column in index.columns:\n",
    "            if column.name in condition:\n",
    "                param_selectivities.append(column, query_plan[\"Plan Rows\"]/column.table.row_count)\n",
    "    \n",
    "\n",
    "columns_features = {}\n",
    "def evaluate_query(query_plan):\n",
    "    operator = query_plan[\"Node Type\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "\n",
    "# parameters\n",
    "error_threshold = 0.05\n",
    "training_sample_size_alpha = None\n",
    "training_sample_size_beta = None\n",
    "stopping_threshold = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
