{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def json_dump(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "def q_error(actual, pred):\n",
    "    epsilon = 1e-4\n",
    "    q_e = 0\n",
    "    for i in range(len(pred)):\n",
    "        q_e += max((actual[i]+epsilon)/(pred[i]+epsilon),(pred[i]+epsilon)/(actual[i]+epsilon))\n",
    "    return q_e/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from selection.data_preparation import read_csv, index_conversion\n",
    "from selection.workload import Query, Index, Column, Table\n",
    "from constant import TPC_DS_TABLE_PREFIX, TPC_H_TABLE_PREFIX, LOGICAL_OPERATORS, PHYSICAL_OPERATORS, PHYISCAL_TO_LOGICAL_OPERATOR_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_info(row_info_filepath, column_info_filepath):\n",
    "    data_table_info = read_csv(row_info_filepath)[2:-2]\n",
    "    table_dict = OrderedDict()\n",
    "    for table_info in data_table_info:\n",
    "        table_info_tuple = table_info[0].split('|')\n",
    "        if len(table_info_tuple) < 2: continue\n",
    "        table_name = table_info_tuple[0].strip()\n",
    "        row_count = float(table_info_tuple[1].strip())\n",
    "        table = Table(table_name)\n",
    "        table.set_row_count(row_count)\n",
    "        table_dict[table_name] = table\n",
    "        \n",
    "    TABLE_PREFIX_MAP = None\n",
    "    if re.search(\"TPC_DS\", column_info_filepath) or re.search(\"dsb\", column_info_filepath):\n",
    "        TABLE_PREFIX_MAP = TPC_DS_TABLE_PREFIX\n",
    "    elif re.search(\"TPCH\", column_info_filepath):\n",
    "        TABLE_PREFIX_MAP = TPC_H_TABLE_PREFIX\n",
    "    elif re.search(\"IMDB\", column_info_filepath): \n",
    "        TABLE_PREFIX_MAP = None\n",
    "    else: \n",
    "        raise ValueError(\"Specified dataset not supported\")\n",
    "    \n",
    "    data_column_info = read_csv(column_info_filepath)[2:-2]\n",
    "    column_dict = OrderedDict()\n",
    "    for column_info in data_column_info:\n",
    "        column_info_tuple = column_info[0].split('|')\n",
    "        if len(column_info_tuple) < 3: continue\n",
    "        \n",
    "        if TABLE_PREFIX_MAP == None:\n",
    "            ## IMDB\n",
    "            table_name = column_info_tuple[0].strip()\n",
    "            column_name = column_info_tuple[1].strip()\n",
    "            null_frac = float(column_info_tuple[2].strip())\n",
    "            n_distinct = float(column_info_tuple[3].strip())\n",
    "            column = Column(column_name)\n",
    "            column.set_cardinality(-n_distinct * row_count if n_distinct < 0 else n_distinct)\n",
    "            column.set_null_fraction(null_frac)\n",
    "            if table_name in table_dict.keys():\n",
    "                column.table = table_dict[table_name]\n",
    "                column.table.add_column(column)\n",
    "        else:\n",
    "            column_name = column_info_tuple[0].strip()\n",
    "            null_frac = float(column_info_tuple[1].strip())\n",
    "            n_distinct = float(column_info_tuple[2].strip())\n",
    "            column = Column(column_name)\n",
    "            column.set_cardinality(-n_distinct * row_count if n_distinct < 0 else n_distinct)\n",
    "            column.set_null_fraction(null_frac)\n",
    "            if (prefix := column_name.split('_')[0]) in TABLE_PREFIX_MAP.keys():\n",
    "                table_name = TABLE_PREFIX_MAP[prefix]\n",
    "                if table_name in table_dict.keys():\n",
    "                    column.table = table_dict[table_name]\n",
    "                    column.table.add_column(column)      \n",
    "        column_dict[column_name] = column\n",
    "    return table_dict, column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict_DS_50G, column_dict_DS_50G = read_table_info(\"../data/TPC_DS_50G/tpcds50trow.csv\", \"../data/TPC_DS_50G/tpcds50stats.csv\")\n",
    "table_dict_DS_10G, column_dict_DS_10G = read_table_info(\"../data/TPC_DS_10G/tpcds10trow.csv\", \"../data/TPC_DS_10G/tpcds10stats.csv\")\n",
    "table_dict_H, column_dict_H = read_table_info(\"../data/TPCH/tpchtrow.csv\", \"../data/TPCH/tpchstats.csv\")\n",
    "# table_dict_DSB, column_dict_DSB = read_table_info(\"../data/DSB/dsbrow.csv\", \"../data/DSB/dsbstats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict_IMDB, column_dict_IMDB = read_table_info(\"../data/IMDB/imdb_trows.csv\", \"../data/IMDB/imdb_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25, 8, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_dict_DS_50G), len(table_dict_DS_10G), len(table_dict_H), len(table_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425, 425, 61, 38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_dict_DS_50G), len(column_dict_DS_10G), len(column_dict_H), len(column_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_configuration_to_obj(columns_dict, config_string):\n",
    "    configs = []\n",
    "    for config_s in config_string:\n",
    "        if config_s == \"[]\": \n",
    "            configs.append([])\n",
    "            continue\n",
    "        config = []\n",
    "        indexes_s = config_s.split('I')\n",
    "        for index_s in indexes_s:\n",
    "            if index_s == '': continue\n",
    "            table_columns = index_s.split('C')\n",
    "            indexed_columns = []\n",
    "            for table_column in table_columns:\n",
    "                table_column = table_column.strip('(), ')\n",
    "                if table_column == '': continue\n",
    "                column_name = table_column.split('.')[-1]\n",
    "                if column_name in columns_dict:\n",
    "                    indexed_columns.append(columns_dict[column_name])\n",
    "            config.append(Index(indexed_columns))\n",
    "        configs.append(config)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query_and_index_data(filepath, column_dict):\n",
    "    # data_list[i][0]: Query ID and Query text for the i-th query\n",
    "    # data_list[i][1]: Index configurations for the i-th query\n",
    "    # data_list[i][2]: Average cost of each configuration for the i-th query\n",
    "    # data_list[i][3]: Query execution plan of each configuration for the i-th query\n",
    "    # data_list[i][4]: Details execution costs (each query is executed 4 times and the last 3 times are recorded) of each configuration for the i-th query\n",
    "\n",
    "    data_list_string = read_csv(filepath)\n",
    "    data, queries = [], []\n",
    "    for i in range(len(data_list_string)):\n",
    "        data_list_string[i][0] = ast.literal_eval(data_list_string[i][0])\n",
    "        query = Query(data_list_string[i][0][0], data_list_string[i][0][1])\n",
    "        query.columns = [column for column_name, column in column_dict.items() if column_name in query.text]\n",
    "        queries.append(query)\n",
    "        indexes_string = index_conversion(data_list_string[i][1])\n",
    "        index_configurations = convert_configuration_to_obj(column_dict, indexes_string)\n",
    "        average_costs = ast.literal_eval(data_list_string[i][2])\n",
    "        plans = ast.literal_eval(data_list_string[i][3])\n",
    "        data.append([query, index_configurations, average_costs, plans])\n",
    "    return data, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_data, _ = read_query_and_index_data(\"../data/IMDB/imdb_job.csv\",column_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_50G_data, _ = read_query_and_index_data(\"../data/TPC_DS_50G/TPC_DS_50GB.csv\", column_dict_DS_50G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_10G_data, _ = read_query_and_index_data(\"../data/TPC_DS_10G/TPC_DS_10GB.csv\", column_dict_DS_10G)\n",
    "H_data, _ = read_query_and_index_data(\"../data/TPCH/TPC_H_10.csv\", column_dict_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSB_data, _ = read_query_and_index_data(\"../data/DSB/dsb.csv\", column_dict_DSB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_child_node(query_plan):\n",
    "    return \"Plans\" in query_plan.keys()\n",
    "\n",
    "def has_filtering_property(query_plan):\n",
    "    if \"Filter\" in query_plan.keys():\n",
    "        return query_plan[\"Filter\"]\n",
    "    if \"Hash Cond\" in query_plan.keys():\n",
    "        return query_plan[\"Hash Cond\"]\n",
    "    if \"Join Filter\" in query_plan.keys():\n",
    "        return query_plan[\"Join Filter\"]\n",
    "    return \"\"\n",
    "\n",
    "def is_join_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Join\"\n",
    "\n",
    "def is_sort_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Sort\"\n",
    "\n",
    "def is_aggregate_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Aggregate\"\n",
    "\n",
    "def is_scan_operator(operator):\n",
    "    return PHYISCAL_TO_LOGICAL_OPERATOR_MAP[operator] == \"Scan\"\n",
    "\n",
    "def check_indexed_column_in_condition(index, condition):\n",
    "    for column in index.columns:\n",
    "        if column.name in condition:\n",
    "            return True\n",
    "\n",
    "def get_table_from_plan_node(query_plan):\n",
    "    table = \"\"\n",
    "    if \"Relation Name\" in query_plan.keys():\n",
    "        table = query_plan[\"Relation Name\"]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_physical_operators(physical_operators, query_plan): \n",
    "    physical_operators.add(query_plan[\"Node Type\"])\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _collect_physical_operators(physical_operators, child_node)\n",
    "            \n",
    "def collect_physical_operators(data):\n",
    "    physical_operators = set()\n",
    "    for sample in data:\n",
    "        query_plans = sample[3]\n",
    "        for query_plan in query_plans:\n",
    "            _collect_physical_operators(physical_operators, query_plan)\n",
    "    return physical_operators\n",
    "\n",
    "\n",
    "physical_operators = collect_physical_operators(DS_10G_data)\n",
    "physical_operators.union(collect_physical_operators(DS_50G_data))\n",
    "physical_operators.union(collect_physical_operators(H_data))\n",
    "physical_operators.union(collect_physical_operators(IMDB_data))\n",
    "physical_operators = list(physical_operators)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 1\n",
    "def estimate_index_utility(index, original_query_plan, indexed_query_plan):\n",
    "    total_cost = 0\n",
    "    if has_child_node(original_query_plan):\n",
    "        for original_child_node, indexed_child_node in zip(indexed_query_plan[\"Plans\"], indexed_query_plan[\"Plans\"]):\n",
    "            total_cost += estimate_index_utility(index, original_child_node, indexed_child_node)\n",
    "    current_operator = indexed_query_plan[\"Node Type\"]\n",
    "    current_cost = original_query_plan[\"Total Cost\"]\n",
    "    if (condition := has_filtering_property(indexed_query_plan)) != \"\":\n",
    "        if is_join_operator(current_operator):\n",
    "            join_output_rows = indexed_query_plan[\"Plan Rows\"]\n",
    "            left_input_rows = indexed_query_plan[\"Plans\"][0][\"Plan Rows\"]\n",
    "            right_input_rows = indexed_query_plan[\"Plans\"][1][\"Plan Rows\"]\n",
    "            if check_indexed_column_in_condition(index, condition):    \n",
    "                current_cost = (1-np.sqrt(join_output_rows/(left_input_rows*right_input_rows)))*original_query_plan[\"Total Cost\"]\n",
    "        else:\n",
    "            selectivities = [indexed_query_plan[\"Plan Rows\"]/column.table.row_count for column in index.columns if column.name in condition]\n",
    "            average_selectivity = sum(selectivities)/len(selectivities) if len(selectivities) > 0 else 0\n",
    "            current_cost = (1-average_selectivity)*original_query_plan[\"Total Cost\"]\n",
    "    elif is_sort_operator(current_operator) and \"Sort Key\" in indexed_query_plan.keys():\n",
    "        sort_conditions = indexed_query_plan[\"Sort Key\"]\n",
    "        for sort_condition in sort_conditions:\n",
    "            if check_indexed_column_in_condition(index, sort_condition):\n",
    "                current_cost = indexed_query_plan[\"Total Cost\"]\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in indexed_query_plan.keys():\n",
    "        group_conditions = indexed_query_plan[\"Group Key\"]\n",
    "        for group_condition in group_conditions:\n",
    "            if check_indexed_column_in_condition(index, group_condition):\n",
    "                current_cost = indexed_query_plan[\"Total Cost\"]\n",
    "    return total_cost+current_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 2\n",
    "def extract_shape_of_query_and_index(index, original_query_plan, indexed_query_plan):\n",
    "    query_shape, index_shape = {}, []\n",
    "    _extract_query_shape(query_shape, original_query_plan)\n",
    "    visited = set()\n",
    "    _extract_index_shape(index_shape, index, indexed_query_plan, visited)\n",
    "    return query_shape, index_shape\n",
    "\n",
    "def _extract_query_shape(query_shape, query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    logical_operator = PHYISCAL_TO_LOGICAL_OPERATOR_MAP[current_operator]\n",
    "    if is_scan_operator(current_operator):\n",
    "        table = get_table_from_plan_node(query_plan)\n",
    "        if table in query_shape.keys():\n",
    "            query_shape[table].append(logical_operator)\n",
    "        else:\n",
    "            query_shape[table] = [logical_operator]\n",
    "        return table\n",
    "    \n",
    "    tables = []    \n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            table = _extract_query_shape(query_shape, child_node)\n",
    "            if table and logical_operator:\n",
    "                tables.append(table)\n",
    "                query_shape[table].append(logical_operator)\n",
    "    return tables[0] if 0<len(tables)<2 else \"\"\n",
    "\n",
    "def _extract_index_shape(index_shape, index, query_plan, visited):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    logical_operator = PHYISCAL_TO_LOGICAL_OPERATOR_MAP[current_operator]\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _extract_index_shape(index_shape, index, child_node, visited)\n",
    "            \n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        for column in index.columns:\n",
    "            if column in visited: continue\n",
    "            elif column.name in condition:\n",
    "                index_shape.append(logical_operator)\n",
    "                visited.add(column)\n",
    "    elif is_sort_operator(current_operator) and \"Sort Key\" in query_plan.keys():\n",
    "        sort_conditions = query_plan[\"Sort Key\"]\n",
    "        for sort_condition in sort_conditions:\n",
    "            for column in index.columns:\n",
    "                if column in visited: continue\n",
    "                elif column.name in sort_condition:\n",
    "                    index_shape.append(logical_operator)\n",
    "                    visited.add(column)\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        aggregate_conditions = query_plan[\"Group Key\"]\n",
    "        for aggregate_condition in aggregate_conditions:\n",
    "            for column in index.columns:\n",
    "                if column in visited: continue\n",
    "                elif column.name in aggregate_condition:\n",
    "                    index_shape.append(logical_operator)\n",
    "                    visited.add(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 3\n",
    "def evaluate_operator_relevance(index, query_plan):\n",
    "    result = {}\n",
    "    _evaluate_operator_relevance(result, index, query_plan)\n",
    "    return result\n",
    "\n",
    "def _evaluate_operator_relevance(operator_relevance, index, query_plan):\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _evaluate_operator_relevance(operator_relevance, index, child_node)\n",
    "            \n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    relevance = 0\n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        selectivities = [query_plan[\"Plan Rows\"]/column.table.row_count for column in index.columns if column.name in condition]\n",
    "        relevance = sum(selectivities)/len(selectivities) if len(selectivities) > 0 else 0\n",
    "    elif is_sort_operator(current_operator) and \"Sort Key\" in query_plan.keys():\n",
    "        densities = []\n",
    "        conditions = query_plan[\"Sort Key\"]\n",
    "        for condition in conditions:\n",
    "            for column in index.columns:\n",
    "                if column.name in condition:\n",
    "                    densities.append(column.cardinality/column.table.row_count)\n",
    "        relevance = sum(densities)/len(densities) if len(densities) > 0 else 0\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        densities = []\n",
    "        conditions = query_plan[\"Group Key\"]\n",
    "        for condition in conditions:\n",
    "            for column in index.columns:\n",
    "                if column.name in condition:\n",
    "                    densities.append(column.cardinality/column.table.row_count)\n",
    "        relevance = sum(densities)/len(densities) if len(densities) > 0 else 0\n",
    "    if current_operator not in operator_relevance: \n",
    "        operator_relevance[current_operator] = []\n",
    "    operator_relevance[current_operator].append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal 4\n",
    "def get_number_of_pages(query_plan):\n",
    "    return query_plan[\"Shared Hit Blocks\"] + query_plan[\"Shared Read Blocks\"] + query_plan[\"Local Hit Blocks\"] + query_plan[\"Local Read Blocks\"]\n",
    "\n",
    "# check primary key instead\n",
    "# not used\n",
    "def count_clustered_index(db_connector, table_name):\n",
    "    count = db_connector.count_clustered_indexes(table_name)\n",
    "    return count\n",
    "\n",
    "def check_bitmap(query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    use = False\n",
    "    if is_scan_operator(current_operator):\n",
    "        use = \"Bitmap\" in current_operator\n",
    "    \n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            use |= check_bitmap(child_node)\n",
    "    return use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      " [3821.840666666667, 16285.343666666668, 117.93266666666666, 6416.822666666667, 6354.952333333334, 6355.659666666666, 6457.203666666665, 6407.342666666666, 6349.016333333333, 6415.018666666667, 6649.308666666667, 4284.687999999999, 4400.314333333333, 4409.274666666667, 4306.645, 2499.605666666667, 2430.9223333333334, 1219.0106666666668, 148.68966666666668, 518.3246666666666, 493.834, 2563.286, 2632.2236666666663, 2532.9833333333336, 2638.4143333333336, 2631.3766666666666, 1159.6956666666667, 387.03099999999995, 1093.7616666666665, 2532.7793333333334, 1421.9309999999998, 2114.7153333333335, 2479.487333333333, 2542.610666666667, 2549.241, 2155.045, 1119.1516666666666, 2138.8213333333333, 2019.4913333333334, 2041.689333333333, 2149.953666666667, 927.7326666666668, 180.31333333333336, 844.1093333333333, 5028.141333333333, 4871.205666666666, 1515.2766666666666, 5094.430333333333, 70961.72466666666, 54874.82233333334, 51074.741, 52452.49966666667, 52518.73233333334, 51555.38433333333, 51681.649, 2368.5000000000005, 2288.1536666666666, 2386.524, 307.74333333333334, 2172.9376666666667, 2405.385333333333, 2482.491333333333, 2627.050333333333, 4125.783333333333, 2601.589, 2621.024, 2826.6480000000006, 2924.808333333334, 2956.8133333333335, 2910.665333333333, 4266.2699999999995, 8928.077666666666, 2261.4503333333337, 4255.229666666666, 4278.030333333333, 8735.357666666665, 2084.404666666667, 8869.220000000001, 4019.5676666666664, 4140.865000000001, 3827.4623333333334, 3115.986333333333, 3605.0496666666672, 3921.222333333333, 3805.74, 1975.813, 1631.0613333333333, 1981.4076666666667, 3437.020666666667, 3513.3716666666674, 4198.214333333333, 1055.502, 4179.415, 4297.836666666666, 12303.568333333335, 12332.175333333333, 11888.502666666667, 14457.813, 365.61966666666666, 1372.1243333333332, 844.0966666666667, 1562.2906666666665, 1006.826, 1690.6453333333332, 649.967, 1685.1143333333332, 2382.5229999999997, 0.016, 0.012666666666666668, 1076.3863333333331, 632.024, 653.1276666666666, 654.681, 701.0893333333333, 649.1753333333332, 2597.9303333333332, 2591.6026666666667, 2596.302333333333, 2576.6749999999997, 2565.8523333333337, 2428.896, 2574.5303333333336, 2540.9593333333332, 12805.114333333333, 12989.528, 2621.954666666667, 2481.3253333333337, 2536.751666666667, 4281.399, 5002.066666666667, 12815.965333333334, 4748.891666666666, 5027.415666666667, 4449.565, 4664.673, 4930.953666666667, 649.8686666666666, 2587.999, 136119.483, 3496.843666666666, 13811.831666666665, 1691.5773333333334, 705.5406666666667, 1682.3213333333333, 2033.544, 1646.068, 2028.0993333333333, 1645.2126666666666, 832.5703333333332, 1647.017, 3986.515, 3831.8909999999996, 3955.7856666666667, 3733.392, 3432.265, 3891.2479999999996, 3948.579333333333, 3666.578666666667, 2808.0166666666664, 4591.011333333333, 3961.176, 3595.364, 6654.157666666666, 6227.8876666666665, 2772.234, 3532.911666666667, 3885.905, 3668.572666666667, 3613.233, 3704.2236666666668, 3505.0723333333335, 3923.679666666667, 9907.459333333332, 10009.752, 4368.145333333333, 4441.924333333333, 4470.054999999999, 4885.602333333333, 4772.332333333334, 3864.7623333333336, 4414.165333333333, 4465.080333333334, 3440.2116666666666, 5560.358333333334, 4481.741666666668, 2959.911666666667, 3947.6493333333333, 4108.287, 4052.6876666666667, 4298.285666666667, 4053.8546666666666, 3969.541333333333, 1328.0996666666667, 2074.4646666666663, 2063.404, 1600.5753333333332, 11792.838333333333, 12173.520666666669, 12184.307333333332, 3831.8019999999997, 1806.8126666666667, 3382.2899999999995, 2210.6683333333335, 2406.833666666667, 2431.3790000000004, 2458.586333333333, 2408.385, 2117.02, 2023.948666666667, 2457.9553333333333, 2292.6073333333334, 1715.155, 76.46966666666667, 1698.2136666666668, 1672.5903333333333, 1687.4846666666665, 1562.9176666666665, 1589.1090000000002, 1669.3893333333333, 1660.2273333333335, 10309.021333333332, 10328.485666666667, 2191.251666666667, 2824.3469999999998, 2516.9410000000003, 1791.9659999999997, 2797.5750000000003, 13867.142666666667, 13604.874666666665, 13479.341999999999, 13537.031333333332, 13307.231999999998, 13659.11, 13751.356, 5549.2570000000005, 18810.017333333333, 12891.838000000002, 1450.0053333333335, 183.83433333333332, 1545.5330000000001, 12270.921333333332, 12322.358666666667, 2411.913666666667, 2417.8263333333334, 2588.7003333333337, 2307.0636666666664, 3448.035, 4343.589, 3715.6389999999997, 4265.502666666666, 4304.353, 4133.1466666666665, 4207.065666666666, 4210.346333333334, 4222.185333333334, 19386.472666666665, 19634.493333333332, 22337.297666666665, 20073.708333333336, 19011.358999999997, 18970.042666666664, 2385.425333333333, 2330.0913333333333, 4710.591333333333, 4694.167666666666, 4714.982666666667, 4408.930666666667, 4649.783333333333, 4693.629, 4560.312333333333, 4074.3070000000002, 4769.284666666666, 4711.232333333334, 4741.893666666667, 4529.938999999999, 4810.608, 4674.31, 1707.7076666666665, 4663.468, 5841.616333333334, 468.636, 520.8253333333333, 506.9113333333333, 498.9823333333333, 495.42966666666666, 334.3016666666667, 527.7796666666667, 144.783, 200.92333333333332, 304.44800000000004, 312.34200000000004, 312.321, 327.0786666666666, 883.474, 1126.7486666666666, 1282.9463333333333, 4226.949, 1292.719, 1545.9683333333332, 1853.7313333333332, 12552.294666666668, 12685.384, 12324.711333333335, 14745.099333333332, 11724.046, 11452.909666666666, 11741.582, 2799.788666666667, 2838.645666666667, 985.0540000000001, 993.9630000000001, 53.48466666666667, 234.45633333333333, 170.80100000000002, 242.665, 181.90199999999996, 283.13233333333335, 276.91799999999995, 229.61566666666667, 25054.138333333332, 516.5266666666666, 24712.175333333336, 25627.513666666666, 514.0616666666666, 64.64333333333333, 1000600.5723333334, 277976.405, 79476.434, 1143030.2936666666, 1018010.626, 1528.073666666667, 1525.1313333333335, 6021.347333333334, 5848.854, 6015.578, 647.5106666666667, 1850.978, 1853.5846666666666, 2519.63, 3618.090666666667, 3813.6543333333334, 10233.812, 75.32333333333332, 6453.399666666667, 6408.2733333333335, 6409.4890000000005, 6477.055, 6502.665333333333, 6361.561000000001, 6431.314666666666, 6683.931333333333, 4052.9553333333333, 4154.477, 4274.466, 4104.5650000000005, 2528.445333333333, 2450.0286666666666, 2449.304333333333, 152.22166666666666, 519.074, 495.38366666666667, 2550.114666666667, 2614.9893333333334, 2544.9016666666666, 2640.896666666667, 2625.984, 1158.354, 384.579, 1080.003, 2599.210333333333, 1380.9993333333332, 2355.02, 2517.7176666666664, 2591.3880000000004, 2564.6296666666663, 2200.455, 1151.9669999999999, 2169.4003333333335, 2015.3686666666665, 2095.357, 2188.4643333333333, 928.5746666666668, 177.50466666666668, 842.8506666666666, 5004.060666666667, 4854.662666666667, 1247.6513333333332, 5123.704333333334, 71165.14266666667, 44809.728, 40294.07766666666, 41294.189666666665, 40467.755000000005, 40642.560000000005, 40934.24533333333, 2531.175333333333, 2219.0413333333336, 2546.0843333333337, 2484.5866666666666, 315.297, 2506.3673333333336, 2736.6306666666665, 4139.321, 2662.257, 2691.8623333333335, 2801.518, 2923.0753333333337, 2945.396666666667, 2858.0753333333328, 7237.406, 7244.647666666667, 7233.4929999999995, 4155.5363333333335, 8791.426666666666, 2268.0319999999997, 4191.216333333333, 4173.275, 8706.585333333334, 2019.57, 8791.635333333334, 4231.000666666667, 4214.074666666666, 3876.847333333333, 4195.473, 3641.6150000000002, 3931.083666666667, 3822.765, 1996.2803333333334, 1611.2913333333333, 1989.9956666666667, 3437.9599999999996, 3499.829, 4263.114666666667, 1036.8983333333333, 4247.436333333333, 4009.312, 12206.322333333332, 12360.864333333333, 11836.193666666668, 14473.318, 354.327, 1405.9296666666667, 839.696, 1573.9046666666666, 990.814, 1693.7806666666668, 841.0496666666668, 1687.1006666666665, 2423.1803333333332, 0.020666666666666667, 0.012000000000000002, 1089.3550000000002, 641.715, 634.6136666666667, 644.6383333333334, 693.5793333333332, 641.9313333333333, 2580.824666666667, 2618.186, 2625.2156666666665, 2585.5723333333335, 2594.5776666666666, 2445.4393333333333, 2624.521, 2525.7873333333337, 13281.949, 13174.106333333335, 2647.6156666666666, 2518.456, 2543.2490000000003, 4348.647, 5050.657333333334, 12941.818666666666, 4756.471, 5063.147, 4467.017666666667, 4730.830333333333, 4978.980333333333, 609.6926666666667, 2549.6059999999998, 114730.46033333334, 3618.8963333333336, 13841.805, 1685.7886666666666, 844.2890000000001, 1689.1306666666667, 1949.1450000000002, 1547.804666666667, 1919.6136666666664, 1640.508, 182.72400000000002, 4064.0203333333334, 3842.199666666667, 4150.324333333333, 3876.782333333334, 3683.4249999999997, 3974.5956666666666, 4045.720333333333, 3793.229, 3979.6433333333334, 4626.815, 4079.5190000000002, 3728.922666666667, 6665.549333333333, 6242.040666666667, 2837.8716666666664, 3472.2343333333333, 3817.852666666666, 3652.799333333334, 3619.8986666666665, 3700.8683333333333, 3584.172666666667, 3878.7943333333333, 9794.031333333334, 9970.006, 4292.3223333333335, 4431.8606666666665, 4420.796666666668, 4809.318, 4692.678333333333, 3866.982666666667, 4339.406, 4451.8679999999995, 4341.792666666667, 5496.892, 4430.156666666667, 2124.503666666667, 3837.893666666667, 4035.2180000000003, 4060.961333333333, 4330.001333333334, 2235.7473333333332, 3877.51, 1252.5896666666667, 2010.6646666666666, 2018.350333333333, 1581.3326666666665, 11856.980666666665, 12165.839, 12294.026333333333, 3767.5733333333337, 1804.1686666666665, 3272.539666666666, 2256.404666666667, 2344.833666666667, 2346.379, 2388.3596666666667, 2326.840333333333, 2047.8990000000001, 1961.695, 2339.8086666666672, 2198.4086666666667, 1703.807, 73.76299999999999, 1709.811, 1784.1813333333332, 1700.379, 1559.938, 1708.9436666666668, 1596.5469999999998, 1690.2323333333334, 10400.186333333333, 10333.869666666667, 2786.0139999999997, 2167.03, 2768.605333333333, 2506.073, 2761.525, 13560.379666666668, 13524.978333333333, 13746.351999999999, 13377.882666666666, 13162.234333333334, 13532.722666666667, 13556.510666666667, 5491.168333333332, 18826.553, 12807.543, 1457.4986666666666, 192.34766666666667, 1542.7146666666667, 12367.415666666668, 12396.703666666668, 2295.5620000000004, 2372.9126666666666, 2373.831666666667, 2580.5436666666665, 3458.75, 4332.736, 3708.1986666666667, 4263.291333333334, 4287.980666666666, 4109.977333333333, 4243.008666666666, 4173.787, 4262.138333333333, 8087.987666666668, 8511.199333333332, 11279.000333333335, 8955.136666666667, 7963.671333333333, 8026.209333333333, 2090.222666666667, 2073.0563333333334, 2074.6596666666665, 2106.2219999999998, 1955.7793333333336, 4729.661, 4672.029333333333, 3981.164666666667, 4406.797, 4685.32, 4697.797666666666, 4565.452, 4110.764, 4741.005666666667, 4690.637333333333, 4744.760666666666, 4552.156333333333, 4783.136666666666, 4685.856, 1694.0349999999999, 4694.207666666666, 5175.056, 515.846, 579.8606666666667, 539.0243333333334, 551.9140000000001, 504.51933333333335, 394.5316666666667, 557.4603333333333, 140.0633333333333, 190.97133333333332, 306.39933333333335, 314.596, 317.117, 308.8136666666667, 1048.533333333333, 1141.5663333333334, 1336.6113333333333, 3982.85, 1353.096, 1564.8826666666666, 1845.705, 12503.401666666667, 12667.779666666667, 12134.728666666668, 14743.909999999998, 11739.142333333331, 11668.995333333334, 11490.491, 2761.515000000001, 2841.5833333333335, 991.3123333333333, 1006.053, 48.910000000000004, 239.95566666666664, 171.3836666666667, 249.92, 186.66033333333334, 288.471, 276.18199999999996, 235.70833333333334, 120673.42966666666, 1027.5606666666667, 124156.62200000002, 123221.58766666666, 1048.1296666666667, 430.65366666666665, 42544.911, 50024.602999999996, 35625.994333333336, 10380.503333333334, 42554.15233333333, 1535.2866666666669, 1548.3410000000001, 6013.160666666667, 5769.173333333333, 6033.704000000001, 656.8946666666667, 1839.6423333333332, 1858.9620000000002, 2517.218666666666, 3625.628666666667, 3801.3520000000003, 6920.336666666667, 70.554, 6448.360333333334, 6031.969, 6392.940666666666, 6494.802, 6466.518000000001, 6408.016666666666, 6462.693666666666, 6695.7446666666665, 4447.7406666666675, 4496.520333333334, 4575.828333333334, 4473.644666666666, 2549.0886666666665, 2457.4973333333332, 2465.4283333333333, 146.839, 519.6413333333334, 507.86899999999997, 2549.028333333333, 2633.929, 2539.3463333333334, 2651.749666666667, 2620.8006666666665, 1158.6, 400.202, 1102.16, 2581.1730000000002, 1419.3853333333334, 2114.422, 2478.7726666666663, 2562.969, 2545.003, 2168.791333333333, 1123.5023333333331, 2174.5936666666666, 2039.8266666666666, 2095.563666666667, 2151.333, 919.8876666666666, 187.43233333333333, 841.2143333333333, 5063.931333333333, 4857.057, 1891.689, 5128.005666666667, 72196.17933333333, 43595.914333333334, 40560.84099999999, 41029.35833333334, 40620.80066666667, 40905.689333333336, 41428.01733333333, 2637.406, 2421.894333333333, 2462.387666666667, 304.2383333333333, 2201.0396666666666, 2493.424666666667, 2548.7063333333335, 2709.0346666666665, 4213.586, 2792.301, 2700.8170000000005, 2573.799333333333, 2716.6839999999997, 2690.7909999999997, 2641.6516666666666, 7149.433, 7258.911999999999, 7214.300333333333, 7229.363666666667, 4249.725666666666, 8930.236333333332, 2264.8349999999996, 4243.081000000001, 4214.925, 8747.782333333334, 2065.411, 8885.648666666666, 4108.759666666666, 4139.508000000001, 3832.679666666667, 3098.8043333333335, 3632.516333333333, 3897.4346666666665, 3793.0813333333335, 1989.3819999999998, 1645.7256666666665, 1989.9436666666668, 3444.36, 3479.127, 4252.375333333333, 1032.4799999999998, 4246.909333333334, 3864.7033333333334, 12205.971666666666, 12345.211333333333, 11857.325666666666, 14306.803, 380.637, 1379.2873333333334, 842.9163333333335, 1577.8663333333334, 985.341, 1704.2823333333333, 826.5326666666666, 1812.094, 2411.3709999999996, 0.015333333333333332, 0.014333333333333332, 1091.882, 666.4653333333334, 692.4013333333334, 655.5163333333334, 637.2216666666667, 642.2776666666667, 2603.151, 2614.372, 2613.010333333333, 2707.467, 2678.5443333333333, 2551.1843333333336, 2736.6126666666664, 2647.6473333333333, 13362.268666666665, 13404.681333333334, 2832.0033333333336, 2616.0699999999997, 2640.3920000000003, 4538.606333333333, 5310.468333333333, 12877.418, 4772.048, 5070.11, 4522.113333333334, 4715.452333333334, 4974.894333333333, 749.7479999999999, 2528.8823333333335, 161180.60466666668, 3640.646666666666, 13776.036666666667, 1677.164, 840.8539999999999, 1678.1123333333333, 3782.4303333333337, 1626.4673333333333, 2028.1826666666666, 1653.11, 744.5893333333333, 4178.788, 4075.8043333333335, 4105.697, 4008.9420000000005, 4264.819333333333, 3554.9600000000005, 4179.512, 4216.811, 4093.9300000000003, 4236.323333333333, 3832.5440000000003, 6589.157666666667, 6225.746666666666, 2861.0049999999997, 3555.036666666667, 3802.7876666666666, 3648.007, 3631.438, 3687.995666666667, 3482.7526666666668, 3864.1450000000004, 9912.584, 9955.642333333333, 4331.529333333333, 4409.434, 4456.550333333333, 4799.7300000000005, 4718.935333333334, 3993.5480000000002, 4338.342333333334, 4423.549666666667, 4377.8043333333335, 5715.155, 4423.986000000001, 2990.1110000000003, 3839.9396666666667, 4018.1083333333336, 4036.0253333333335, 4312.376333333334, 4053.7443333333335, 3829.1806666666666, 1267.0576666666666, 2007.3913333333333, 1994.856, 1599.8563333333334, 11733.461666666664, 12136.027, 12215.908666666668, 3692.7073333333333, 1718.5960000000002, 3231.375, 2042.4443333333336, 2348.729, 2351.803, 2382.414666666667, 2338.39, 2033.443, 1958.9106666666667, 2324.082, 2176.1453333333334, 13159.481666666665, 12227.860666666667, 13062.511333333334, 13092.453000000001, 13323.802666666665, 13076.205666666667, 13094.906333333332, 12881.921999999999, 13191.577666666666, 10356.133666666667, 10330.597666666667, 2187.8193333333334, 2804.7369999999996, 2514.135, 1787.0856666666666, 2784.839, 13215.590000000002, 13302.617, 13046.828333333333, 13083.069000000001, 12810.779333333334, 13219.371333333334, 13346.530333333334, 5504.966666666667, 18231.774666666664, 12508.883333333331, 1464.5123333333333, 188.734, 1538.5906666666667, 12319.175333333333, 12335.616666666667, 2360.8863333333334, 2360.7926666666667, 2539.369666666667, 2296.3596666666667, 4329.763, 3709.729, 4267.420666666667, 4372.438333333333, 4099.681333333334, 4216.801333333334, 4245.891, 4206.975333333333, 8021.397333333333, 8463.353666666668, 11243.158000000001, 8960.927333333333, 7944.360000000001, 7909.632666666667, 2262.0796666666665, 2232.3826666666664, 2235.775, 2206.9313333333334, 2221.937, 2185.5916666666667, 4719.946666666667, 4714.289333333333, 3923.2386666666666, 4611.820333333333, 4923.133666666667, 4698.382333333334, 4553.711333333334, 4132.312666666668, 4734.432333333333, 4686.681333333333, 4748.8026666666665, 4569.166666666667, 4785.599666666667, 4614.464, 1710.1456666666666, 4676.957666666666, 5185.683333333333, 518.5070000000001, 565.4763333333334, 570.9713333333333, 544.0363333333333, 535.9766666666666, 399.46999999999997, 570.544, 141.75333333333333, 200.11266666666666, 305.99, 319.1186666666667, 311.16633333333334, 312.7656666666667, 997.8710000000001, 1136.1253333333332, 1292.4309999999998, 3845.6246666666666, 1308.119, 1560.5816666666667, 1864.8346666666666, 12566.062666666667, 12609.285666666668, 12111.732666666669, 14727.079, 11684.322999999999, 11478.533333333333, 11797.243333333332, 2752.0243333333333, 2801.2556666666665, 996.0690000000001, 985.3429999999998, 48.716, 235.352, 169.0343333333333, 232.398, 179.94933333333333, 276.31333333333333, 346.76366666666667, 227.37199999999999, 76725.346, 769.7416666666667, 79691.22666666667, 79330.445, 760.6863333333334, 64.158, 1443130.7876666666, 412258.65233333333, 109421.95633333334, 1633019.6293333333, 1446986.5606666666, 1513.299, 1508.5896666666667, 6044.973333333332, 5827.953666666667, 6088.776666666668, 695.0763333333334, 1879.7323333333334, 1879.8426666666667, 2646.1333333333337, 3520.933333333333, 3820.2153333333335, 16034.976, 140.53066666666666, 6453.012333333333, 6153.200333333334, 6396.961333333334, 6508.241999999999, 6452.704999999999, 6387.367333333333, 6484.1990000000005, 6712.483, 4607.139666666667, 4700.280666666667, 4770.989666666667, 4687.141666666666, 2540.4563333333335, 2465.837666666667, 1242.3056666666669, 170.45399999999998, 518.5886666666667, 502.2746666666667, 2536.3843333333334, 2623.783, 2552.9623333333334, 2622.479, 2629.676, 1178.914, 379.30266666666665, 1073.4636666666665, 2713.6406666666667, 1348.8253333333334, 2369.1820000000002, 2614.7729999999997, 2723.7726666666667, 2671.1793333333335, 2117.021, 2010.6760000000002, 2051.898, 2122.147, 832.4263333333333, 927.3123333333333, 172.514, 853.2750000000001, 5029.497, 4878.254, 1834.0493333333334, 5097.049666666667, 71402.43766666666, 54594.65933333334, 50249.93633333334, 51461.968666666675, 49894.28566666666, 52022.456000000006, 53402.579, 2528.2606666666666, 2270.763, 310.36466666666666, 2523.1716666666666, 2544.3933333333334, 2491.3623333333335, 2647.57, 4122.7413333333325, 2602.731, 2614.397333333333, 2602.946, 2721.1673333333333, 2727.1046666666666, 2686.8553333333334, 4264.682333333333, 9062.272666666666, 2262.9626666666663, 4260.335, 4305.319333333333, 8792.212333333335, 2070.4706666666666, 9034.191, 4209.627333333333, 4339.704666666667, 4054.611666666666, 4330.305333333334, 3773.387, 4080.5246666666662, 3921.347333333333, 1998.2666666666667, 1636.3756666666668, 2002.228, 3430.1176666666665, 3483.874333333333, 4225.491666666668, 1025.8073333333334, 4259.143333333333, 3608.4353333333333, 12211.512666666667, 12395.685333333333, 11821.452666666666, 14456.69, 378.11566666666664, 1378.6756666666668, 852.241, 1583.7983333333334, 988.3516666666666, 1703.9856666666665, 841.9173333333334, 1689.2186666666666, 2413.9956666666662, 0.015, 0.038, 1084.9709999999998, 643.3253333333333, 686.6350000000001, 656.7723333333333, 643.0476666666667, 635.116, 2636.0063333333333, 2636.3776666666668, 2655.403, 2649.0393333333336, 2574.762333333333, 2436.1020000000003, 2634.0876666666663, 2545.5236666666665, 12913.806333333334, 12890.597666666668, 2680.9426666666664, 2484.893, 2553.2019999999998, 4347.262, 5044.236, 13357.030666666666, 4771.702333333334, 5037.083666666666, 4531.4890000000005, 4725.651333333334, 4965.697666666667, 643.1836666666667, 2576.7183333333337, 135760.939, 3687.1813333333334, 13824.394333333335, 1683.4363333333333, 711.6323333333333, 1705.0510000000002, 1938.1943333333331, 1542.0813333333333, 1925.3713333333333, 1644.7616666666665, 643.5983333333334, 4123.145666666666, 3963.5573333333327, 4121.426333333333, 4092.712333333333, 3615.252, 4073.4103333333333, 4177.564666666666, 4203.7626666666665, 3000.9076666666665, 4424.164, 4216.444666666666, 3715.634666666667, 6307.3189999999995, 5852.055666666667, 2980.6910000000003, 3503.3506666666667, 3883.0603333333333, 3713.0826666666667, 3670.116, 3753.197666666667, 3591.1216666666664, 3904.113, 9892.172, 9981.659, 4275.083, 4428.047333333333, 4420.653, 4781.559666666667, 4733.188333333333, 3863.69, 4354.428333333333, 4448.599333333334, 4384.070333333334, 5487.355666666666, 4428.322, 722.9713333333334, 893.6523333333334, 1106.2773333333332, 929.058, 1308.6546666666666, 740.189, 114.13400000000001, 1252.3803333333333, 2040.8116666666667, 2047.5656666666666, 1641.5103333333334, 11554.273000000001, 11864.447999999999, 12011.302666666665, 3790.643, 1779.341333333333, 3287.2233333333334, 2286.672666666667, 2343.5596666666665, 2320.639, 2407.049333333333, 2322.679666666667, 2056.256, 1960.8156666666666, 2324.8176666666664, 2184.0726666666665, 12111.314999999997, 11115.186333333333, 11999.017333333331, 12152.783666666668, 12196.599666666667, 12011.420666666667, 12078.597000000002, 11750.173333333332, 11941.388333333334, 10328.445666666667, 10301.317333333334, 2796.7360000000003, 2162.195, 2769.08, 2506.529666666667, 2792.892333333333, 12763.552000000001, 12815.798666666667, 12569.400666666668, 12722.108999999999, 12413.253666666666, 12816.242333333334, 12793.500666666667, 5465.317666666667, 17707.657, 12035.86, 1473.8970000000002, 185.7413333333333, 1544.5256666666667, 12595.69133333333, 12734.916, 2373.6696666666667, 2545.529666666667, 2308.653, 2363.1983333333337, 3509.0069999999996, 4500.239333333332, 3797.9853333333326, 4375.749333333333, 4395.730333333334, 4253.384999999999, 4208.529666666666, 4206.199333333333, 4232.552, 19401.269, 19574.438333333335, 22329.800666666666, 19977.38266666667, 18955.803000000004, 18907.39933333333, 2553.7690000000002, 2527.0589999999997, 4740.431, 4674.625, 3858.3036666666667, 4446.093, 4689.672666666666, 4730.181, 4578.46, 4129.647, 4792.791333333334, 4688.594666666667, 4764.376666666667, 4563.159666666666, 4782.642666666667, 4658.881666666667, 1663.8893333333333, 4670.5019999999995, 4912.082, 483.6836666666666, 563.7829999999999, 555.6106666666668, 504.86100000000005, 490.9476666666667, 353.7653333333333, 546.4256666666668, 143.37, 205.44733333333338, 319.1656666666666, 326.11466666666666, 326.2633333333333, 320.33933333333334, 907.3916666666668, 1117.2796666666666, 1290.2913333333333, 3344.1273333333334, 1320.2, 1547.3696666666667, 1858.4923333333334, 12592.837, 12760.477666666666, 12224.779666666667, 14817.501333333334, 11679.985999999999, 11481.582333333334, 11814.307, 2763.251, 2825.5983333333334, 979.2543333333333, 986.1999999999999, 66.84533333333333, 233.14633333333333, 170.07266666666666, 242.0476666666667, 188.545, 267.16066666666666, 275.7136666666667, 229.56266666666667, 100109.817, 918.8616666666667, 101593.11300000001, 101044.49833333334, 909.7173333333334, 441.065, 33324.820999999996, 37696.56266666667, 26976.838999999996, 9442.014333333333, 33660.30866666667, 1520.6753333333334, 1499.1933333333334, 6056.38, 5789.636333333333, 6037.696333333333, 1714.559, 1848.6313333333335, 1851.5493333333334, 2524.3053333333332, 3964.378, 3832.7436666666667, 4223.517666666667, 28.558333333333337, 6486.968666666667, 6392.958666666666, 6395.350666666666, 6486.705666666668, 6463.608666666667, 6418.876, 6494.516333333333, 6710.974333333333, 4359.573666666666, 4472.236666666667, 4498.3910000000005, 4409.758, 2550.4953333333333, 2477.0313333333334, 2461.397333333334, 161.87966666666668, 516.445, 506.00399999999996, 2585.197, 2641.671333333333, 2548.498333333333, 2642.7273333333337, 2627.909, 1180.8253333333332, 366.541, 1089.9846666666665, 2653.4556666666663, 1502.7036666666665, 2236.889333333333, 2557.421333333333, 2625.3626666666664, 2628.7856666666667, 2199.411333333333, 855.9953333333333, 2206.759, 2038.4956666666667, 2123.722666666667, 2193.789666666667, 926.1619999999999, 189.26466666666667, 848.09, 5105.993666666666, 4921.347000000001, 1509.9573333333335, 5060.2643333333335, 73258.37433333334, 43820.632000000005, 40852.683333333334, 41710.077, 41123.41333333334, 40995.22466666667, 41093.532333333336, 2510.853, 2404.805, 2563.3393333333333, 317.7663333333333, 2221.365666666667, 2553.4166666666665, 2457.253, 2599.453, 4099.8623333333335, 2591.4443333333334, 2592.411666666667, 2874.556666666667, 2983.835333333333, 2978.7286666666664, 2927.9223333333334, 7229.317999999999, 7241.121, 7214.376666666667, 4153.490000000001, 8898.618666666667, 2246.7436666666667, 4125.064333333333, 4159.815, 8761.553333333335, 2034.4396666666664, 9002.361, 4196.245, 4186.551666666667, 3913.49, 3155.3903333333333, 3764.482, 3949.5499999999997, 3831.9553333333333, 1999.3256666666666, 1603.9350000000002, 1998.1499999999999, 3453.169, 3536.143, 4289.257333333333, 1054.834, 4273.431666666666, 4166.048, 12267.932333333332, 12430.572666666667, 11913.504666666666, 14436.125, 359.26133333333337, 1393.1573333333333, 840.7199999999999, 1593.3580000000002, 995.1619999999999, 1695.7786666666668, 829.2936666666666, 1689.7123333333332, 2430.146, 3895.939, 2919.846666666667, 657.1419999999999, 654.5179999999999, 700.2173333333334, 654.4263333333333, 639.2126666666667, 2636.277, 2624.979333333333, 2624.124, 2616.719, 2586.5716666666667, 2457.0343333333335, 2614.171, 2537.3126666666667, 12948.508000000002, 13010.059333333333, 2667.286, 2525.3106666666667, 2585.618, 4340.391333333334, 5095.506333333334, 12844.987, 4795.896333333333, 5097.4456666666665, 4530.4766666666665, 4739.125666666667, 5019.808666666667, 730.4203333333334, 2522.579666666667, 160561.619, 3617.7763333333332, 13827.113666666666, 1682.1539999999998, 740.98, 1680.618333333333, 2017.7263333333333, 1613.6490000000001, 1984.6893333333335, 1642.3663333333334, 173.65733333333333, 4300.825666666667, 4195.460666666667, 4303.717666666667, 4261.290333333333, 3625.519666666667, 4274.297, 4334.7300000000005, 4408.583333333333, 4279.967666666667, 4486.642333333333, 4433.208333333333, 3939.633333333333, 6658.3550000000005, 6183.790666666667, 3022.745666666667, 3605.1186666666667, 3873.505333333333, 3674.1929999999998, 3629.482666666667, 3710.418333333333, 3429.804666666667, 3854.4480000000003, 9869.700666666666, 10005.345666666666, 4276.748666666666, 4437.158666666667, 4451.993333333333, 4844.6269999999995, 4719.244333333333, 3913.142333333333, 4366.365000000001, 4430.353666666667, 4387.8679999999995, 5560.964666666667, 4438.952666666667, 729.3566666666667, 909.9523333333333, 1111.6833333333334, 913.1733333333333, 1290.3036666666665, 740.6370000000001, 123.37599999999999, 1260.981, 1982.4713333333332, 1994.4976666666664, 1565.2493333333332, 11657.137333333332, 11801.956666666665, 11823.521333333332, 3839.947, 1862.464, 3375.7996666666672, 2370.246, 2365.0769999999998, 2354.6290000000004, 2391.6820000000002, 2350.703, 2058.9616666666666, 1956.6179999999997, 2337.5066666666667, 2185.067666666667, 1692.7046666666668, 83.77766666666666, 1659.5323333333333, 1608.5766666666666, 1812.4386666666667, 1552.5169999999998, 1763.9950000000001, 1744.5306666666665, 1660.823, 10369.873000000001, 10357.336333333335, 2167.983333333333, 2810.9193333333333, 2526.014333333333, 1785.936, 2814.815666666667, 13215.419666666667, 13152.483333333332, 12960.344666666666, 13092.761666666667, 12752.666, 13268.723333333333, 13213.307, 5486.384333333334, 18153.03066666667, 12421.991, 1438.8976666666667, 181.0666666666667, 1534.102, 12258.183666666666, 12257.056666666665, 2336.212666666667, 2389.3763333333336, 2561.014333333333, 2390.974, 4346.212666666667, 3758.8043333333335, 4271.621999999999, 4347.157333333334, 4130.7683333333325, 4234.729, 4193.125666666667, 4245.178333333333, 19366.467666666667, 19625.102333333332, 22429.41633333333, 20110.051666666666, 19196.346, 19048.369000000002, 2249.7443333333335, 2234.3399999999997, 2241.2599999999998, 2196.940666666667, 2210.5243333333333, 2185.3326666666667, 4733.587666666666, 4707.276999999999, 4003.2949999999996, 4445.663333333334, 4675.595333333334, 4740.126333333334, 4606.359666666667, 4190.276333333333, 4782.643, 4700.501, 4766.705, 4613.583666666666, 4805.04, 4640.3043333333335, 1708.8943333333334, 4644.453333333334, 5535.300666666667, 497.11999999999995, 611.5403333333334, 550.2076666666667, 579.6343333333334, 508.1023333333333, 417.06200000000007, 605.5763333333333, 146.84566666666666, 212.0233333333333, 312.97700000000003, 324.3163333333333, 334.78399999999993, 319.27366666666666, 1017.0326666666666, 1124.2803333333331, 1291.9260000000002, 3823.5849999999996, 1303.4383333333333, 1559.3293333333334, 1860.8923333333332, 12602.251666666665, 12728.991333333333, 12148.943, 14763.087666666666, 11745.961666666664, 11799.935333333333, 11658.005333333333, 11484.098333333333, 2782.0409999999997, 2808.2659999999996, 991.6750000000001, 1003.8069999999999, 49.84799999999999, 234.66033333333334, 170.79633333333334, 250.279, 172.705, 270.92, 259.15000000000003, 247.62766666666667, 81537.88633333333, 769.308, 81805.799, 82540.16033333333, 772.3196666666666, 441.31600000000003, 1077118.5843333334, 331469.5476666666, 110723.88766666666, 1181094.9293333334, 1070624.3776666666, 1502.7920000000001, 1503.2446666666667, 6028.155666666667, 5810.076, 6206.108333333334, 1715.0556666666669, 1842.8006666666668, 1848.3316666666667, 2506.711333333333, 3362.301]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utility</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>use_bitmap</th>\n",
       "      <th>relevance_Scan</th>\n",
       "      <th>relevance_Join</th>\n",
       "      <th>relevance_Aggregate</th>\n",
       "      <th>relevance_Sort</th>\n",
       "      <th>query_shape_operator0_on_table0</th>\n",
       "      <th>query_shape_operator1_on_table0</th>\n",
       "      <th>query_shape_operator2_on_table0</th>\n",
       "      <th>...</th>\n",
       "      <th>query_shape_operator0_on_table25</th>\n",
       "      <th>query_shape_operator1_on_table25</th>\n",
       "      <th>query_shape_operator2_on_table25</th>\n",
       "      <th>query_shape_operator3_on_table25</th>\n",
       "      <th>query_shape_operator4_on_table25</th>\n",
       "      <th>index_shape_operator0</th>\n",
       "      <th>index_shape_operator1</th>\n",
       "      <th>index_shape_operator2</th>\n",
       "      <th>index_shape_operator3</th>\n",
       "      <th>index_shape_operator4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2901.467276</td>\n",
       "      <td>562857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.402088</td>\n",
       "      <td>56.402088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901.182714</td>\n",
       "      <td>525494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.172649</td>\n",
       "      <td>60772</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3911.049982</td>\n",
       "      <td>1361424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3709.016622</td>\n",
       "      <td>1393972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>4334.308592</td>\n",
       "      <td>525443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>3914.950787</td>\n",
       "      <td>1324316</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>4330.627794</td>\n",
       "      <td>524140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>1828.820880</td>\n",
       "      <td>373949</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>807.220415</td>\n",
       "      <td>2340225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1711 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          utility  num_pages  use_bitmap  relevance_Scan  relevance_Join  \\\n",
       "0     2901.467276     562857           0               0               0   \n",
       "1      901.182714     525494           0               0               0   \n",
       "2       37.172649      60772           1               0               0   \n",
       "3     3911.049982    1361424           0               0               0   \n",
       "4     3709.016622    1393972           0               0               0   \n",
       "...           ...        ...         ...             ...             ...   \n",
       "1706  4334.308592     525443           0               0               0   \n",
       "1707  3914.950787    1324316           0               0               0   \n",
       "1708  4330.627794     524140           0               0               0   \n",
       "1709  1828.820880     373949           0               0               0   \n",
       "1710   807.220415    2340225           0               0               0   \n",
       "\n",
       "      relevance_Aggregate  relevance_Sort query_shape_operator0_on_table0  \\\n",
       "0               56.402088       56.402088                             NaN   \n",
       "1                0.000000        0.000000                             NaN   \n",
       "2                0.000000        0.000000                             NaN   \n",
       "3                0.000000        0.000000                             NaN   \n",
       "4                0.000000        0.000000                             NaN   \n",
       "...                   ...             ...                             ...   \n",
       "1706             0.000000        0.000000                             NaN   \n",
       "1707             0.000000        0.000000                             NaN   \n",
       "1708             0.000000        0.000000                             NaN   \n",
       "1709             0.000000        0.000000                             NaN   \n",
       "1710             0.000000        0.000000                             NaN   \n",
       "\n",
       "     query_shape_operator1_on_table0 query_shape_operator2_on_table0  ...  \\\n",
       "0                                NaN                             NaN  ...   \n",
       "1                                NaN                             NaN  ...   \n",
       "2                                NaN                             NaN  ...   \n",
       "3                                NaN                             NaN  ...   \n",
       "4                                NaN                             NaN  ...   \n",
       "...                              ...                             ...  ...   \n",
       "1706                             NaN                             NaN  ...   \n",
       "1707                             NaN                             NaN  ...   \n",
       "1708                             NaN                             NaN  ...   \n",
       "1709                             NaN                             NaN  ...   \n",
       "1710                             NaN                             NaN  ...   \n",
       "\n",
       "     query_shape_operator0_on_table25 query_shape_operator1_on_table25  \\\n",
       "0                                 NaN                              NaN   \n",
       "1                                 NaN                              NaN   \n",
       "2                                 NaN                              NaN   \n",
       "3                                 NaN                              NaN   \n",
       "4                                 NaN                              NaN   \n",
       "...                               ...                              ...   \n",
       "1706                              NaN                              NaN   \n",
       "1707                              NaN                              NaN   \n",
       "1708                              NaN                              NaN   \n",
       "1709                              NaN                              NaN   \n",
       "1710                              NaN                              NaN   \n",
       "\n",
       "     query_shape_operator2_on_table25 query_shape_operator3_on_table25  \\\n",
       "0                                 NaN                              NaN   \n",
       "1                                 NaN                              NaN   \n",
       "2                                 NaN                              NaN   \n",
       "3                                 NaN                              NaN   \n",
       "4                                 NaN                              NaN   \n",
       "...                               ...                              ...   \n",
       "1706                              NaN                              NaN   \n",
       "1707                              NaN                              NaN   \n",
       "1708                              NaN                              NaN   \n",
       "1709                              NaN                              NaN   \n",
       "1710                              NaN                              NaN   \n",
       "\n",
       "     query_shape_operator4_on_table25 index_shape_operator0  \\\n",
       "0                                 NaN             Aggregate   \n",
       "1                                 NaN                  Join   \n",
       "2                                 NaN                   NaN   \n",
       "3                                 NaN                  Join   \n",
       "4                                 NaN                   NaN   \n",
       "...                               ...                   ...   \n",
       "1706                              NaN                  Join   \n",
       "1707                              NaN                   NaN   \n",
       "1708                              NaN                  Join   \n",
       "1709                              NaN                  Join   \n",
       "1710                              NaN                   NaN   \n",
       "\n",
       "     index_shape_operator1 index_shape_operator2 index_shape_operator3  \\\n",
       "0                      NaN                   NaN                   NaN   \n",
       "1                      NaN                   NaN                   NaN   \n",
       "2                      NaN                   NaN                   NaN   \n",
       "3                      NaN                   NaN                   NaN   \n",
       "4                      NaN                   NaN                   NaN   \n",
       "...                    ...                   ...                   ...   \n",
       "1706                   NaN                   NaN                   NaN   \n",
       "1707                   NaN                   NaN                   NaN   \n",
       "1708                  Join                   NaN                   NaN   \n",
       "1709                   NaN                   NaN                   NaN   \n",
       "1710                   NaN                   NaN                   NaN   \n",
       "\n",
       "     index_shape_operator4  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "1706                   NaN  \n",
       "1707                   NaN  \n",
       "1708                   NaN  \n",
       "1709                   NaN  \n",
       "1710                   NaN  \n",
       "\n",
       "[1711 rows x 142 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TABLE_NUM = 25\n",
    "def construct_features_and_labels(data, table_dict):\n",
    "    labels = []\n",
    "    feature_columns = [\"utility\", \"num_pages\", \"use_bitmap\"]\n",
    "    operator_relevance_columns = [f\"relevance_{operator}\" for operator in LOGICAL_OPERATORS]\n",
    "    feature_columns.extend(operator_relevance_columns)\n",
    "    query_shape_columns = [f\"query_shape_operator{i}_on_table{j}\" for j in range(MAX_TABLE_NUM+1) for i in range(5)]\n",
    "    feature_columns.extend(query_shape_columns)\n",
    "    index_shape_columns = [f\"index_shape_operator{i}\" for i in range(5)]\n",
    "    feature_columns.extend(index_shape_columns)\n",
    "\n",
    "    single_index_query_ids = [(i,j) for i, entry in enumerate(data) for j, config in enumerate(entry[1]) if len(config) == 1]\n",
    "    features = pd.DataFrame(columns=feature_columns, index=range(len(single_index_query_ids)))\n",
    "\n",
    "    for k, (i,j) in enumerate(single_index_query_ids):\n",
    "        index_configs = data[i][1]\n",
    "        costs = data[i][2]\n",
    "        plans = data[i][3]\n",
    "        \n",
    "        # only consider single index config\n",
    "        index = index_configs[j][0]\n",
    "        labels.append(costs[j])\n",
    "        original_query_plan = plans[0] # no indexed query plan\n",
    "        original_query_cost = costs[0] # no index query cost\n",
    "        indexed_query_plan = plans[j]\n",
    "        utility = estimate_index_utility(index, original_query_plan, indexed_query_plan)/original_query_cost\n",
    "        query_shape, index_shape = extract_shape_of_query_and_index(index, original_query_plan, indexed_query_plan)\n",
    "        table_keys = [table_key for table_key,_ in table_dict.items()]\n",
    "        for table, operator_seq in query_shape.items():\n",
    "            for j, operator in enumerate(operator_seq):\n",
    "                if table in table_keys:\n",
    "                    table_index = table_keys.index(table)\n",
    "                    features.iloc[k][f\"query_shape_operator{j}_on_{table_index}\"] = operator\n",
    "        for j, operator in enumerate(index_shape):\n",
    "            features.iloc[k][f\"index_shape_operator{j}\"] = operator\n",
    "        relevance = evaluate_operator_relevance(index, original_query_plan)\n",
    "        for operator in LOGICAL_OPERATORS:\n",
    "            if operator in relevance: features.iloc[k][f\"relevance_{operator}\"] = sum(relevance[operator])/len(relevance[operator])\n",
    "            else: features.iloc[k][f\"relevance_{operator}\"] = 0\n",
    "        num_pages = get_number_of_pages(indexed_query_plan)\n",
    "        use_bitmap = check_bitmap(indexed_query_plan)\n",
    "        features.iloc[k][\"utility\"] = utility\n",
    "        features.iloc[k][\"num_pages\"] = num_pages\n",
    "        features.iloc[k][\"use_bitmap\"] = int(use_bitmap)\n",
    "        \n",
    "    features[[\"utility\", \"num_pages\"]+operator_relevance_columns] = features[[\"utility\", \"num_pages\"]+operator_relevance_columns].apply(pd.to_numeric)\n",
    "    features[\"use_bitmap\"] = features[\"use_bitmap\"].astype('int')\n",
    "    \n",
    "    return features, labels, single_index_query_ids\n",
    "\n",
    "\n",
    "\n",
    "features_DS_50G, labels_DS_50G, single_index_query_ids_DS_50G = construct_features_and_labels(DS_50G_data, table_dict_DS_50G)\n",
    "features_DS_10G, labels_DS_10G, single_index_query_ids_DS_10G = construct_features_and_labels(DS_10G_data, table_dict_DS_10G)\n",
    "features_H, labels_H, single_index_query_ids_H = construct_features_and_labels(H_data, table_dict_H)\n",
    "# features_DSB, labels_DSB, single_index_query_ids_DSB = construct_features_and_labels(DSB_data, table_dict_DSB)\n",
    "print(\"labels:\\n\", labels_DS_10G)\n",
    "features_DS_10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utility</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>use_bitmap</th>\n",
       "      <th>relevance_Scan</th>\n",
       "      <th>relevance_Join</th>\n",
       "      <th>relevance_Aggregate</th>\n",
       "      <th>relevance_Sort</th>\n",
       "      <th>query_shape_operator0_on_table0</th>\n",
       "      <th>query_shape_operator1_on_table0</th>\n",
       "      <th>query_shape_operator2_on_table0</th>\n",
       "      <th>...</th>\n",
       "      <th>query_shape_operator0_on_table25</th>\n",
       "      <th>query_shape_operator1_on_table25</th>\n",
       "      <th>query_shape_operator2_on_table25</th>\n",
       "      <th>query_shape_operator3_on_table25</th>\n",
       "      <th>query_shape_operator4_on_table25</th>\n",
       "      <th>index_shape_operator0</th>\n",
       "      <th>index_shape_operator1</th>\n",
       "      <th>index_shape_operator2</th>\n",
       "      <th>index_shape_operator3</th>\n",
       "      <th>index_shape_operator4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2993.190102</td>\n",
       "      <td>2808510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2282.666436</td>\n",
       "      <td>2589713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sort</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1588.985611</td>\n",
       "      <td>14223598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2282.254151</td>\n",
       "      <td>2586357</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4500.301597</td>\n",
       "      <td>6781512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>828.058530</td>\n",
       "      <td>11683592</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>2999.280964</td>\n",
       "      <td>2808510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>2764.022450</td>\n",
       "      <td>2590435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sort</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2761.515419</td>\n",
       "      <td>2590424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>2762.959924</td>\n",
       "      <td>2586150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         utility  num_pages  use_bitmap  relevance_Scan  relevance_Join  \\\n",
       "0    2993.190102    2808510           0               0               0   \n",
       "1    2282.666436    2589713           0               0               0   \n",
       "2    1588.985611   14223598           0               0               0   \n",
       "3    2282.254151    2586357           1               0               0   \n",
       "4    4500.301597    6781512           0               0               0   \n",
       "..           ...        ...         ...             ...             ...   \n",
       "707   828.058530   11683592           0               0               0   \n",
       "708  2999.280964    2808510           0               0               0   \n",
       "709  2764.022450    2590435           0               0               0   \n",
       "710  2761.515419    2590424           0               0               0   \n",
       "711  2762.959924    2586150           1               0               0   \n",
       "\n",
       "     relevance_Aggregate  relevance_Sort query_shape_operator0_on_table0  \\\n",
       "0             281.956092      281.956092                             NaN   \n",
       "1               0.002738        0.002738                             NaN   \n",
       "2               0.000000        0.000000                             NaN   \n",
       "3               0.000000        0.000000                             NaN   \n",
       "4               0.000000        0.000000                             NaN   \n",
       "..                   ...             ...                             ...   \n",
       "707             0.000000        0.000000                             NaN   \n",
       "708           281.956092      281.956092                             NaN   \n",
       "709             0.002738        0.002738                             NaN   \n",
       "710             0.000000        0.000000                             NaN   \n",
       "711             0.000000        0.000000                             NaN   \n",
       "\n",
       "    query_shape_operator1_on_table0 query_shape_operator2_on_table0  ...  \\\n",
       "0                               NaN                             NaN  ...   \n",
       "1                               NaN                             NaN  ...   \n",
       "2                               NaN                             NaN  ...   \n",
       "3                               NaN                             NaN  ...   \n",
       "4                               NaN                             NaN  ...   \n",
       "..                              ...                             ...  ...   \n",
       "707                             NaN                             NaN  ...   \n",
       "708                             NaN                             NaN  ...   \n",
       "709                             NaN                             NaN  ...   \n",
       "710                             NaN                             NaN  ...   \n",
       "711                             NaN                             NaN  ...   \n",
       "\n",
       "    query_shape_operator0_on_table25 query_shape_operator1_on_table25  \\\n",
       "0                                NaN                              NaN   \n",
       "1                                NaN                              NaN   \n",
       "2                                NaN                              NaN   \n",
       "3                                NaN                              NaN   \n",
       "4                                NaN                              NaN   \n",
       "..                               ...                              ...   \n",
       "707                              NaN                              NaN   \n",
       "708                              NaN                              NaN   \n",
       "709                              NaN                              NaN   \n",
       "710                              NaN                              NaN   \n",
       "711                              NaN                              NaN   \n",
       "\n",
       "    query_shape_operator2_on_table25 query_shape_operator3_on_table25  \\\n",
       "0                                NaN                              NaN   \n",
       "1                                NaN                              NaN   \n",
       "2                                NaN                              NaN   \n",
       "3                                NaN                              NaN   \n",
       "4                                NaN                              NaN   \n",
       "..                               ...                              ...   \n",
       "707                              NaN                              NaN   \n",
       "708                              NaN                              NaN   \n",
       "709                              NaN                              NaN   \n",
       "710                              NaN                              NaN   \n",
       "711                              NaN                              NaN   \n",
       "\n",
       "    query_shape_operator4_on_table25 index_shape_operator0  \\\n",
       "0                                NaN             Aggregate   \n",
       "1                                NaN                  Sort   \n",
       "2                                NaN                   NaN   \n",
       "3                                NaN                  Join   \n",
       "4                                NaN                  Join   \n",
       "..                               ...                   ...   \n",
       "707                              NaN                   NaN   \n",
       "708                              NaN             Aggregate   \n",
       "709                              NaN                  Sort   \n",
       "710                              NaN                  Join   \n",
       "711                              NaN                  Join   \n",
       "\n",
       "    index_shape_operator1 index_shape_operator2 index_shape_operator3  \\\n",
       "0                     NaN                   NaN                   NaN   \n",
       "1                     NaN                   NaN                   NaN   \n",
       "2                     NaN                   NaN                   NaN   \n",
       "3                     NaN                   NaN                   NaN   \n",
       "4                    Join                   NaN                   NaN   \n",
       "..                    ...                   ...                   ...   \n",
       "707                   NaN                   NaN                   NaN   \n",
       "708                   NaN                   NaN                   NaN   \n",
       "709                   NaN                   NaN                   NaN   \n",
       "710                  Join                   NaN                   NaN   \n",
       "711                   NaN                   NaN                   NaN   \n",
       "\n",
       "    index_shape_operator4  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "..                    ...  \n",
       "707                   NaN  \n",
       "708                   NaN  \n",
       "709                   NaN  \n",
       "710                   NaN  \n",
       "711                   NaN  \n",
       "\n",
       "[712 rows x 142 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_DS_50G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utility</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>use_bitmap</th>\n",
       "      <th>relevance_Scan</th>\n",
       "      <th>relevance_Join</th>\n",
       "      <th>relevance_Aggregate</th>\n",
       "      <th>relevance_Sort</th>\n",
       "      <th>query_shape_operator0_on_table0</th>\n",
       "      <th>query_shape_operator1_on_table0</th>\n",
       "      <th>query_shape_operator2_on_table0</th>\n",
       "      <th>...</th>\n",
       "      <th>query_shape_operator0_on_table25</th>\n",
       "      <th>query_shape_operator1_on_table25</th>\n",
       "      <th>query_shape_operator2_on_table25</th>\n",
       "      <th>query_shape_operator3_on_table25</th>\n",
       "      <th>query_shape_operator4_on_table25</th>\n",
       "      <th>index_shape_operator0</th>\n",
       "      <th>index_shape_operator1</th>\n",
       "      <th>index_shape_operator2</th>\n",
       "      <th>index_shape_operator3</th>\n",
       "      <th>index_shape_operator4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3496.203192</td>\n",
       "      <td>1386656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>963.402543</td>\n",
       "      <td>3136376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2951.076145</td>\n",
       "      <td>1421568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3514.665738</td>\n",
       "      <td>1421615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3166.148023</td>\n",
       "      <td>2051027</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>717.036863</td>\n",
       "      <td>992011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregate</td>\n",
       "      <td>Join</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>3313.075446</td>\n",
       "      <td>1142956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scan</td>\n",
       "      <td>Scan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>834.988675</td>\n",
       "      <td>101765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>3327.060319</td>\n",
       "      <td>1163655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scan</td>\n",
       "      <td>Scan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>3332.541579</td>\n",
       "      <td>1165701</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1401 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          utility  num_pages  use_bitmap  relevance_Scan  relevance_Join  \\\n",
       "0     3496.203192    1386656           0               0               0   \n",
       "1      963.402543    3136376           0               0               0   \n",
       "2     2951.076145    1421568           0               0               0   \n",
       "3     3514.665738    1421615           0               0               0   \n",
       "4     3166.148023    2051027           0               0               0   \n",
       "...           ...        ...         ...             ...             ...   \n",
       "1396   717.036863     992011           0               0               0   \n",
       "1397  3313.075446    1142956           1               0               0   \n",
       "1398   834.988675     101765           0               0               0   \n",
       "1399  3327.060319    1163655           1               0               0   \n",
       "1400  3332.541579    1165701           1               0               0   \n",
       "\n",
       "      relevance_Aggregate  relevance_Sort query_shape_operator0_on_table0  \\\n",
       "0                0.000000        0.000000                             NaN   \n",
       "1                0.000000        0.000000                             NaN   \n",
       "2                0.000000        0.000000                             NaN   \n",
       "3                0.000000        0.000000                             NaN   \n",
       "4                0.000000        0.000000                             NaN   \n",
       "...                   ...             ...                             ...   \n",
       "1396             0.003061        0.003444                             NaN   \n",
       "1397             0.000000        0.000000                             NaN   \n",
       "1398             0.000000        0.000000                             NaN   \n",
       "1399             0.000000        0.000000                             NaN   \n",
       "1400             0.000000        0.000000                             NaN   \n",
       "\n",
       "     query_shape_operator1_on_table0 query_shape_operator2_on_table0  ...  \\\n",
       "0                                NaN                             NaN  ...   \n",
       "1                                NaN                             NaN  ...   \n",
       "2                                NaN                             NaN  ...   \n",
       "3                                NaN                             NaN  ...   \n",
       "4                                NaN                             NaN  ...   \n",
       "...                              ...                             ...  ...   \n",
       "1396                             NaN                             NaN  ...   \n",
       "1397                             NaN                             NaN  ...   \n",
       "1398                             NaN                             NaN  ...   \n",
       "1399                             NaN                             NaN  ...   \n",
       "1400                             NaN                             NaN  ...   \n",
       "\n",
       "     query_shape_operator0_on_table25 query_shape_operator1_on_table25  \\\n",
       "0                                 NaN                              NaN   \n",
       "1                                 NaN                              NaN   \n",
       "2                                 NaN                              NaN   \n",
       "3                                 NaN                              NaN   \n",
       "4                                 NaN                              NaN   \n",
       "...                               ...                              ...   \n",
       "1396                              NaN                              NaN   \n",
       "1397                              NaN                              NaN   \n",
       "1398                              NaN                              NaN   \n",
       "1399                              NaN                              NaN   \n",
       "1400                              NaN                              NaN   \n",
       "\n",
       "     query_shape_operator2_on_table25 query_shape_operator3_on_table25  \\\n",
       "0                                 NaN                              NaN   \n",
       "1                                 NaN                              NaN   \n",
       "2                                 NaN                              NaN   \n",
       "3                                 NaN                              NaN   \n",
       "4                                 NaN                              NaN   \n",
       "...                               ...                              ...   \n",
       "1396                              NaN                              NaN   \n",
       "1397                              NaN                              NaN   \n",
       "1398                              NaN                              NaN   \n",
       "1399                              NaN                              NaN   \n",
       "1400                              NaN                              NaN   \n",
       "\n",
       "     query_shape_operator4_on_table25 index_shape_operator0  \\\n",
       "0                                 NaN                  Join   \n",
       "1                                 NaN                   NaN   \n",
       "2                                 NaN                  Join   \n",
       "3                                 NaN                  Join   \n",
       "4                                 NaN                  Join   \n",
       "...                               ...                   ...   \n",
       "1396                              NaN             Aggregate   \n",
       "1397                              NaN                  Scan   \n",
       "1398                              NaN                   NaN   \n",
       "1399                              NaN                  Scan   \n",
       "1400                              NaN                  Scan   \n",
       "\n",
       "     index_shape_operator1 index_shape_operator2 index_shape_operator3  \\\n",
       "0                      NaN                   NaN                   NaN   \n",
       "1                      NaN                   NaN                   NaN   \n",
       "2                     Join                   NaN                   NaN   \n",
       "3                      NaN                   NaN                   NaN   \n",
       "4                      NaN                   NaN                   NaN   \n",
       "...                    ...                   ...                   ...   \n",
       "1396                  Join                   NaN                   NaN   \n",
       "1397                  Scan                   NaN                   NaN   \n",
       "1398                   NaN                   NaN                   NaN   \n",
       "1399                  Scan                   NaN                   NaN   \n",
       "1400                   NaN                   NaN                   NaN   \n",
       "\n",
       "     index_shape_operator4  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "1396                   NaN  \n",
       "1397                   NaN  \n",
       "1398                   NaN  \n",
       "1399                   NaN  \n",
       "1400                   NaN  \n",
       "\n",
       "[1401 rows x 142 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_DSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_IMDB, labels_IMDB, single_index_query_ids_IMDB = construct_features_and_labels(IMDB_data, table_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot_encoding(features):\n",
    "    features_cat = features.select_dtypes(include=\"object\")\n",
    "    features_num = features.select_dtypes(exclude=\"object\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, categories=[LOGICAL_OPERATORS+[np.NaN] for _ in range(len(features_cat.columns))])\n",
    "    features_cat_encoded = encoder.fit_transform(features_cat)\n",
    "    features_encoded = np.concatenate((features_num.to_numpy(), features_cat_encoded), axis=1)\n",
    "    return features_encoded\n",
    "\n",
    "features_DS_50G_encoded = to_onehot_encoding(features_DS_50G)\n",
    "features_DS_10G_encoded = to_onehot_encoding(features_DS_10G)\n",
    "features_H_encoded = to_onehot_encoding(features_H)\n",
    "# features_DSB_encoded = to_onehot_encoding(features_DSB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_IMDB_encoded = to_onehot_encoding(features_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_DS_50G_encoded.shape, features_DS_10G_encoded.shape, features_H_encoded.shape, features_DSB_encoded.shape, features_IMDB_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set and test set\n",
    "train_size = 0.67\n",
    "\n",
    "X_train_DS_50G, X_test_DS_50G, y_train_DS_50G, y_test_DS_50G = features_DS_50G_encoded[:int(train_size * len(features_DS_50G_encoded))], features_DS_50G_encoded[int(train_size * len(features_DS_50G_encoded)):], labels_DS_50G[:int(train_size * len(features_DS_50G_encoded))], labels_DS_50G[int(train_size * len(features_DS_50G_encoded)):]\n",
    "X_train_DS_10G, X_test_DS_10G, y_train_DS_10G, y_test_DS_10G = features_DS_10G_encoded[:int(train_size * len(features_DS_10G_encoded))], features_DS_10G_encoded[int(train_size * len(features_DS_10G_encoded)):], labels_DS_10G[:int(train_size * len(features_DS_10G_encoded))], labels_DS_10G[int(train_size * len(features_DS_10G_encoded)):]\n",
    "X_train_H, X_test_H, y_train_H, y_test_H = features_H_encoded[:int(train_size * len(features_H_encoded))], features_H_encoded[int(train_size * len(features_H_encoded)):], labels_H[:int(train_size * len(features_H_encoded))], labels_H[int(train_size * len(features_H_encoded)):]\n",
    "# X_train_DSB, X_test_DSB, y_train_DSB, y_test_DSB = features_DSB_encoded[:int(train_size * len(features_DSB_encoded))], features_DSB_encoded[int(train_size * len(features_DSB_encoded)):], labels_DSB[:int(train_size * len(features_DSB_encoded))], labels_DSB[int(train_size * len(features_DSB_encoded)):]\n",
    "X_train_IMDB, X_test_IMDB, y_train_IMDB, y_test_IMDB = features_IMDB_encoded[:int(train_size * len(features_IMDB_encoded))], features_IMDB_encoded[int(train_size * len(features_IMDB_encoded)):], labels_IMDB[:int(train_size * len(features_IMDB_encoded))], labels_IMDB[int(train_size * len(features_IMDB_encoded)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on TPC-DS 50G:\n",
      "1.2992742621852893\n"
     ]
    }
   ],
   "source": [
    "# RF parameters are set according to the paper\n",
    "regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_DS_50G, y_train_DS_50G)\n",
    "y_estimated_DS_50G = regr.predict(X_test_DS_50G)\n",
    "print(f\"Average Q-error for index filter model on TPC-DS 50G:\\n{q_error(y_test_DS_50G, y_estimated_DS_50G)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on TPC-DS 10G:\n",
      "1.5628436991912777\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_DS_10G, y_train_DS_10G)\n",
    "y_estimated_DS_10G = regr.predict(X_test_DS_10G)\n",
    "print(f\"Average Q-error for index filter model on TPC-DS 10G:\\n{q_error(y_test_DS_10G, y_estimated_DS_10G)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on TPC-H:\n",
      "1.0058841766195816\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_H, y_train_H)\n",
    "y_estimated_H = regr.predict(X_test_H)\n",
    "print(f\"Average Q-error for index filter model on TPC-H:\\n{q_error(y_test_H, y_estimated_H)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on DSB:\n",
      "44.92673568909705\n"
     ]
    }
   ],
   "source": [
    "# regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_DSB, y_train_DSB)\n",
    "# y_estimated_DSB = regr.predict(X_test_DSB)\n",
    "# print(f\"Average Q-error for index filter model on DSB:\\n{q_error(y_test_DSB, y_estimated_DSB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on IMDB-JOB:\n",
      "2.012576696695624\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_IMDB, y_train_IMDB)\n",
    "y_estimated_IMDB = regr.predict(X_test_IMDB)\n",
    "print(f\"Average Q-error for index filter model on IMDB-JOB:\\n{q_error(y_test_IMDB, y_estimated_IMDB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "sample_count = 10\n",
    "result_dic = {\n",
    "        \"TPC_DS_10\": [q_error([y_test_DS_10G[i]], [y_estimated_DS_10G[i]]) for i in range(len(y_test_DS_10G))],\n",
    "        \"TPC_DS_50\": [q_error([y_test_DS_50G[i]], [y_estimated_DS_50G[i]]) for i in range(len(y_test_DS_50G))],\n",
    "        \"TPC_H\": [q_error([y_test_H[i]], [y_estimated_H[i]]) for i in range(len(y_test_H))],\n",
    "        \"IMDB_JOB\": [q_error([y_test_IMDB[i]], [y_estimated_IMDB[i]]) for i in range(len(y_test_IMDB))],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data = {\n",
    "    \"TPC_DS_10\": sorted(sample(result_dic[\"TPC_DS_10\"], sample_count)),\n",
    "    \"TPC_DS_50\": sorted(sample(result_dic[\"TPC_DS_50\"], sample_count)),\n",
    "    \"TPC_H\": sorted(sample(result_dic[\"TPC_H\"], sample_count)),\n",
    "    \"IMDB_JOB\": sorted(sample(result_dic[\"IMDB_JOB\"], sample_count))\n",
    "})\n",
    "\n",
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1118134296500657,\n",
       " 1.0797810570841453,\n",
       " 1.0035330036034231,\n",
       " 1.2025045637734864)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics as stats\n",
    "stats.median(result_dic[\"TPC_DS_10\"]), stats.median(result_dic[\"TPC_DS_50\"]), stats.median(result_dic[\"TPC_H\"]), stats.median(result_dic[\"IMDB_JOB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.296473866500982\n",
      "4.0528422159396555\n",
      "1.0348830703471696\n",
      "24.635151760751217\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "percentile = 99\n",
    "print(np.percentile(result_dic[\"TPC_DS_10\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_DS_50\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_H\"], percentile))\n",
    "print(np.percentile(result_dic[\"IMDB_JOB\"], percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-error for index filter model on TPC-DS 50G:\n",
      "1.2992742621852893\n",
      "Average Q-error for index filter model on TPC-DS 10G:\n",
      "488.0495022512642\n",
      "Average Q-error for index filter model on TPC-H:\n",
      "4.264007031404978\n",
      "Average Q-error for index filter model on IMDB-JOB:\n",
      "2.7292499975311957\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=40, max_depth=10, random_state=0).fit(X_train_DS_50G, y_train_DS_50G)\n",
    "y_estimated_DS_50G = regr.predict(X_test_DS_50G)\n",
    "print(f\"Average Q-error for index filter model on TPC-DS 50G:\\n{q_error(y_test_DS_50G, y_estimated_DS_50G)}\")\n",
    "y_estimated_DS_10G = regr.predict(features_DS_10G_encoded)\n",
    "print(f\"Average Q-error for index filter model on TPC-DS 10G:\\n{q_error(labels_DS_10G, y_estimated_DS_10G)}\")\n",
    "y_estimated_H = regr.predict(features_H_encoded)\n",
    "print(f\"Average Q-error for index filter model on TPC-H:\\n{q_error(labels_H, y_estimated_H)}\")\n",
    "y_estimated_IMDB = regr.predict(features_IMDB_encoded)\n",
    "print(f\"Average Q-error for index filter model on IMDB-JOB:\\n{q_error(labels_IMDB, y_estimated_IMDB)}\")\n",
    "# y_estimated_DSB = regr.predict(features_DSB_encoded)\n",
    "# print(f\"Average Q-error for index filter model on DSB:\\n{q_error(labels_DSB, y_estimated_DSB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_count = 10\n",
    "result_dic = {\n",
    "        \"TPC_DS_10\": [q_error([labels_DS_10G[i]], [y_estimated_DS_10G[i]]) for i in range(len(labels_DS_10G))],\n",
    "        \"TPC_DS_50\": [q_error([y_test_DS_50G[i]], [y_estimated_DS_50G[i]]) for i in range(len(y_test_DS_50G))],\n",
    "        \"TPC_H\": [q_error([labels_H[i]], [y_estimated_H[i]]) for i in range(len(labels_H))],\n",
    "        \"IMDB_JOB\": [q_error([labels_IMDB[i]], [y_estimated_IMDB[i]]) for i in range(len(labels_IMDB))],\n",
    "        # \"DSB\": [q_error([labels_DSB[i]], [y_estimated_DSB[i]]) for i in range(len(labels_DSB))],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"TPC_DS_10\": sorted(sample(result_dic[\"TPC_DS_10\"], sample_count)),\n",
    "    \"TPC_DS_50\": sorted(sample(result_dic[\"TPC_DS_50\"], sample_count)),\n",
    "    \"TPC_H\": sorted(sample(result_dic[\"TPC_H\"], sample_count)),\n",
    "    \"IMDB_JOB\": sorted(sample(result_dic[\"IMDB_JOB\"], sample_count)),\n",
    "})\n",
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(actual, pred, threshold):\n",
    "    # >= threshold: reduction in cost is significant and hence should not be filtered out\n",
    "    # < threshold: reduction in cost is insignificant and hence shoud be filtered out\n",
    "    assert(len(actual) == len(pred))\n",
    "    fp_count, fn_count, tp_count, tn_count = 0, 0, 0, 0\n",
    "    total = len(actual)\n",
    "    for i in range(total):\n",
    "        if (actual[i] >= threshold) and (pred[i] < threshold): # should not be filtered out but being filtered out\n",
    "            fp_count += 1\n",
    "        if (actual[i] < threshold) and (pred[i] >= threshold): # should be filtered out but not being filtered out\n",
    "            fn_count += 1\n",
    "        if (actual[i] >= threshold) and (pred[i] >= threshold):\n",
    "            tp_count += 1\n",
    "        if (actual[i] < threshold) and (pred[i] < threshold):\n",
    "            tn_count += 1\n",
    "    assert(fp_count+fn_count+tp_count+tn_count == total)\n",
    "    return tp_count/total, tn_count/total, fp_count/total, fn_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-DS 10G: 0.0, False negative rate on TPC-DS 10G: 0.37168141592920356\n"
     ]
    }
   ],
   "source": [
    "filter_threshold = 0.05\n",
    "\n",
    "def get_percentage_diff(data, flattened_ids, actual, pred):\n",
    "    percentage_diff_list_true, percentage_diff_list_estimated = [], []\n",
    "    test_set = flattened_ids[int(train_size * len(flattened_ids)):]\n",
    "    for k, (i,j) in enumerate(test_set):\n",
    "        costs = data[i][2]\n",
    "        original_query_cost = costs[j]\n",
    "        percentage_diff_estimated = (original_query_cost - pred[k])/ original_query_cost\n",
    "        percentage_diff_true = (original_query_cost - actual[k])/original_query_cost\n",
    "        percentage_diff_list_estimated.append(percentage_diff_estimated)\n",
    "        percentage_diff_list_true.append(percentage_diff_true)\n",
    "    return percentage_diff_list_true, percentage_diff_list_estimated\n",
    "    \n",
    "percentage_diff_list_true_DS_10G, percentage_diff_list_estimated_DS_10G = get_percentage_diff(DS_10G_data, single_index_query_ids_DS_10G, y_test_DS_10G, y_estimated_DS_10G)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DS_10G, percentage_diff_list_estimated_DS_10G, filter_threshold)\n",
    "print(f\"False positive rate on TPC-DS 10G: {false_positive}, False negative rate on TPC-DS 10G: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-DS 50G: 0.30638297872340425, False negative rate on TPC-DS 50G: 0.11914893617021277\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_DS_50G, percentage_diff_list_estimated_DS_50G = get_percentage_diff(DS_50G_data, single_index_query_ids_DS_50G, labels_DS_50G, y_estimated_DS_50G)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DS_50G, percentage_diff_list_estimated_DS_50G, filter_threshold)\n",
    "print(f\"False positive rate on TPC-DS 50G: {false_positive}, False negative rate on TPC-DS 50G: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-H: 0.509719222462203, False negative rate on TPC-H: 0.0\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_H, percentage_diff_list_estimated_H = get_percentage_diff(H_data, single_index_query_ids_H, labels_H, y_estimated_H)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_H, percentage_diff_list_estimated_H, filter_threshold)\n",
    "print(f\"False positive rate on TPC-H: {false_positive}, False negative rate on TPC-H: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on IMDB: 0.18604651162790697, False negative rate on IMDB: 0.06976744186046512\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_IMDB, percentage_diff_list_estimated_IMDB = get_percentage_diff(IMDB_data, single_index_query_ids_IMDB, labels_IMDB, y_estimated_IMDB)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_IMDB, percentage_diff_list_estimated_IMDB, filter_threshold)\n",
    "print(f\"False positive rate on IMDB: {false_positive}, False negative rate on IMDB: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on DSB: 0.5161290322580645, False negative rate on DSB: 0.0967741935483871\n"
     ]
    }
   ],
   "source": [
    "# percentage_diff_list_true_DSB, percentage_diff_list_estimated_DSB = get_percentage_diff(DSB_data, single_index_query_ids_DSB, labels_DSB, y_estimated_DSB)\n",
    "# true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DSB, percentage_diff_list_estimated_DSB, filter_threshold)\n",
    "# print(f\"False positive rate on DSB: {false_positive}, False negative rate on DSB: {false_negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Cost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data_by_template(data, template_range):\n",
    "    templates = {entry[0].nr: [entry] for entry in data if entry[0].nr <= template_range}\n",
    "    for entry in data:\n",
    "        query = entry[0]\n",
    "        if query.nr <= template_range: continue\n",
    "        suffix = int(str(query.nr)[-2:])\n",
    "        templates[suffix].append(entry)\n",
    "    return templates\n",
    "\n",
    "templates_DS_50G = group_data_by_template(DS_50G_data, 99)\n",
    "templates_DS_10G = group_data_by_template(DS_10G_data, 99)\n",
    "templates_H = group_data_by_template(H_data, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{templates_H[4][0][0].nr}: {templates_H[4][0][0].text}\")\n",
    "# print(f\"{templates_H[4][1][0].nr}: {templates_H[4][1][0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C store_sales.ss_net_profit, C store_sales.ss_ext_discount_amt, C store_sales.ss_sales_price, C store_sales.ss_ext_sales_price, C item.i_manufact, C date_dim.d_moy, C item.i_manufact_id]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_parameters_for_tpcds(columns, template_id):\n",
    "    root_path = \"tpcds-kit/query_templates\"\n",
    "    filename = f\"query{template_id}.tpl\"\n",
    "    params, variables = set(), []\n",
    "    with open(os.path.join(root_path, filename)) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if \"define\" in line:\n",
    "                variable = line.split(\" \")[1].split(\"=\")[0]\n",
    "                if \"LIMIT\" in variable: continue\n",
    "                if \"text\" in line:\n",
    "                    for column in columns.values():\n",
    "                            if column.name in line:\n",
    "                                params.add(column)\n",
    "                variables.append(variable)\n",
    "            else:\n",
    "                for var in variables:\n",
    "                    if var in line:\n",
    "                        for column in columns.values():\n",
    "                            if column.name in line:\n",
    "                                params.add(column)\n",
    "    return list(params)\n",
    "\n",
    "TPC_DS_50G_params = get_parameters_for_tpcds(column_dict_DS_50G, 3)\n",
    "print(TPC_DS_50G_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{C orders.o_orderdate, C customer.c_mktsegment, C lineitem.l_shipdate}\n"
     ]
    }
   ],
   "source": [
    "def get_parameters_for_tpch(columns, template_id):\n",
    "    root_path = \"tpch-kit/dbgen/queries\"\n",
    "    filename = f\"{template_id}.sql\"\n",
    "    params = set()\n",
    "    with open(os.path.join(root_path, filename)) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if matched := re.search(r\"\\'?(%?:\\d+%?)+\\'?\", line):\n",
    "                for column in columns.values():\n",
    "                    start = matched.span()[0]\n",
    "                    if (pos := line.find(column.name)) != -1 and pos < start:      \n",
    "                        params.add(column)    \n",
    "    return params  \n",
    "            \n",
    "TPC_H_params = get_parameters_for_tpch(column_dict_H, 3)\n",
    "print(TPC_H_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 1\n",
    "def calculate_parameter_selectivity(dataset_name, template_id, query_plan, column_dict):\n",
    "    get_parameters = None\n",
    "    if re.search(\"TPC_DS\", dataset_name) or re.search(\"DSB\", dataset_name): get_parameters = get_parameters_for_tpcds\n",
    "    elif re.search(\"TPCH\", dataset_name): get_parameters = get_parameters_for_tpch\n",
    "    else: raise ValueError(\"Not supported dataset\")\n",
    "    params = get_parameters(column_dict, template_id)\n",
    "    selectivities = {param: 0 for param in params}\n",
    "    _calculate_parameter_selectivity(params, selectivities, query_plan)\n",
    "    return selectivities\n",
    "\n",
    "def _calculate_parameter_selectivity(params, selectivities, query_plan):\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _calculate_parameter_selectivity(params, selectivities, child_node)\n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        for column in params:\n",
    "            if column.name in condition:\n",
    "                selectivities[column]= query_plan[\"Plan Rows\"]/column.table.row_count\n",
    "\n",
    "# feature 2\n",
    "def evaluate_configuration(column_dict, configuration, query_plan):\n",
    "    configuration_features = {}\n",
    "    _evaluate_configuration(column_dict, configuration_features, configuration, query_plan)\n",
    "    return configuration_features\n",
    "\n",
    "def _evaluate_configuration(column_dict, configuration_features, configuration, query_plan):\n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _evaluate_configuration(column_dict, configuration_features, configuration, child_node)\n",
    "    column_keys = [column_key for column_key,_ in column_dict.items()]          \n",
    "    if (condition := has_filtering_property(query_plan)) != \"\":\n",
    "        for index in configuration:\n",
    "            for j, column in enumerate(index.columns):\n",
    "                if column.name in condition:\n",
    "                    configuration_features[f\"selectivity_{column_keys.index(column.name)}\"] = query_plan[\"Plan Rows\"]/column.table.row_count\n",
    "                    feature = f\"operation_{column_keys.index(column.name)}\"\n",
    "                    if feature not in configuration_features.keys(): configuration_features[feature] = set()\n",
    "                    configuration_features[feature].add(current_operator)\n",
    "                    feature = f\"position_{column_keys.index(column.name)}\"\n",
    "                    if  feature not in configuration_features.keys(): configuration_features[feature] = j\n",
    "                    else: configuration_features[feature] = min(configuration_features[feature], j)\n",
    "    elif is_aggregate_operator(current_operator) and \"Group Key\" in query_plan.keys():\n",
    "        conditions = query_plan[\"Group Key\"]\n",
    "        for index in configuration:\n",
    "            for j, column in enumerate(index.columns):\n",
    "                for condition in conditions:\n",
    "                    if column.name in condition:\n",
    "                        feature = f\"operation_{column_keys.index(column.name)}\"\n",
    "                        if feature not in configuration_features.keys(): configuration_features[feature] = set()\n",
    "                        configuration_features[feature].add(current_operator)\n",
    "                        feature = f\"position_{column_keys.index(column.name)}\"\n",
    "                        if  feature not in configuration_features.keys(): configuration_features[feature] = j\n",
    "                        else: configuration_features[feature] = min(configuration_features[feature], j)\n",
    "    elif is_sort_operator(current_operator) and \"Sort Key\" in query_plan:\n",
    "        conditions = query_plan[\"Sort Key\"]\n",
    "        for index in configuration:\n",
    "            for j, column in enumerate(index.columns):\n",
    "                for condition in conditions:\n",
    "                    if column.name in condition:\n",
    "                        feature = f\"order_{column_keys.index(column.name)}\"\n",
    "                        if \"DESC\" in condition: configuration_features[feature] = \"DESC\"\n",
    "                        else: configuration_features[feature] = \"ASC\" # ascending by default\n",
    "                        feature = f\"position_{column_keys.index(column.name)}\"\n",
    "                        if  feature not in configuration_features.keys(): configuration_features[feature] = j\n",
    "                        else: configuration_features[feature] = min(configuration_features[feature], j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_template_features_and_labels(dataset_name, templates, column_dict):\n",
    "    template_feature_labels = {}\n",
    "    for template_id, data in templates.items():\n",
    "        labels = []\n",
    "        get_parameters = None\n",
    "        if re.search(\"TPC_DS\", dataset_name) or re.search(\"DSB\", dataset_name): get_parameters = get_parameters_for_tpcds\n",
    "        elif re.search(\"TPCH\", dataset_name): get_parameters = get_parameters_for_tpch\n",
    "        else: raise ValueError(\"Not supported dataset\")\n",
    "        params = get_parameters(column_dict, template_id)\n",
    "        feature_columns = []\n",
    "        param_selectivity_columns = [f\"param_selectivity_{param.name}\" for param in params]\n",
    "        feature_columns.extend(param_selectivity_columns)\n",
    "        config_selectivity_columns = list(set(f\"selectivity_{col}\" for col in range(len(column_dict))))\n",
    "        feature_columns.extend(config_selectivity_columns)\n",
    "        config_operation_columns = list(set(f\"operation_{col}_{i}\" for col in range(len(column_dict)) for i in range(3)))\n",
    "        feature_columns.extend(config_operation_columns)\n",
    "        config_position_columns = list(set(f\"position_{col}\" for col in range(len(column_dict))))\n",
    "        feature_columns.extend(config_position_columns)\n",
    "        config_order_columns = list(set(f\"order_{col}\" for col in range(len(column_dict))))\n",
    "        feature_columns.extend(config_order_columns)\n",
    "        query_configs = [(entry[0], config) for entry in data for config in entry[1]]\n",
    "        features = pd.DataFrame(columns=feature_columns, index=range(len(query_configs)))\n",
    "        k = 0\n",
    "        for entry in data:\n",
    "            index_configs = entry[1]\n",
    "            costs = entry[2]\n",
    "            plans = entry[3]\n",
    "            for j, (config, plan) in enumerate(zip(index_configs, plans)):\n",
    "                labels.append(costs[j])\n",
    "                selectivities = calculate_parameter_selectivity(dataset_name, template_id, plan, column_dict)\n",
    "                for param, value in selectivities.items():\n",
    "                    features.iloc[k][f\"param_selectivity_{param.name}\"] = value\n",
    "                config_feature = evaluate_configuration(column_dict, config, plan)\n",
    "                for feature_name, value in config_feature.items():\n",
    "                    if isinstance(value, set):\n",
    "                        for l, operator in enumerate(list(value)):\n",
    "                            features.iloc[k][f\"{feature_name}_{l}\"] = operator\n",
    "                    else:\n",
    "                        features.iloc[k][feature_name] = value\n",
    "                k+=1\n",
    "        features[param_selectivity_columns+config_selectivity_columns] = features[param_selectivity_columns+config_selectivity_columns].apply(pd.to_numeric)\n",
    "        features[config_position_columns] = features[config_position_columns].fillna(value=len(config_position_columns))\n",
    "        features[config_position_columns] = features[config_position_columns].astype('float32')\n",
    "        features[config_selectivity_columns] = features[config_selectivity_columns].fillna(value=1)\n",
    "        template_feature_labels[template_id] = (features,labels)\n",
    "    return template_feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_selectivity_d_year</th>\n",
       "      <th>selectivity_285</th>\n",
       "      <th>selectivity_11</th>\n",
       "      <th>selectivity_202</th>\n",
       "      <th>selectivity_349</th>\n",
       "      <th>selectivity_92</th>\n",
       "      <th>selectivity_7</th>\n",
       "      <th>selectivity_80</th>\n",
       "      <th>selectivity_277</th>\n",
       "      <th>selectivity_399</th>\n",
       "      <th>...</th>\n",
       "      <th>order_187</th>\n",
       "      <th>order_88</th>\n",
       "      <th>order_159</th>\n",
       "      <th>order_337</th>\n",
       "      <th>order_130</th>\n",
       "      <th>order_71</th>\n",
       "      <th>order_216</th>\n",
       "      <th>order_201</th>\n",
       "      <th>order_207</th>\n",
       "      <th>order_418</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_selectivity_d_year  selectivity_285  selectivity_11  selectivity_202  \\\n",
       "0                  0.004997              1.0             1.0              1.0   \n",
       "1                  0.000000              1.0             1.0              1.0   \n",
       "2                  0.004997              1.0             1.0              1.0   \n",
       "3                  0.000000              1.0             1.0              1.0   \n",
       "4                  0.004997              1.0             1.0              1.0   \n",
       "5                  0.000000              1.0             1.0              1.0   \n",
       "6                  0.004997              1.0             1.0              1.0   \n",
       "7                  0.000000              1.0             1.0              1.0   \n",
       "\n",
       "   selectivity_349  selectivity_92  selectivity_7  selectivity_80  \\\n",
       "0              1.0             1.0            1.0             1.0   \n",
       "1              1.0             1.0            1.0             1.0   \n",
       "2              1.0             1.0            1.0             1.0   \n",
       "3              1.0             1.0            1.0             1.0   \n",
       "4              1.0             1.0            1.0             1.0   \n",
       "5              1.0             1.0            1.0             1.0   \n",
       "6              1.0             1.0            1.0             1.0   \n",
       "7              1.0             1.0            1.0             1.0   \n",
       "\n",
       "   selectivity_277  selectivity_399  ...  order_187  order_88  order_159  \\\n",
       "0              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "1              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "2              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "3              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "4              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "5              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "6              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "7              1.0              1.0  ...        NaN       NaN        NaN   \n",
       "\n",
       "   order_337  order_130  order_71  order_216  order_201  order_207  order_418  \n",
       "0        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "1        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "2        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "3        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "4        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "5        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "6        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "7        NaN        NaN       NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[8 rows x 2551 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_feature_labels_DS_50G = construct_template_features_and_labels(\"TPC_DS\", templates_DS_50G, column_dict_DS_50G)\n",
    "template_feature_labels_DS_10G = construct_template_features_and_labels(\"TPC_DS\", templates_DS_10G, column_dict_DS_10G)\n",
    "template_feature_labels_H = construct_template_features_and_labels(\"TPCH\", templates_H, column_dict_H)\n",
    "template_feature_labels_DS_50G[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPC-DS 50G: Average Q-error for all templates (99th percentile): 37.36388721944428\n",
      "TPC-DS 10G: Average Q-error for all templates (99th percentile): 2.464829096945506\n",
      "TPC-H: Average Q-error for all templates (99th percentile): 1.1225457224930666\n"
     ]
    }
   ],
   "source": [
    "def template_feature_modeling_and_prediction(dataset_name, template_feature_labels):\n",
    "    template_regressors, results = {}, {}\n",
    "    for template_id, (features, labels) in template_feature_labels.items():\n",
    "        # convert to one-hot\n",
    "        features_cat = features.select_dtypes(include=\"object\")\n",
    "        features_num = features.select_dtypes(exclude=\"object\")\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        features_cat_encoded = encoder.fit_transform(features_cat)\n",
    "        features_encoded = np.concatenate((features_num.to_numpy(), features_cat_encoded), axis=1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_encoded, labels, test_size=0.33, random_state=2024)\n",
    "        template_regressors[template_id] = RandomForestRegressor(n_estimators=5, max_depth=6, random_state=0).fit(X_train, y_train)\n",
    "        y_estimated = template_regressors[template_id].predict(X_test)\n",
    "        error = q_error(y_test, y_estimated)\n",
    "        results[template_id] = error\n",
    "        \n",
    "    results = sorted(list(results.values()))\n",
    "    avg_all_templates = sum(results)/len(results)\n",
    "    print(f\"{dataset_name}: Average Q-error for all templates (99th percentile): {avg_all_templates}\")\n",
    "    return template_regressors, results\n",
    "    \n",
    "_, results_DS_50G = template_feature_modeling_and_prediction(\"TPC-DS 50G\", template_feature_labels_DS_50G)\n",
    "_, results_DS_10G = template_feature_modeling_and_prediction(\"TPC-DS 10G\", template_feature_labels_DS_10G)\n",
    "_, results_H = template_feature_modeling_and_prediction(\"TPC-H\", template_feature_labels_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340.680867261136\n",
      "52.350713599683345\n",
      "2.0395391857573326\n"
     ]
    }
   ],
   "source": [
    "percentile = 100\n",
    "\n",
    "print(np.percentile(results_DS_50G, percentile))\n",
    "print(np.percentile(results_DS_10G, percentile))\n",
    "print(np.percentile(results_H, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "sample_count = 10\n",
    "\n",
    "df = pd.DataFrame.from_dict({\n",
    "    \"TPC_DS_50\": random.sample(results_DS_50G, sample_count),\n",
    "    \"TPC_DS_10\": random.sample(results_DS_10G, sample_count),\n",
    "    \"TPC_H\": random.sample(results_H, sample_count)\n",
    "})\n",
    "\n",
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def get_metadata_on_dataset(data):\n",
    "    total_sample_count, max_config_len = 0, 0\n",
    "    for entry in data:\n",
    "        total_sample_count += len(entry[1])\n",
    "        for index_config in entry[1]:\n",
    "            max_config_len = max(max_config_len, len(index_config))\n",
    "    return total_sample_count, max_config_len\n",
    "\n",
    "sample_count_DS_10G, max_config_len_DS_10G = get_metadata_on_dataset(DS_10G_data)\n",
    "sample_count_DS_50G, max_config_len_DS_50G = get_metadata_on_dataset(DS_50G_data)\n",
    "sample_count_H, max_config_len_H = get_metadata_on_dataset(H_data)\n",
    "# sample_count_DSB, max_config_len_DSB = get_metadata_on_dataset(DSB_data)\n",
    "print(max_config_len_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TABLE_NUM = 25\n",
    "MAX_COLUMN_NUM = 429\n",
    "MAX_CONFIG_LEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_and_labels_combined(data, table_dict, column_dict):\n",
    "    flattened_ids = []\n",
    "    sample_count, _ = get_metadata_on_dataset(data)\n",
    "    feature_columns, labels = [], []\n",
    "    feature_columns.extend([\"num_pages\", \"use_bitmap\"])\n",
    "    utility_columns = [f\"utility_of_index{i}\"for i in range(MAX_CONFIG_LEN)]\n",
    "    feature_columns.extend(utility_columns)\n",
    "    operator_relevance_columns = [f\"relevance_{operator}_of_index{i}\" for i  in range(MAX_CONFIG_LEN) for operator in LOGICAL_OPERATORS]\n",
    "    feature_columns.extend(operator_relevance_columns)\n",
    "    query_shape_columns = [f\"query_shape_operator{k}_on_table{j}_of_index{i}\" for i in range(MAX_CONFIG_LEN) for j in range(MAX_TABLE_NUM+1) for k in range(5)]\n",
    "    feature_columns.extend(query_shape_columns)\n",
    "    index_shape_columns = [f\"index_shape_operator{k}_of_index{i}\" for i in range(MAX_CONFIG_LEN) for k in range(5)]\n",
    "    feature_columns.extend(index_shape_columns)\n",
    "    \n",
    "    config_selectivity_columns = list(set(f\"selectivity_{col}\" for col in range(MAX_COLUMN_NUM)))\n",
    "    feature_columns.extend(config_selectivity_columns)\n",
    "    config_operation_columns = list(set(f\"operation_{col}_{i}\" for col in range(MAX_COLUMN_NUM) for i in range(3)))\n",
    "    feature_columns.extend(config_operation_columns)\n",
    "    config_position_columns = list(set(f\"position_{col}\" for col in range(MAX_COLUMN_NUM)))\n",
    "    feature_columns.extend(config_position_columns)\n",
    "    config_order_columns = list(set(f\"order_{col}\" for col in range(MAX_COLUMN_NUM)))\n",
    "    feature_columns.extend(config_order_columns)\n",
    "\n",
    "    features = pd.DataFrame(columns=feature_columns, index=range(sample_count))\n",
    "    k = 0\n",
    "    for id, entry in enumerate(data):\n",
    "        index_configs = entry[1]\n",
    "        costs = entry[2]\n",
    "        plans = entry[3]\n",
    "        original_query_plan = plans[0] # no indexed query plan\n",
    "        original_query_cost = costs[0] # no index query cost\n",
    "        for j, index_config in enumerate(index_configs):\n",
    "            labels.append(costs[j])\n",
    "            indexed_query_plan = plans[j]\n",
    "            for i, index in enumerate(index_config):\n",
    "                utility = estimate_index_utility(index, original_query_plan, indexed_query_plan)/original_query_cost\n",
    "                features.iloc[k][f\"utility_of_index{i}\"] = utility\n",
    "                query_shape, index_shape = extract_shape_of_query_and_index(index, original_query_plan, indexed_query_plan)\n",
    "                table_keys = [table_key for table_key,_ in table_dict.items()]\n",
    "                for table, operator_seq in query_shape.items():\n",
    "                    for o, operator in enumerate(operator_seq):\n",
    "                        if table in table_keys:\n",
    "                            table_index = table_keys.index(table)\n",
    "                            features.iloc[k][f\"query_shape_operator{o}_on_{table_index}_of_index{i}\"] = operator\n",
    "                for o, operator in enumerate(index_shape):\n",
    "                    features.iloc[k][f\"index_shape_operator{o}_of_index{i}\"] = operator\n",
    "                relevance = evaluate_operator_relevance(index, original_query_plan)\n",
    "                for operator in LOGICAL_OPERATORS:\n",
    "                    if operator in relevance: features.iloc[k][f\"relevance_{operator}_of_index{i}\"] = sum(relevance[operator])/len(relevance[operator])\n",
    "            num_pages = get_number_of_pages(indexed_query_plan)\n",
    "            use_bitmap = check_bitmap(indexed_query_plan)\n",
    "            features.iloc[k][f\"num_pages\"] = num_pages\n",
    "            features.iloc[k][f\"use_bitmap\"] = int(use_bitmap)\n",
    "            \n",
    "            config_feature = evaluate_configuration(column_dict, index_config, indexed_query_plan)\n",
    "            for feature_name, value in config_feature.items():\n",
    "                if isinstance(value, set):\n",
    "                    for o, operator in enumerate(list(value)):\n",
    "                        features.iloc[k][f\"{feature_name}_{o}\"] = operator\n",
    "                else:\n",
    "                    features.iloc[k][feature_name] = value\n",
    "            flattened_ids.append((id, j))\n",
    "            k+=1\n",
    "\n",
    "    features[[\"num_pages\", \"use_bitmap\"]+utility_columns+operator_relevance_columns+config_selectivity_columns] = features[[\"num_pages\", \"use_bitmap\"]+utility_columns+operator_relevance_columns+config_selectivity_columns].astype('float32')\n",
    "    features[operator_relevance_columns] = features[operator_relevance_columns].fillna(value=0)\n",
    "    features[config_position_columns] = features[config_position_columns].fillna(value=len(config_position_columns))\n",
    "    features[config_position_columns] = features[config_position_columns].astype('float32')\n",
    "    features[config_selectivity_columns] = features[config_selectivity_columns].fillna(value=1)\n",
    "    features[utility_columns] = features[utility_columns].fillna(value=0)\n",
    "    \n",
    "    # onehot\n",
    "    logical_operator_encoder = OneHotEncoder(sparse_output=False, categories=[LOGICAL_OPERATORS+[np.NaN] for _ in range(len(query_shape_columns)+len(index_shape_columns))])\n",
    "    physical_operator_encoder = OneHotEncoder(sparse_output=False, categories=[PHYSICAL_OPERATORS+[np.NaN] for _ in range(len(config_operation_columns))])\n",
    "    order_encoder = OneHotEncoder(sparse_output=False, categories=[[\"ASC\", \"DESC\", np.NaN] for _ in range(len(config_order_columns))])\n",
    "    features_num = features.select_dtypes(exclude=\"object\")\n",
    "    logical_operator_features_encoded = logical_operator_encoder.fit_transform(features[query_shape_columns + index_shape_columns])\n",
    "    physical_operator_features_encoded = physical_operator_encoder.fit_transform(features[config_operation_columns])\n",
    "    order_features_encoded = order_encoder.fit_transform(features[config_order_columns])\n",
    "    features_encoded = np.concatenate((features_num.to_numpy(), logical_operator_features_encoded, physical_operator_features_encoded, order_features_encoded), axis=1)\n",
    "    return features, features_encoded, np.array(labels,dtype='float32'), flattened_ids\n",
    "\n",
    "\n",
    "features_combined_DS_10G, features_combined_DS_10G_encoded, labels_combined_DS_10G, flattened_ids_combined_DS_10G = construct_features_and_labels_combined(DS_10G_data, table_dict_DS_10G, column_dict_DS_10G)\n",
    "print(\"labels:\\n\", labels_combined_DS_10G)\n",
    "features_combined_DS_10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_combined_DS_50G, features_combined_DS_50G_encoded, labels_combined_DS_50G, flattened_ids_combined_DS_50G = construct_features_and_labels_combined(DS_50G_data, table_dict_DS_50G, column_dict_DS_50G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_combined_H, features_combined_H_encoded, labels_combined_H, flattened_ids_combined_H = construct_features_and_labels_combined(H_data, table_dict_H, column_dict_H)\n",
    "features_combined_DSB, features_combined_DSB_encoded, labels_combined_DSB, flattened_ids_combined_DSB = construct_features_and_labels_combined(DSB_data, table_dict_DSB, column_dict_DSB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_combined_JOB, features_combined_JOB_encoded, labels_combined_JOB, flattened_ids_combined_JOB = construct_features_and_labels_combined(IMDB_data, table_dict_IMDB, column_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4726, 3136), (28039, 3136), (4471, 3136), (455, 3136))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_combined_DS_50G.shape, features_combined_DS_10G.shape, features_combined_H.shape, features_combined_DSB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.67\n",
    "\n",
    "X_train_combined_DS_10G, X_test_combined_DS_10G, y_train_combined_DS_10G, y_test_combined_DS_10G = train_test_split(features_combined_DS_10G_encoded, labels_combined_DS_10G, test_size=0.33, random_state=2024)\n",
    "X_train_combined_DS_50G, X_test_combined_DS_50G, y_train_combined_DS_50G, y_test_combined_DS_50G = train_test_split(features_combined_DS_50G_encoded, labels_combined_DS_50G, test_size=0.33, random_state=2024)\n",
    "X_train_combined_H, X_test_combined_H, y_train_combined_H, y_test_combined_H = train_test_split(features_combined_H_encoded, labels_combined_H, test_size=0.33, random_state=2024)\n",
    "X_train_combined_JOB, X_test_combined_JOB, y_train_combined_JOB, y_test_combined_JOB = train_test_split(features_combined_JOB_encoded, labels_combined_JOB, test_size=0.33, random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6287194670527019"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(n_estimators=50, max_depth=23, random_state=0).fit(X_train_combined_DS_50G, y_train_combined_DS_50G)\n",
    "y_estimated_combined_DS_50G = regr.predict(X_test_combined_DS_50G)\n",
    "q_error(y_test_combined_DS_50G, y_estimated_combined_DS_50G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.825654382937871"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regr = RandomForestRegressor(n_estimators=50, max_depth=23, random_state=0).fit(X_train_combined_DS_10G, y_train_combined_DS_10G)\n",
    "y_estimated_combined_DS_10G = regr.predict(X_test_combined_DS_10G)\n",
    "q_error(y_test_combined_DS_10G, y_estimated_combined_DS_10G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.67941698659425"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regr = RandomForestRegressor(n_estimators=50, max_depth=23, random_state=0).fit(X_train_combined_H, y_train_combined_H)\n",
    "y_estimated_combined_H = regr.predict(X_test_combined_H)\n",
    "q_error(y_test_combined_H, y_estimated_combined_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.62584849578142"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regr = RandomForestRegressor(n_estimators=50, max_depth=23, random_state=0).fit(X_train_combined_JOB, y_train_combined_JOB)\n",
    "y_estimated_combined_JOB = regr.predict(X_test_combined_JOB)\n",
    "q_error(y_test_combined_JOB, y_estimated_combined_JOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "sample_count = 10\n",
    "result_dic = {\n",
    "        \"TPC_DS_10\": [q_error([y_test_combined_DS_10G[i]], [y_estimated_combined_DS_10G[i]]) for i in range(len(y_test_combined_DS_10G))],\n",
    "        \"TPC_DS_50\": [q_error([y_test_combined_DS_50G[i]], [y_estimated_combined_DS_50G[i]]) for i in range(len(y_test_combined_DS_50G))],\n",
    "        \"TPC_H\": [q_error([y_test_combined_H[i]], [y_estimated_combined_H[i]]) for i in range(len(y_test_combined_H))],\n",
    "        \"IMDB_JOB\": [q_error([y_test_combined_JOB[i]], [y_estimated_combined_JOB[i]]) for i in range(len(y_test_combined_JOB))],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data = {\n",
    "    \"TPC_DS_10\": sorted(sample(result_dic[\"TPC_DS_10\"], sample_count)),\n",
    "    \"TPC_DS_50\": sorted(sample(result_dic[\"TPC_DS_50\"], sample_count)),\n",
    "    \"TPC_H\": sorted(sample(result_dic[\"TPC_H\"], sample_count)),\n",
    "    \"IMDB_JOB\": sorted(sample(result_dic[\"IMDB_JOB\"], sample_count))\n",
    "})\n",
    "\n",
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335.8080320209354\n",
      "5044.934871795195\n",
      "25.962888212559157\n",
      "42.19263745964304\n"
     ]
    }
   ],
   "source": [
    "percentile = 100\n",
    "print(np.percentile(result_dic[\"TPC_DS_50\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_DS_10\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_H\"], percentile))\n",
    "print(np.percentile(result_dic[\"IMDB_JOB\"], percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-DS 10G: 0.013184913001188804, False negative rate on TPC-DS 10G: 0.010050794336971793\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_DS_10G, percentage_diff_list_estimated_DS_10G = get_percentage_diff(DS_10G_data, flattened_ids_combined_DS_10G, y_test_combined_DS_10G, y_estimated_combined_DS_10G)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DS_10G, percentage_diff_list_estimated_DS_10G, filter_threshold)\n",
    "print(f\"False positive rate on TPC-DS 10G: {false_positive}, False negative rate on TPC-DS 10G: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-DS 50G: 0.025, False negative rate on TPC-DS 50G: 0.28012820512820513\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_DS_50G, percentage_diff_list_estimated_DS_50G = get_percentage_diff(DS_50G_data, flattened_ids_combined_DS_50G, labels_combined_DS_50G, y_estimated_combined_DS_50G)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DS_50G, percentage_diff_list_estimated_DS_50G, filter_threshold)\n",
    "print(f\"False positive rate on TPC-DS 50G: {false_positive}, False negative rate on TPC-DS 50G: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on TPC-H: 0.08536585365853659, False negative rate on TPC-H: 0.08197831978319783\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_H, percentage_diff_list_estimated_H = get_percentage_diff(H_data, flattened_ids_combined_H, labels_combined_H, y_estimated_combined_H)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_H, percentage_diff_list_estimated_H, filter_threshold)\n",
    "print(f\"False positive rate on TPC-H: {false_positive}, False negative rate on TPC-H: {false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate on DSB: 0.11920529801324503, False negative rate on DSB: 0.06622516556291391\n"
     ]
    }
   ],
   "source": [
    "percentage_diff_list_true_DSB, percentage_diff_list_estimated_DSB = get_percentage_diff(DSB_data, flattened_ids_combined_DSB, labels_combined_DSB, y_estimated_combined_DSB)\n",
    "true_positive, true_negative, false_positive, false_negative = precision_score(percentage_diff_list_true_DSB, percentage_diff_list_estimated_DSB, filter_threshold)\n",
    "print(f\"False positive rate on DSB: {false_positive}, False negative rate on DSB: {false_negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.mse_loss(input.view(-1), target.view(-1), reduction='mean')\n",
    "    \n",
    "    \n",
    "class QLoss(nn.Module):\n",
    "    def __init__(self, weight=None, min_val=1e-5, penalty_negative=1e5):\n",
    "        self.min_val = min_val\n",
    "        self.penalty_negative = penalty_negative\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        qerror = []\n",
    "        for i in range(len(target)):\n",
    "            q_err = max(target[i]/input[i], input[i]/target[i])\n",
    "            qerror.append(q_err)\n",
    "        return torch.mean(torch.cat(qerror))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, 32)\n",
    "        self.output_fc = nn.Linear(32, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        y_pred = self.output_fc(h_1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.op_encoders = nn.ModuleList(nn.Linear(6,1) for _ in PHYSICAL_OPERATORS)\n",
    "        self.fc = MLP(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = None\n",
    "        for op, input in x[\"op_features\"].items():\n",
    "            id = PHYSICAL_OPERATORS.index(op)\n",
    "            if hidden == None: hidden = self.op_encoders[id](input)\n",
    "            else: hidden = torch.cat((hidden, self.op_encoders[id](input)), 1)\n",
    "\n",
    "        hidden = torch.cat((F.leaky_relu(hidden), x[\"index_features\"], x[\"table_stats\"]),1)\n",
    "        return self.fc(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_operator_feature(features, query_plan):\n",
    "    if has_child_node(query_plan):\n",
    "        for child_node in query_plan[\"Plans\"]:\n",
    "            _extract_operator_feature(features, child_node)\n",
    "            \n",
    "    current_operator = query_plan[\"Node Type\"]\n",
    "    op_feature = features[current_operator]\n",
    "    op_feature[\"actual_rows\"].append(query_plan[\"Actual Rows\"])\n",
    "    op_feature[\"actual_loops\"].append(query_plan[\"Actual Loops\"])\n",
    "    op_feature[\"plan_rows\"].append(query_plan[\"Plan Rows\"])\n",
    "    op_feature[\"plan_width\"].append(query_plan[\"Plan Width\"])\n",
    "    op_feature[\"cost\"].append(query_plan[\"Total Cost\"] - query_plan[\"Startup Cost\"])\n",
    "    op_feature[\"actual_time\"].append(query_plan[\"Actual Total Time\"] - query_plan[\"Actual Startup Time\"])\n",
    "\n",
    "def extract_operator_features(query_plan):\n",
    "    result = {op: {\"actual_rows\": [], \"actual_loops\": [], \"plan_rows\": [], \"plan_width\": [], \"cost\": [], \"actual_time\": []} for op in PHYSICAL_OPERATORS}\n",
    "    _extract_operator_feature(result, query_plan)\n",
    "    for op, v in result.items():\n",
    "        result[op] = np.array([sum(v[\"actual_rows\"])/len(v[\"actual_rows\"]) if len(v[\"actual_rows\"]) > 0 else 0, \n",
    "            sum(v[\"actual_loops\"])/len(v[\"actual_loops\"]) if len(v[\"actual_loops\"]) > 0 else 0,\n",
    "            sum(v[\"plan_rows\"])/len(v[\"plan_rows\"]) if len(v[\"plan_rows\"]) > 0 else 0,\n",
    "            sum(v[\"plan_width\"])/len(v[\"plan_width\"]) if len(v[\"plan_width\"]) > 0 else 0,\n",
    "            sum(v[\"cost\"])/len(v[\"cost\"]) if len(v[\"cost\"]) > 0 else 0,\n",
    "            sum(v[\"actual_time\"])/len(v[\"actual_time\"]) if len(v[\"actual_time\"]) > 0 else 0,\n",
    "        ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "web_sales\n"
     ]
    }
   ],
   "source": [
    "max_column_num_per_table, table_name = 0, None\n",
    "for _, table in table_dict_DS_50G.items():\n",
    "    if max_column_num_per_table < len(table.columns):\n",
    "        max_column_num_per_table = len(table.columns)\n",
    "        table_name = table.name\n",
    "for _, table in table_dict_H.items():\n",
    "    if max_column_num_per_table < len(table.columns):\n",
    "        max_column_num_per_table = len(table.columns)\n",
    "        table_name = table.name\n",
    "for _, table in table_dict_IMDB.items():\n",
    "    if max_column_num_per_table < len(table.columns):\n",
    "        max_column_num_per_table = len(table.columns)\n",
    "        table_name = table.name\n",
    "print(max_column_num_per_table)\n",
    "print(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_table_features(data, table_dict):\n",
    "    table_num = 25\n",
    "    column_per_table = 34\n",
    "    keys = [f\"row_count_{i}\" for i in range(table_num)]\n",
    "    keys.extend([f\"n_distinct_{i}_{j}\" for i in range(table_num) for j in range(column_per_table)])\n",
    "    keys.extend([f\"null_frac_{i}_{j}\" for i in range(table_num) for j in range(column_per_table)])\n",
    "    table_stats = {k: 0 for k in keys}\n",
    "    table_names = sorted(list(table_dict.keys()))\n",
    "    table_list = list(table_dict[k] for k in table_names)\n",
    "    for i, table in enumerate(table_list):\n",
    "        table_stats[f\"row_count_{i}\"] = table.row_count\n",
    "        for j, column in enumerate(table.columns):\n",
    "            table_stats[f\"n_distinct_{i}_{j}\"] = column.cardinality\n",
    "            table_stats[f\"null_frac_{i}_{j}\"] = column.null_frac\n",
    "    sample_count, _ = get_metadata_on_dataset(data)\n",
    "    for k, v in table_stats.items():\n",
    "        table_stats[k] = [v for _ in range(sample_count)]\n",
    "    table_stats = pd.DataFrame.from_dict(table_stats)\n",
    "    return table_stats\n",
    "\n",
    "def get_metadata_on_dataset(data):\n",
    "    total_sample_count, max_config_len = 0, 0\n",
    "    for entry in data:\n",
    "        total_sample_count += len(entry[1])\n",
    "        for index_config in entry[1]:\n",
    "            max_config_len = max(max_config_len, len(index_config))\n",
    "    return total_sample_count, max_config_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TABLE_NUM = 25\n",
    "MAX_COLUMN_NUM = 429\n",
    "MAX_CONFIG_LEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_operator_index_features(data, table_dict, column_dict):\n",
    "    sample_count, _ = get_metadata_on_dataset(data)\n",
    "    index_feature_columns, labels = [], []\n",
    "    operator_relevance_columns = [f\"relevance_{operator}_of_index{i}\" for i  in range(MAX_CONFIG_LEN) for operator in LOGICAL_OPERATORS]\n",
    "    index_feature_columns.extend(operator_relevance_columns)\n",
    "    query_shape_columns = [f\"query_shape_operator{k}_on_table{j}_of_index{i}\" for i in range(MAX_CONFIG_LEN) for j in range(MAX_TABLE_NUM+1) for k in range(5)]\n",
    "    index_feature_columns.extend(query_shape_columns)\n",
    "    index_shape_columns = [f\"index_shape_operator{k}_of_index{i}\" for i in range(MAX_CONFIG_LEN) for k in range(5)]\n",
    "    index_feature_columns.extend(index_shape_columns)\n",
    "    \n",
    "    config_operation_columns = list(set(f\"operation_{col}_{i}\" for col in range(MAX_COLUMN_NUM) for i in range(3)))\n",
    "    index_feature_columns.extend(config_operation_columns)\n",
    "    config_position_columns = list(set(f\"position_{col}\" for col in range(MAX_COLUMN_NUM)))\n",
    "    index_feature_columns.extend(config_position_columns)\n",
    "    config_order_columns = list(set(f\"order_{col}\" for col in range(MAX_COLUMN_NUM)))\n",
    "    index_feature_columns.extend(config_order_columns)\n",
    "    \n",
    "    operator_features = []\n",
    "    index_features = pd.DataFrame(columns=index_feature_columns, index=range(sample_count))\n",
    "    k = 0\n",
    "    for entry in data:\n",
    "        index_configs = entry[1]\n",
    "        costs = entry[2]\n",
    "        plans = entry[3]\n",
    "        original_query_plan = plans[0] # no indexed query plan\n",
    "        original_query_cost = costs[0] # no index query cost\n",
    "        for j, index_config in enumerate(index_configs):\n",
    "            labels.append(costs[j])\n",
    "            indexed_query_plan = plans[j]\n",
    "            for i, index in enumerate(index_config):\n",
    "                query_shape, index_shape = extract_shape_of_query_and_index(index, original_query_plan, indexed_query_plan)\n",
    "                table_keys = [table_key for table_key,_ in table_dict.items()]\n",
    "                for table, operator_seq in query_shape.items():\n",
    "                    for o, operator in enumerate(operator_seq):\n",
    "                        if table in table_keys:\n",
    "                            table_index = table_keys.index(table)\n",
    "                            index_features.iloc[k][f\"query_shape_operator{o}_on_{table_index}_of_index{i}\"] = operator\n",
    "                for o, operator in enumerate(index_shape):\n",
    "                    index_features.iloc[k][f\"index_shape_operator{o}_of_index{i}\"] = operator\n",
    "                relevance = evaluate_operator_relevance(index, original_query_plan)\n",
    "                for operator in LOGICAL_OPERATORS:\n",
    "                    if operator in relevance: index_features.iloc[k][f\"relevance_{operator}_of_index{i}\"] = sum(relevance[operator])/len(relevance[operator])\n",
    "            \n",
    "            config_feature = evaluate_configuration(column_dict, index_config, indexed_query_plan)\n",
    "            for feature_name, value in config_feature.items():\n",
    "                if isinstance(value, set):\n",
    "                    for o, operator in enumerate(list(value)):\n",
    "                        index_features.iloc[k][f\"{feature_name}_{o}\"] = operator\n",
    "                else:\n",
    "                    index_features.iloc[k][feature_name] = value\n",
    "            operator_features.append(extract_operator_features(indexed_query_plan))\n",
    "            k+=1\n",
    "\n",
    "    index_features[operator_relevance_columns] = index_features[operator_relevance_columns].fillna(value=0)\n",
    "    index_features[config_position_columns] = index_features[config_position_columns].fillna(value=len(config_position_columns))\n",
    "    \n",
    "    # onehot\n",
    "    logical_operator_encoder = OneHotEncoder(sparse_output=False, categories=[LOGICAL_OPERATORS+[np.NaN] for _ in range(len(query_shape_columns)+len(index_shape_columns))])\n",
    "    physical_operator_encoder = OneHotEncoder(sparse_output=False, categories=[PHYSICAL_OPERATORS+[np.NaN] for _ in range(len(config_operation_columns))])\n",
    "    order_encoder = OneHotEncoder(sparse_output=False, categories=[[\"ASC\", \"DESC\", np.NaN] for _ in range(len(config_order_columns))])\n",
    "    features_num = index_features.select_dtypes(exclude=\"object\")\n",
    "    logical_operator_features_encoded = logical_operator_encoder.fit_transform(index_features[query_shape_columns + index_shape_columns])\n",
    "    physical_operator_features_encoded = physical_operator_encoder.fit_transform(index_features[config_operation_columns])\n",
    "    order_features_encoded = order_encoder.fit_transform(index_features[config_order_columns])\n",
    "    index_features_encoded = np.concatenate((features_num.to_numpy(), logical_operator_features_encoded, physical_operator_features_encoded, order_features_encoded), axis=1)\n",
    "    return index_features, index_features_encoded, operator_features, np.array(labels,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      " [18838.602 19060.65  91201.34  ... 39908.49  40009.418 39667.855]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_Scan_of_index0</th>\n",
       "      <th>relevance_Join_of_index0</th>\n",
       "      <th>relevance_Aggregate_of_index0</th>\n",
       "      <th>relevance_Sort_of_index0</th>\n",
       "      <th>relevance_Scan_of_index1</th>\n",
       "      <th>relevance_Join_of_index1</th>\n",
       "      <th>relevance_Aggregate_of_index1</th>\n",
       "      <th>relevance_Sort_of_index1</th>\n",
       "      <th>relevance_Scan_of_index2</th>\n",
       "      <th>relevance_Join_of_index2</th>\n",
       "      <th>...</th>\n",
       "      <th>order_177</th>\n",
       "      <th>order_45</th>\n",
       "      <th>order_220</th>\n",
       "      <th>order_158</th>\n",
       "      <th>order_290</th>\n",
       "      <th>order_93</th>\n",
       "      <th>order_40</th>\n",
       "      <th>order_263</th>\n",
       "      <th>order_103</th>\n",
       "      <th>order_61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>281.956092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4726 rows × 2701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      relevance_Scan_of_index0  relevance_Join_of_index0  \\\n",
       "0                            0                         0   \n",
       "1                            0                         0   \n",
       "2                            0                         0   \n",
       "3                            0                         0   \n",
       "4                            0                         0   \n",
       "...                        ...                       ...   \n",
       "4721                         0                         0   \n",
       "4722                         0                         0   \n",
       "4723                         0                         0   \n",
       "4724                         0                         0   \n",
       "4725                         0                         0   \n",
       "\n",
       "      relevance_Aggregate_of_index0  relevance_Sort_of_index0  \\\n",
       "0                          0.000000                  0.000000   \n",
       "1                        281.956092                281.956092   \n",
       "2                          0.000000                  0.000000   \n",
       "3                          0.000000                  0.000000   \n",
       "4                          0.000000                  0.000000   \n",
       "...                             ...                       ...   \n",
       "4721                       0.000000                  0.000000   \n",
       "4722                       0.002738                  0.002738   \n",
       "4723                       0.000000                  0.000000   \n",
       "4724                       0.000000                  0.000000   \n",
       "4725                       0.000000                  0.000000   \n",
       "\n",
       "      relevance_Scan_of_index1  relevance_Join_of_index1  \\\n",
       "0                            0                         0   \n",
       "1                            0                         0   \n",
       "2                            0                         0   \n",
       "3                            0                         0   \n",
       "4                            0                         0   \n",
       "...                        ...                       ...   \n",
       "4721                         0                         0   \n",
       "4722                         0                         0   \n",
       "4723                         0                         0   \n",
       "4724                         0                         0   \n",
       "4725                         0                         0   \n",
       "\n",
       "      relevance_Aggregate_of_index1  relevance_Sort_of_index1  \\\n",
       "0                          0.000000                  0.000000   \n",
       "1                          0.000000                  0.000000   \n",
       "2                          0.000000                  0.000000   \n",
       "3                          0.002738                  0.002738   \n",
       "4                          0.000000                  0.000000   \n",
       "...                             ...                       ...   \n",
       "4721                       0.000000                  0.000000   \n",
       "4722                       0.000000                  0.000000   \n",
       "4723                       0.002738                  0.002738   \n",
       "4724                       0.000000                  0.000000   \n",
       "4725                       0.000000                  0.000000   \n",
       "\n",
       "      relevance_Scan_of_index2  relevance_Join_of_index2  ...  order_177  \\\n",
       "0                            0                         0  ...        NaN   \n",
       "1                            0                         0  ...        NaN   \n",
       "2                            0                         0  ...        NaN   \n",
       "3                            0                         0  ...        NaN   \n",
       "4                            0                         0  ...        NaN   \n",
       "...                        ...                       ...  ...        ...   \n",
       "4721                         0                         0  ...        NaN   \n",
       "4722                         0                         0  ...        NaN   \n",
       "4723                         0                         0  ...        NaN   \n",
       "4724                         0                         0  ...        NaN   \n",
       "4725                         0                         0  ...        NaN   \n",
       "\n",
       "      order_45  order_220  order_158  order_290  order_93 order_40 order_263  \\\n",
       "0          NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "1          NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "2          NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "3          NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "4          NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "...        ...        ...        ...        ...       ...      ...       ...   \n",
       "4721       NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "4722       NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "4723       NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "4724       NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "4725       NaN        NaN        NaN        NaN       NaN      NaN       NaN   \n",
       "\n",
       "     order_103 order_61  \n",
       "0          NaN      NaN  \n",
       "1          NaN      ASC  \n",
       "2          NaN      NaN  \n",
       "3          NaN      NaN  \n",
       "4          NaN      NaN  \n",
       "...        ...      ...  \n",
       "4721       NaN      NaN  \n",
       "4722       NaN      NaN  \n",
       "4723       NaN      NaN  \n",
       "4724       NaN      NaN  \n",
       "4725       NaN      NaN  \n",
       "\n",
       "[4726 rows x 2701 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_features_DS_50G, index_features_encoded_DS_50G, operator_features_DS_50G, labels_DS_50G = construct_operator_index_features(DS_50G_data, table_dict_DS_50G, column_dict_DS_50G)\n",
    "print(\"labels:\\n\", labels_DS_50G)\n",
    "index_features_DS_50G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features_DS_10G, index_features_encoded_DS_10G, operator_features_DS_10G, labels_DS_10G = construct_operator_index_features(DS_10G_data, table_dict_DS_10G, column_dict_DS_10G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features_H, index_features_encoded_H, operator_features_H, labels_H = construct_operator_index_features(H_data, table_dict_H, column_dict_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_features_JOB, index_features_encoded_JOB, operator_features_JOB, labels_JOB = construct_operator_index_features(IMDB_data, table_dict_IMDB, column_dict_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4471, 41755), (1138, 41755), (4726, 41755))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_features_encoded_H.shape, index_features_encoded_JOB.shape, index_features_encoded_DS_50G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count_0</th>\n",
       "      <th>row_count_1</th>\n",
       "      <th>row_count_2</th>\n",
       "      <th>row_count_3</th>\n",
       "      <th>row_count_4</th>\n",
       "      <th>row_count_5</th>\n",
       "      <th>row_count_6</th>\n",
       "      <th>row_count_7</th>\n",
       "      <th>row_count_8</th>\n",
       "      <th>row_count_9</th>\n",
       "      <th>...</th>\n",
       "      <th>null_frac_24_24</th>\n",
       "      <th>null_frac_24_25</th>\n",
       "      <th>null_frac_24_26</th>\n",
       "      <th>null_frac_24_27</th>\n",
       "      <th>null_frac_24_28</th>\n",
       "      <th>null_frac_24_29</th>\n",
       "      <th>null_frac_24_30</th>\n",
       "      <th>null_frac_24_31</th>\n",
       "      <th>null_frac_24_32</th>\n",
       "      <th>null_frac_24_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4724</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11718.0</td>\n",
       "      <td>7197954.0</td>\n",
       "      <td>71997670.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>383000.0</td>\n",
       "      <td>1920800.0</td>\n",
       "      <td>73049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4726 rows × 1725 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_count_0  row_count_1  row_count_2  row_count_3  row_count_4  \\\n",
       "0             8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "1             8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "2             8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "3             8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "4             8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "4721          8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "4722          8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "4723          8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "4724          8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "4725          8.0      11718.0    7197954.0   71997670.0     766000.0   \n",
       "\n",
       "      row_count_5  row_count_6  row_count_7  row_count_8  row_count_9  ...  \\\n",
       "0        383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "1        383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "2        383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "3        383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "4        383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "4721     383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "4722     383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "4723     383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "4724     383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "4725     383000.0    1920800.0      73049.0          1.0       7200.0  ...   \n",
       "\n",
       "      null_frac_24_24  null_frac_24_25  null_frac_24_26  null_frac_24_27  \\\n",
       "0                 0.0              0.0                0                0   \n",
       "1                 0.0              0.0                0                0   \n",
       "2                 0.0              0.0                0                0   \n",
       "3                 0.0              0.0                0                0   \n",
       "4                 0.0              0.0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "4721              0.0              0.0                0                0   \n",
       "4722              0.0              0.0                0                0   \n",
       "4723              0.0              0.0                0                0   \n",
       "4724              0.0              0.0                0                0   \n",
       "4725              0.0              0.0                0                0   \n",
       "\n",
       "      null_frac_24_28  null_frac_24_29  null_frac_24_30  null_frac_24_31  \\\n",
       "0                   0                0                0                0   \n",
       "1                   0                0                0                0   \n",
       "2                   0                0                0                0   \n",
       "3                   0                0                0                0   \n",
       "4                   0                0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "4721                0                0                0                0   \n",
       "4722                0                0                0                0   \n",
       "4723                0                0                0                0   \n",
       "4724                0                0                0                0   \n",
       "4725                0                0                0                0   \n",
       "\n",
       "      null_frac_24_32  null_frac_24_33  \n",
       "0                   0                0  \n",
       "1                   0                0  \n",
       "2                   0                0  \n",
       "3                   0                0  \n",
       "4                   0                0  \n",
       "...               ...              ...  \n",
       "4721                0                0  \n",
       "4722                0                0  \n",
       "4723                0                0  \n",
       "4724                0                0  \n",
       "4725                0                0  \n",
       "\n",
       "[4726 rows x 1725 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_stats_DS_50G = construct_table_features(DS_50G_data, table_dict_DS_50G)\n",
    "table_stats_DS_10G = construct_table_features(DS_10G_data, table_dict_DS_10G)\n",
    "table_stats_H = construct_table_features(H_data, table_dict_H)\n",
    "table_stats_JOB = construct_table_features(IMDB_data, table_dict_IMDB)\n",
    "table_stats_DS_50G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38374, 1725)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_stats_DS_50G.shape, table_stats_DS_10G.shape, table_stats_H.shape, table_stats_JOB.shape\n",
    "table_stats = pd.concat([table_stats_DS_50G, table_stats_DS_10G, table_stats_H, table_stats_JOB], axis=0)\n",
    "table_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38374"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator_features = operator_features_DS_50G + operator_features_DS_10G + operator_features_H + operator_features_JOB\n",
    "len(operator_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38374, 41755)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_features_encoded_DS_50G.shape, index_features_encoded_DS_10G.shape, index_features_encoded_H.shape, index_features_encoded_JOB.shape\n",
    "index_features = np.concatenate((index_features_encoded_DS_50G, index_features_encoded_DS_10G, index_features_encoded_H, index_features_encoded_JOB), axis=0)\n",
    "index_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38374"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.concatenate((labels_DS_50G,labels_DS_10G,labels_H,labels_JOB), axis=0)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, operator_features, index_features, table_stats, labels):\n",
    "        self.operator_features = np.array(operator_features)\n",
    "        self.index_features = index_features\n",
    "        self.table_stats = table_stats.to_numpy()\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return {\"op_features\": self.operator_features[index], \"index_features\": self.index_features[index], \"table_stats\": self.table_stats[index]}, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def create_dataset(operator_features, index_features, table_stats, labels):\n",
    "    dataset = CustomDataset(operator_features, index_features, table_stats, labels)\n",
    "    batch_size = 64\n",
    "    validation_split = .3\n",
    "    shuffle_dataset = True\n",
    "    random_seed = 2024\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(PHYSICAL_OPERATORS) + len(index_features_encoded_DS_50G[0]) + len(table_stats_DS_50G.columns)\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# model = LinearModel(INPUT_DIM, OUTPUT_DIM).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0000005)\n",
    "# criterion = MSELoss()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# path = \"saved_model.pt\"\n",
    "                      \n",
    "# num_epochs = 10000\n",
    "# for epoch in range(num_epochs):\n",
    "#     losses = []\n",
    "#     for batch_num, (batch_input, batch_label) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         for op, v in batch_input[\"op_features\"].items():\n",
    "#             batch_input[\"op_features\"][op] = v.to(device, torch.float32)\n",
    "#         batch_input[\"index_features\"] = batch_input[\"index_features\"].to(device, torch.float32)\n",
    "#         batch_input[\"table_stats\"] = batch_input[\"table_stats\"].to(device, torch.float32)\n",
    "        \n",
    "#         batch_label = batch_label.to(device, torch.float32)\n",
    "#         output = model(batch_input)\n",
    "#         loss = criterion(output, batch_label)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch_num % 40 == 0:\n",
    "#             print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "#     torch.save(model, path)\n",
    "#     print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (op_encoders): ModuleList(\n",
       "    (0-27): 28 x Linear(in_features=6, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): MLP(\n",
       "    (input_fc): Linear(in_features=43508, out_features=32, bias=True)\n",
       "    (output_fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearModel(INPUT_DIM, OUTPUT_DIM).to(device)\n",
    "model.load_state_dict(torch.load(\"saved_model_stat_dict_copy.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, validation_loader_DS_50G = create_dataset(operator_features_DS_50G, index_features_encoded_DS_50G, table_stats_DS_50G, labels_DS_50G)\n",
    "_, validation_loader_DS_10G = create_dataset(operator_features_DS_10G, index_features_encoded_DS_10G, table_stats_DS_10G, labels_DS_10G)\n",
    "_, validation_loader_H = create_dataset(operator_features_H, index_features_encoded_H, table_stats_H, labels_H)\n",
    "_, validation_loader_JOB = create_dataset(operator_features_JOB, index_features_encoded_JOB, table_stats_JOB, labels_JOB)\n",
    "\n",
    "def validate(model, validation_loader):\n",
    "    estimates, actuals = [], []\n",
    "    for _, (batch_input, batch_label) in enumerate(validation_loader):\n",
    "        for op, v in batch_input[\"op_features\"].items():\n",
    "            batch_input[\"op_features\"][op] = v.to(device, torch.float32)\n",
    "        batch_input[\"index_features\"] = batch_input[\"index_features\"].to(device, torch.float32)\n",
    "        batch_input[\"table_stats\"] = batch_input[\"table_stats\"].to(device, torch.float32)\n",
    "        batch_estimates = model(batch_input).cpu().detach().numpy().reshape(-1)\n",
    "        batch_label = batch_label.numpy()\n",
    "        estimates.extend(list(batch_estimates))\n",
    "        actuals.extend(list(batch_label))\n",
    "        # q_errors.append(q_error(batch_label, batch_estimates))\n",
    "    return estimates, actuals\n",
    "\n",
    "estimates_DS_50G, actuals_DS_50G = validate(model, validation_loader_DS_50G)\n",
    "estimates_DS_10G, actuals_DS_10G = validate(model, validation_loader_DS_10G)\n",
    "estimates_H, actuals_H = validate(model, validation_loader_H)\n",
    "estimates_JOB, actuals_JOB = validate(model, validation_loader_JOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475.77037529593935\n",
      "69.53694562238142\n",
      "2.3628752973919163\n",
      "1.7765838939275371\n"
     ]
    }
   ],
   "source": [
    "print(q_error(actuals_DS_50G, estimates_DS_50G))\n",
    "print(q_error(actuals_DS_10G, estimates_DS_10G))\n",
    "print(q_error(actuals_H, estimates_H))\n",
    "print(q_error(actuals_JOB, estimates_JOB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = {\n",
    "    \"TPC_DS_50G\": [q_error([actuals_DS_50G[i]], [estimates_DS_50G[i]]) for i in range(len(actuals_DS_50G))],\n",
    "    \"TPC_DS_10G\": [q_error([actuals_DS_10G[i]], [estimates_DS_10G[i]]) for i in range(len(actuals_DS_10G))],\n",
    "    \"TPC_H\" : [q_error([actuals_H[i]], [estimates_H[i]]) for i in range(len(actuals_H))],\n",
    "    \"IMDB_JOB\": [q_error([actuals_JOB[i]], [estimates_JOB[i]]) for i in range(len(actuals_JOB))],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345188.4235610019\n",
      "128109.19063709957\n",
      "18.883376576080273\n",
      "18.647597924639744\n"
     ]
    }
   ],
   "source": [
    "percentile = 100\n",
    "print(np.percentile(result_dic[\"TPC_DS_50G\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_DS_10G\"], percentile))\n",
    "print(np.percentile(result_dic[\"TPC_H\"], percentile))\n",
    "print(np.percentile(result_dic[\"IMDB_JOB\"], percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import sample\n",
    "sample_count = 10\n",
    "df = pd.DataFrame(data = {\n",
    "    \"TPC_DS_10\": sorted(sample(result_dic[\"TPC_DS_10G\"], sample_count)),\n",
    "    \"TPC_DS_50\": sorted(sample(result_dic[\"TPC_DS_50G\"], sample_count)),\n",
    "    \"TPC_H\": sorted(sample(result_dic[\"TPC_H\"], sample_count)),\n",
    "    \"IMDB_JOB\": sorted(sample(result_dic[\"IMDB_JOB\"], sample_count))\n",
    "})\n",
    "\n",
    "df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (input_fc): Linear(in_features=42190, out_features=32, bias=True)\n",
      "  (output_fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 42190\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = MLP(INPUT_DIM, OUTPUT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "criterion = MSELoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss 168868720.00\n",
      "\tEpoch 0 | Batch 40 | Loss 43889736.00\n",
      "\tEpoch 0 | Batch 80 | Loss 15292338.00\n",
      "\tEpoch 0 | Batch 120 | Loss 42534776.00\n",
      "\tEpoch 0 | Batch 160 | Loss 55118660.00\n",
      "\tEpoch 0 | Batch 200 | Loss 102404240.00\n",
      "\tEpoch 0 | Batch 240 | Loss 26973768.00\n",
      "\tEpoch 0 | Batch 280 | Loss 28395008.00\n",
      "Epoch 0 | Loss 683001746.03\n",
      "\tEpoch 1 | Batch 0 | Loss 11523412.00\n",
      "\tEpoch 1 | Batch 40 | Loss 43456264.00\n",
      "\tEpoch 1 | Batch 80 | Loss 1007229312.00\n",
      "\tEpoch 1 | Batch 120 | Loss 51614500.00\n",
      "\tEpoch 1 | Batch 160 | Loss 55254996.00\n",
      "\tEpoch 1 | Batch 200 | Loss 18617336.00\n",
      "\tEpoch 1 | Batch 240 | Loss 22876464.00\n",
      "\tEpoch 1 | Batch 280 | Loss 15509198.00\n",
      "Epoch 1 | Loss 155828280.73\n",
      "\tEpoch 2 | Batch 0 | Loss 18795264.00\n",
      "\tEpoch 2 | Batch 40 | Loss 120083312.00\n",
      "\tEpoch 2 | Batch 80 | Loss 171503040.00\n",
      "\tEpoch 2 | Batch 120 | Loss 30483456.00\n",
      "\tEpoch 2 | Batch 160 | Loss 41743988.00\n",
      "\tEpoch 2 | Batch 200 | Loss 17684996.00\n",
      "\tEpoch 2 | Batch 240 | Loss 44479436.00\n",
      "\tEpoch 2 | Batch 280 | Loss 31135890.00\n",
      "Epoch 2 | Loss 101453726.45\n",
      "\tEpoch 3 | Batch 0 | Loss 46660696.00\n",
      "\tEpoch 3 | Batch 40 | Loss 61774712.00\n",
      "\tEpoch 3 | Batch 80 | Loss 107789336.00\n",
      "\tEpoch 3 | Batch 120 | Loss 50873912.00\n",
      "\tEpoch 3 | Batch 160 | Loss 84475240.00\n",
      "\tEpoch 3 | Batch 200 | Loss 32167616.00\n",
      "\tEpoch 3 | Batch 240 | Loss 50488712.00\n",
      "\tEpoch 3 | Batch 280 | Loss 153372192.00\n",
      "Epoch 3 | Loss 94700297.14\n",
      "\tEpoch 4 | Batch 0 | Loss 92446312.00\n",
      "\tEpoch 4 | Batch 40 | Loss 37056732.00\n",
      "\tEpoch 4 | Batch 80 | Loss 36773768.00\n",
      "\tEpoch 4 | Batch 120 | Loss 105139928.00\n",
      "\tEpoch 4 | Batch 160 | Loss 30792096.00\n",
      "\tEpoch 4 | Batch 200 | Loss 121258232.00\n",
      "\tEpoch 4 | Batch 240 | Loss 1096995072.00\n",
      "\tEpoch 4 | Batch 280 | Loss 130243816.00\n",
      "Epoch 4 | Loss 92395674.12\n",
      "\tEpoch 5 | Batch 0 | Loss 26954192.00\n",
      "\tEpoch 5 | Batch 40 | Loss 64493920.00\n",
      "\tEpoch 5 | Batch 80 | Loss 138825728.00\n",
      "\tEpoch 5 | Batch 120 | Loss 50038364.00\n",
      "\tEpoch 5 | Batch 160 | Loss 181209888.00\n",
      "\tEpoch 5 | Batch 200 | Loss 12357391.00\n",
      "\tEpoch 5 | Batch 240 | Loss 95886912.00\n",
      "\tEpoch 5 | Batch 280 | Loss 24042492.00\n",
      "Epoch 5 | Loss 92024024.57\n",
      "\tEpoch 6 | Batch 0 | Loss 17974632.00\n",
      "\tEpoch 6 | Batch 40 | Loss 85679544.00\n",
      "\tEpoch 6 | Batch 80 | Loss 24614342.00\n",
      "\tEpoch 6 | Batch 120 | Loss 41728300.00\n",
      "\tEpoch 6 | Batch 160 | Loss 46543920.00\n",
      "\tEpoch 6 | Batch 200 | Loss 10954416.00\n",
      "\tEpoch 6 | Batch 240 | Loss 57620112.00\n",
      "\tEpoch 6 | Batch 280 | Loss 87962304.00\n",
      "Epoch 6 | Loss 89965479.22\n",
      "\tEpoch 7 | Batch 0 | Loss 39736520.00\n",
      "\tEpoch 7 | Batch 40 | Loss 43937272.00\n",
      "\tEpoch 7 | Batch 80 | Loss 220957728.00\n",
      "\tEpoch 7 | Batch 120 | Loss 13681354.00\n",
      "\tEpoch 7 | Batch 160 | Loss 215638944.00\n",
      "\tEpoch 7 | Batch 200 | Loss 38000404.00\n",
      "\tEpoch 7 | Batch 240 | Loss 94301440.00\n",
      "\tEpoch 7 | Batch 280 | Loss 88436072.00\n",
      "Epoch 7 | Loss 87859090.21\n",
      "\tEpoch 8 | Batch 0 | Loss 75217088.00\n",
      "\tEpoch 8 | Batch 40 | Loss 11078378.00\n",
      "\tEpoch 8 | Batch 80 | Loss 96188152.00\n",
      "\tEpoch 8 | Batch 120 | Loss 20952630.00\n",
      "\tEpoch 8 | Batch 160 | Loss 215266320.00\n",
      "\tEpoch 8 | Batch 200 | Loss 98302656.00\n",
      "\tEpoch 8 | Batch 240 | Loss 87441120.00\n",
      "\tEpoch 8 | Batch 280 | Loss 183772992.00\n",
      "Epoch 8 | Loss 83293933.89\n",
      "\tEpoch 9 | Batch 0 | Loss 72728928.00\n",
      "\tEpoch 9 | Batch 40 | Loss 30041004.00\n",
      "\tEpoch 9 | Batch 80 | Loss 355871392.00\n",
      "\tEpoch 9 | Batch 120 | Loss 1347177728.00\n",
      "\tEpoch 9 | Batch 160 | Loss 51178416.00\n",
      "\tEpoch 9 | Batch 200 | Loss 77078360.00\n",
      "\tEpoch 9 | Batch 240 | Loss 39169112.00\n",
      "\tEpoch 9 | Batch 280 | Loss 53176184.00\n",
      "Epoch 9 | Loss 82599913.91\n",
      "\tEpoch 10 | Batch 0 | Loss 131707080.00\n",
      "\tEpoch 10 | Batch 40 | Loss 56202248.00\n",
      "\tEpoch 10 | Batch 80 | Loss 5734048.00\n",
      "\tEpoch 10 | Batch 120 | Loss 90642856.00\n",
      "\tEpoch 10 | Batch 160 | Loss 199945280.00\n",
      "\tEpoch 10 | Batch 200 | Loss 8334828.00\n",
      "\tEpoch 10 | Batch 240 | Loss 65969880.00\n",
      "\tEpoch 10 | Batch 280 | Loss 48200552.00\n",
      "Epoch 10 | Loss 80145257.16\n",
      "\tEpoch 11 | Batch 0 | Loss 30095204.00\n",
      "\tEpoch 11 | Batch 40 | Loss 13099098.00\n",
      "\tEpoch 11 | Batch 80 | Loss 77465920.00\n",
      "\tEpoch 11 | Batch 120 | Loss 95051016.00\n",
      "\tEpoch 11 | Batch 160 | Loss 74034640.00\n",
      "\tEpoch 11 | Batch 200 | Loss 37870792.00\n",
      "\tEpoch 11 | Batch 240 | Loss 27774004.00\n",
      "\tEpoch 11 | Batch 280 | Loss 168267456.00\n",
      "Epoch 11 | Loss 74529747.16\n",
      "\tEpoch 12 | Batch 0 | Loss 7923740.00\n",
      "\tEpoch 12 | Batch 40 | Loss 34777524.00\n",
      "\tEpoch 12 | Batch 80 | Loss 13918778.00\n",
      "\tEpoch 12 | Batch 120 | Loss 87123768.00\n",
      "\tEpoch 12 | Batch 160 | Loss 21554744.00\n",
      "\tEpoch 12 | Batch 200 | Loss 29960772.00\n",
      "\tEpoch 12 | Batch 240 | Loss 64515736.00\n",
      "\tEpoch 12 | Batch 280 | Loss 121342448.00\n",
      "Epoch 12 | Loss 78717857.09\n",
      "\tEpoch 13 | Batch 0 | Loss 112900096.00\n",
      "\tEpoch 13 | Batch 40 | Loss 101319632.00\n",
      "\tEpoch 13 | Batch 80 | Loss 45627164.00\n",
      "\tEpoch 13 | Batch 120 | Loss 86955064.00\n",
      "\tEpoch 13 | Batch 160 | Loss 1032806528.00\n",
      "\tEpoch 13 | Batch 200 | Loss 78268280.00\n",
      "\tEpoch 13 | Batch 240 | Loss 5791680.00\n",
      "\tEpoch 13 | Batch 280 | Loss 12333898.00\n",
      "Epoch 13 | Loss 71484021.01\n",
      "\tEpoch 14 | Batch 0 | Loss 47509940.00\n",
      "\tEpoch 14 | Batch 40 | Loss 2820742.75\n",
      "\tEpoch 14 | Batch 80 | Loss 7122632.00\n",
      "\tEpoch 14 | Batch 120 | Loss 34960504.00\n",
      "\tEpoch 14 | Batch 160 | Loss 27434086.00\n",
      "\tEpoch 14 | Batch 200 | Loss 6018085.50\n",
      "\tEpoch 14 | Batch 240 | Loss 68792624.00\n",
      "\tEpoch 14 | Batch 280 | Loss 40205968.00\n",
      "Epoch 14 | Loss 74197826.25\n",
      "\tEpoch 15 | Batch 0 | Loss 85417688.00\n",
      "\tEpoch 15 | Batch 40 | Loss 42171616.00\n",
      "\tEpoch 15 | Batch 80 | Loss 172609840.00\n",
      "\tEpoch 15 | Batch 120 | Loss 15383017.00\n",
      "\tEpoch 15 | Batch 160 | Loss 6564096.50\n",
      "\tEpoch 15 | Batch 200 | Loss 17350062.00\n",
      "\tEpoch 15 | Batch 240 | Loss 56672756.00\n",
      "\tEpoch 15 | Batch 280 | Loss 51726812.00\n",
      "Epoch 15 | Loss 67569488.05\n",
      "\tEpoch 16 | Batch 0 | Loss 78566256.00\n",
      "\tEpoch 16 | Batch 40 | Loss 62420028.00\n",
      "\tEpoch 16 | Batch 80 | Loss 22274654.00\n",
      "\tEpoch 16 | Batch 120 | Loss 56691112.00\n",
      "\tEpoch 16 | Batch 160 | Loss 163646192.00\n",
      "\tEpoch 16 | Batch 200 | Loss 6738088.00\n",
      "\tEpoch 16 | Batch 240 | Loss 106807128.00\n",
      "\tEpoch 16 | Batch 280 | Loss 42889308.00\n",
      "Epoch 16 | Loss 66527740.50\n",
      "\tEpoch 17 | Batch 0 | Loss 79711448.00\n",
      "\tEpoch 17 | Batch 40 | Loss 6411069.00\n",
      "\tEpoch 17 | Batch 80 | Loss 15630399.00\n",
      "\tEpoch 17 | Batch 120 | Loss 75152496.00\n",
      "\tEpoch 17 | Batch 160 | Loss 96080928.00\n",
      "\tEpoch 17 | Batch 200 | Loss 2611293.75\n",
      "\tEpoch 17 | Batch 240 | Loss 18067654.00\n",
      "\tEpoch 17 | Batch 280 | Loss 6881399.00\n",
      "Epoch 17 | Loss 67639520.44\n",
      "\tEpoch 18 | Batch 0 | Loss 49254764.00\n",
      "\tEpoch 18 | Batch 40 | Loss 52932208.00\n",
      "\tEpoch 18 | Batch 80 | Loss 6250007.00\n",
      "\tEpoch 18 | Batch 120 | Loss 88791800.00\n",
      "\tEpoch 18 | Batch 160 | Loss 70800320.00\n",
      "\tEpoch 18 | Batch 200 | Loss 43008144.00\n",
      "\tEpoch 18 | Batch 240 | Loss 13790272.00\n",
      "\tEpoch 18 | Batch 280 | Loss 36575476.00\n",
      "Epoch 18 | Loss 65348581.71\n",
      "\tEpoch 19 | Batch 0 | Loss 244716896.00\n",
      "\tEpoch 19 | Batch 40 | Loss 162343664.00\n",
      "\tEpoch 19 | Batch 80 | Loss 29141904.00\n",
      "\tEpoch 19 | Batch 120 | Loss 77438816.00\n",
      "\tEpoch 19 | Batch 160 | Loss 2196474.00\n",
      "\tEpoch 19 | Batch 200 | Loss 100528888.00\n",
      "\tEpoch 19 | Batch 240 | Loss 136646896.00\n",
      "\tEpoch 19 | Batch 280 | Loss 61444212.00\n",
      "Epoch 19 | Loss 66707706.62\n",
      "\tEpoch 20 | Batch 0 | Loss 35753452.00\n",
      "\tEpoch 20 | Batch 40 | Loss 47772128.00\n",
      "\tEpoch 20 | Batch 80 | Loss 36103260.00\n",
      "\tEpoch 20 | Batch 120 | Loss 72688000.00\n",
      "\tEpoch 20 | Batch 160 | Loss 19200708.00\n",
      "\tEpoch 20 | Batch 200 | Loss 10091335.00\n",
      "\tEpoch 20 | Batch 240 | Loss 75757648.00\n",
      "\tEpoch 20 | Batch 280 | Loss 294954432.00\n",
      "Epoch 20 | Loss 67013600.26\n",
      "\tEpoch 21 | Batch 0 | Loss 102382360.00\n",
      "\tEpoch 21 | Batch 40 | Loss 101057504.00\n",
      "\tEpoch 21 | Batch 80 | Loss 31031860.00\n",
      "\tEpoch 21 | Batch 120 | Loss 37828584.00\n",
      "\tEpoch 21 | Batch 160 | Loss 16629006.00\n",
      "\tEpoch 21 | Batch 200 | Loss 88999416.00\n",
      "\tEpoch 21 | Batch 240 | Loss 6305747.00\n",
      "\tEpoch 21 | Batch 280 | Loss 57168752.00\n",
      "Epoch 21 | Loss 60806205.58\n",
      "\tEpoch 22 | Batch 0 | Loss 99187432.00\n",
      "\tEpoch 22 | Batch 40 | Loss 308888128.00\n",
      "\tEpoch 22 | Batch 80 | Loss 28864856.00\n",
      "\tEpoch 22 | Batch 120 | Loss 33731764.00\n",
      "\tEpoch 22 | Batch 160 | Loss 55802232.00\n",
      "\tEpoch 22 | Batch 200 | Loss 26939422.00\n",
      "\tEpoch 22 | Batch 240 | Loss 3490788.00\n",
      "\tEpoch 22 | Batch 280 | Loss 1324150.88\n",
      "Epoch 22 | Loss 60821073.13\n",
      "\tEpoch 23 | Batch 0 | Loss 110201112.00\n",
      "\tEpoch 23 | Batch 40 | Loss 40476096.00\n",
      "\tEpoch 23 | Batch 80 | Loss 4672955.50\n",
      "\tEpoch 23 | Batch 120 | Loss 5487517.50\n",
      "\tEpoch 23 | Batch 160 | Loss 36487464.00\n",
      "\tEpoch 23 | Batch 200 | Loss 17849458.00\n",
      "\tEpoch 23 | Batch 240 | Loss 4855620.00\n",
      "\tEpoch 23 | Batch 280 | Loss 30847192.00\n",
      "Epoch 23 | Loss 58936760.05\n",
      "\tEpoch 24 | Batch 0 | Loss 48886748.00\n",
      "\tEpoch 24 | Batch 40 | Loss 131367792.00\n",
      "\tEpoch 24 | Batch 80 | Loss 23067424.00\n",
      "\tEpoch 24 | Batch 120 | Loss 55441192.00\n",
      "\tEpoch 24 | Batch 160 | Loss 137158560.00\n",
      "\tEpoch 24 | Batch 200 | Loss 45051048.00\n",
      "\tEpoch 24 | Batch 240 | Loss 127042800.00\n",
      "\tEpoch 24 | Batch 280 | Loss 62105580.00\n",
      "Epoch 24 | Loss 59788012.78\n",
      "\tEpoch 25 | Batch 0 | Loss 97930608.00\n",
      "\tEpoch 25 | Batch 40 | Loss 58613700.00\n",
      "\tEpoch 25 | Batch 80 | Loss 5248338.00\n",
      "\tEpoch 25 | Batch 120 | Loss 6430198.00\n",
      "\tEpoch 25 | Batch 160 | Loss 32742240.00\n",
      "\tEpoch 25 | Batch 200 | Loss 27010932.00\n",
      "\tEpoch 25 | Batch 240 | Loss 21444158.00\n",
      "\tEpoch 25 | Batch 280 | Loss 94191232.00\n",
      "Epoch 25 | Loss 58239472.15\n",
      "\tEpoch 26 | Batch 0 | Loss 36635244.00\n",
      "\tEpoch 26 | Batch 40 | Loss 5904510.50\n",
      "\tEpoch 26 | Batch 80 | Loss 24476192.00\n",
      "\tEpoch 26 | Batch 120 | Loss 42813728.00\n",
      "\tEpoch 26 | Batch 160 | Loss 53647428.00\n",
      "\tEpoch 26 | Batch 200 | Loss 11439810.00\n",
      "\tEpoch 26 | Batch 240 | Loss 43484380.00\n",
      "\tEpoch 26 | Batch 280 | Loss 13163755.00\n",
      "Epoch 26 | Loss 61281138.89\n",
      "\tEpoch 27 | Batch 0 | Loss 32524048.00\n",
      "\tEpoch 27 | Batch 40 | Loss 21494292.00\n",
      "\tEpoch 27 | Batch 80 | Loss 13480526.00\n",
      "\tEpoch 27 | Batch 120 | Loss 16253440.00\n",
      "\tEpoch 27 | Batch 160 | Loss 40005980.00\n",
      "\tEpoch 27 | Batch 200 | Loss 9221688.00\n",
      "\tEpoch 27 | Batch 240 | Loss 91583008.00\n",
      "\tEpoch 27 | Batch 280 | Loss 38391832.00\n",
      "Epoch 27 | Loss 59873378.99\n",
      "\tEpoch 28 | Batch 0 | Loss 29644854.00\n",
      "\tEpoch 28 | Batch 40 | Loss 24173568.00\n",
      "\tEpoch 28 | Batch 80 | Loss 28191526.00\n",
      "\tEpoch 28 | Batch 120 | Loss 170357936.00\n",
      "\tEpoch 28 | Batch 160 | Loss 25403772.00\n",
      "\tEpoch 28 | Batch 200 | Loss 6189394.00\n",
      "\tEpoch 28 | Batch 240 | Loss 74892168.00\n",
      "\tEpoch 28 | Batch 280 | Loss 12834765.00\n",
      "Epoch 28 | Loss 59109239.62\n",
      "\tEpoch 29 | Batch 0 | Loss 143765440.00\n",
      "\tEpoch 29 | Batch 40 | Loss 47892640.00\n",
      "\tEpoch 29 | Batch 80 | Loss 38467924.00\n",
      "\tEpoch 29 | Batch 120 | Loss 78463104.00\n",
      "\tEpoch 29 | Batch 160 | Loss 94730856.00\n",
      "\tEpoch 29 | Batch 200 | Loss 34234980.00\n",
      "\tEpoch 29 | Batch 240 | Loss 65901872.00\n",
      "\tEpoch 29 | Batch 280 | Loss 45015852.00\n",
      "Epoch 29 | Loss 55908813.78\n",
      "\tEpoch 30 | Batch 0 | Loss 32292976.00\n",
      "\tEpoch 30 | Batch 40 | Loss 58316088.00\n",
      "\tEpoch 30 | Batch 80 | Loss 4087942.50\n",
      "\tEpoch 30 | Batch 120 | Loss 65449296.00\n",
      "\tEpoch 30 | Batch 160 | Loss 4805647.00\n",
      "\tEpoch 30 | Batch 200 | Loss 90160536.00\n",
      "\tEpoch 30 | Batch 240 | Loss 4738510.00\n",
      "\tEpoch 30 | Batch 280 | Loss 47179812.00\n",
      "Epoch 30 | Loss 56990610.68\n",
      "\tEpoch 31 | Batch 0 | Loss 9744666.00\n",
      "\tEpoch 31 | Batch 40 | Loss 14849014.00\n",
      "\tEpoch 31 | Batch 80 | Loss 3940533.25\n",
      "\tEpoch 31 | Batch 120 | Loss 99325792.00\n",
      "\tEpoch 31 | Batch 160 | Loss 7656723.00\n",
      "\tEpoch 31 | Batch 200 | Loss 9991701.00\n",
      "\tEpoch 31 | Batch 240 | Loss 69029992.00\n",
      "\tEpoch 31 | Batch 280 | Loss 7593868.00\n",
      "Epoch 31 | Loss 57477456.61\n",
      "\tEpoch 32 | Batch 0 | Loss 31529008.00\n",
      "\tEpoch 32 | Batch 40 | Loss 6872811.00\n",
      "\tEpoch 32 | Batch 80 | Loss 8291101.00\n",
      "\tEpoch 32 | Batch 120 | Loss 31165264.00\n",
      "\tEpoch 32 | Batch 160 | Loss 4504662.00\n",
      "\tEpoch 32 | Batch 200 | Loss 138901104.00\n",
      "\tEpoch 32 | Batch 240 | Loss 277610848.00\n",
      "\tEpoch 32 | Batch 280 | Loss 4133832.50\n",
      "Epoch 32 | Loss 56831156.86\n",
      "\tEpoch 33 | Batch 0 | Loss 8946937.00\n",
      "\tEpoch 33 | Batch 40 | Loss 17265636.00\n",
      "\tEpoch 33 | Batch 80 | Loss 25668028.00\n",
      "\tEpoch 33 | Batch 120 | Loss 59068476.00\n",
      "\tEpoch 33 | Batch 160 | Loss 33565384.00\n",
      "\tEpoch 33 | Batch 200 | Loss 7828208.00\n",
      "\tEpoch 33 | Batch 240 | Loss 38723524.00\n",
      "\tEpoch 33 | Batch 280 | Loss 82545416.00\n",
      "Epoch 33 | Loss 52024294.12\n",
      "\tEpoch 34 | Batch 0 | Loss 29059536.00\n",
      "\tEpoch 34 | Batch 40 | Loss 72232136.00\n",
      "\tEpoch 34 | Batch 80 | Loss 33534872.00\n",
      "\tEpoch 34 | Batch 120 | Loss 22235278.00\n",
      "\tEpoch 34 | Batch 160 | Loss 12707180.00\n",
      "\tEpoch 34 | Batch 200 | Loss 22554896.00\n",
      "\tEpoch 34 | Batch 240 | Loss 31310306.00\n",
      "\tEpoch 34 | Batch 280 | Loss 10421714.00\n",
      "Epoch 34 | Loss 53741507.35\n",
      "\tEpoch 35 | Batch 0 | Loss 34738824.00\n",
      "\tEpoch 35 | Batch 40 | Loss 40283832.00\n",
      "\tEpoch 35 | Batch 80 | Loss 101819128.00\n",
      "\tEpoch 35 | Batch 120 | Loss 101845832.00\n",
      "\tEpoch 35 | Batch 160 | Loss 84177760.00\n",
      "\tEpoch 35 | Batch 200 | Loss 15309190.00\n",
      "\tEpoch 35 | Batch 240 | Loss 98475152.00\n",
      "\tEpoch 35 | Batch 280 | Loss 47099424.00\n",
      "Epoch 35 | Loss 53863146.47\n",
      "\tEpoch 36 | Batch 0 | Loss 31602648.00\n",
      "\tEpoch 36 | Batch 40 | Loss 93197096.00\n",
      "\tEpoch 36 | Batch 80 | Loss 54199856.00\n",
      "\tEpoch 36 | Batch 120 | Loss 30056872.00\n",
      "\tEpoch 36 | Batch 160 | Loss 44819988.00\n",
      "\tEpoch 36 | Batch 200 | Loss 24354346.00\n",
      "\tEpoch 36 | Batch 240 | Loss 15389425.00\n",
      "\tEpoch 36 | Batch 280 | Loss 19702024.00\n",
      "Epoch 36 | Loss 55626439.41\n",
      "\tEpoch 37 | Batch 0 | Loss 77202080.00\n",
      "\tEpoch 37 | Batch 40 | Loss 15902993.00\n",
      "\tEpoch 37 | Batch 80 | Loss 34535176.00\n",
      "\tEpoch 37 | Batch 120 | Loss 43386796.00\n",
      "\tEpoch 37 | Batch 160 | Loss 4437480.50\n",
      "\tEpoch 37 | Batch 200 | Loss 6150925.50\n",
      "\tEpoch 37 | Batch 240 | Loss 15127379.00\n",
      "\tEpoch 37 | Batch 280 | Loss 19071870.00\n",
      "Epoch 37 | Loss 48760561.17\n",
      "\tEpoch 38 | Batch 0 | Loss 49125856.00\n",
      "\tEpoch 38 | Batch 40 | Loss 44778848.00\n",
      "\tEpoch 38 | Batch 80 | Loss 66668452.00\n",
      "\tEpoch 38 | Batch 120 | Loss 8123828.00\n",
      "\tEpoch 38 | Batch 160 | Loss 5096206.50\n",
      "\tEpoch 38 | Batch 200 | Loss 7087323.00\n",
      "\tEpoch 38 | Batch 240 | Loss 4171521.50\n",
      "\tEpoch 38 | Batch 280 | Loss 43501484.00\n",
      "Epoch 38 | Loss 56821427.15\n",
      "\tEpoch 39 | Batch 0 | Loss 4281928.00\n",
      "\tEpoch 39 | Batch 40 | Loss 111738560.00\n",
      "\tEpoch 39 | Batch 80 | Loss 34677208.00\n",
      "\tEpoch 39 | Batch 120 | Loss 30159526.00\n",
      "\tEpoch 39 | Batch 160 | Loss 43216128.00\n",
      "\tEpoch 39 | Batch 200 | Loss 64163432.00\n",
      "\tEpoch 39 | Batch 240 | Loss 374294432.00\n",
      "\tEpoch 39 | Batch 280 | Loss 10744671.00\n",
      "Epoch 39 | Loss 50064184.50\n",
      "\tEpoch 40 | Batch 0 | Loss 22969882.00\n",
      "\tEpoch 40 | Batch 40 | Loss 182037040.00\n",
      "\tEpoch 40 | Batch 80 | Loss 86480680.00\n",
      "\tEpoch 40 | Batch 120 | Loss 10638462.00\n",
      "\tEpoch 40 | Batch 160 | Loss 8573924.00\n",
      "\tEpoch 40 | Batch 200 | Loss 230917360.00\n",
      "\tEpoch 40 | Batch 240 | Loss 11827365.00\n",
      "\tEpoch 40 | Batch 280 | Loss 30829044.00\n",
      "Epoch 40 | Loss 48555519.59\n",
      "\tEpoch 41 | Batch 0 | Loss 5100574.50\n",
      "\tEpoch 41 | Batch 40 | Loss 3182484.75\n",
      "\tEpoch 41 | Batch 80 | Loss 3262433.00\n",
      "\tEpoch 41 | Batch 120 | Loss 264172816.00\n",
      "\tEpoch 41 | Batch 160 | Loss 6452088.50\n",
      "\tEpoch 41 | Batch 200 | Loss 21330294.00\n",
      "\tEpoch 41 | Batch 240 | Loss 208534848.00\n",
      "\tEpoch 41 | Batch 280 | Loss 14004796.00\n",
      "Epoch 41 | Loss 50130103.10\n",
      "\tEpoch 42 | Batch 0 | Loss 6249787.00\n",
      "\tEpoch 42 | Batch 40 | Loss 36130608.00\n",
      "\tEpoch 42 | Batch 80 | Loss 10225519.00\n",
      "\tEpoch 42 | Batch 120 | Loss 111665440.00\n",
      "\tEpoch 42 | Batch 160 | Loss 22327164.00\n",
      "\tEpoch 42 | Batch 200 | Loss 9067107.00\n",
      "\tEpoch 42 | Batch 240 | Loss 8516044.00\n",
      "\tEpoch 42 | Batch 280 | Loss 3808871.50\n",
      "Epoch 42 | Loss 53010300.12\n",
      "\tEpoch 43 | Batch 0 | Loss 5680323.00\n",
      "\tEpoch 43 | Batch 40 | Loss 5028871.50\n",
      "\tEpoch 43 | Batch 80 | Loss 4366112.00\n",
      "\tEpoch 43 | Batch 120 | Loss 35884848.00\n",
      "\tEpoch 43 | Batch 160 | Loss 9835837.00\n",
      "\tEpoch 43 | Batch 200 | Loss 20360560.00\n",
      "\tEpoch 43 | Batch 240 | Loss 4228409.50\n",
      "\tEpoch 43 | Batch 280 | Loss 201764800.00\n",
      "Epoch 43 | Loss 49807094.07\n",
      "\tEpoch 44 | Batch 0 | Loss 4916969.00\n",
      "\tEpoch 44 | Batch 40 | Loss 29365978.00\n",
      "\tEpoch 44 | Batch 80 | Loss 42914400.00\n",
      "\tEpoch 44 | Batch 120 | Loss 185881248.00\n",
      "\tEpoch 44 | Batch 160 | Loss 10775060.00\n",
      "\tEpoch 44 | Batch 200 | Loss 14368444.00\n",
      "\tEpoch 44 | Batch 240 | Loss 85471136.00\n",
      "\tEpoch 44 | Batch 280 | Loss 40695648.00\n",
      "Epoch 44 | Loss 52457802.50\n",
      "\tEpoch 45 | Batch 0 | Loss 7972080.00\n",
      "\tEpoch 45 | Batch 40 | Loss 75670432.00\n",
      "\tEpoch 45 | Batch 80 | Loss 93849248.00\n",
      "\tEpoch 45 | Batch 120 | Loss 22705232.00\n",
      "\tEpoch 45 | Batch 160 | Loss 592056256.00\n",
      "\tEpoch 45 | Batch 200 | Loss 153729264.00\n",
      "\tEpoch 45 | Batch 240 | Loss 16115026.00\n",
      "\tEpoch 45 | Batch 280 | Loss 20606984.00\n",
      "Epoch 45 | Loss 47930747.73\n",
      "\tEpoch 46 | Batch 0 | Loss 20269244.00\n",
      "\tEpoch 46 | Batch 40 | Loss 45567256.00\n",
      "\tEpoch 46 | Batch 80 | Loss 14089880.00\n",
      "\tEpoch 46 | Batch 120 | Loss 6702048.00\n",
      "\tEpoch 46 | Batch 160 | Loss 3783242.00\n",
      "\tEpoch 46 | Batch 200 | Loss 387397856.00\n",
      "\tEpoch 46 | Batch 240 | Loss 5451368.00\n",
      "\tEpoch 46 | Batch 280 | Loss 16638904.00\n",
      "Epoch 46 | Loss 49149518.51\n",
      "\tEpoch 47 | Batch 0 | Loss 4275161.00\n",
      "\tEpoch 47 | Batch 40 | Loss 144873888.00\n",
      "\tEpoch 47 | Batch 80 | Loss 109054552.00\n",
      "\tEpoch 47 | Batch 120 | Loss 22220920.00\n",
      "\tEpoch 47 | Batch 160 | Loss 52384552.00\n",
      "\tEpoch 47 | Batch 200 | Loss 9910394.00\n",
      "\tEpoch 47 | Batch 240 | Loss 62580636.00\n",
      "\tEpoch 47 | Batch 280 | Loss 19452694.00\n",
      "Epoch 47 | Loss 54098693.09\n",
      "\tEpoch 48 | Batch 0 | Loss 35656900.00\n",
      "\tEpoch 48 | Batch 40 | Loss 32373228.00\n",
      "\tEpoch 48 | Batch 80 | Loss 23885806.00\n",
      "\tEpoch 48 | Batch 120 | Loss 19831278.00\n",
      "\tEpoch 48 | Batch 160 | Loss 3919636.25\n",
      "\tEpoch 48 | Batch 200 | Loss 4389126.00\n",
      "\tEpoch 48 | Batch 240 | Loss 300786304.00\n",
      "\tEpoch 48 | Batch 280 | Loss 3610513.00\n",
      "Epoch 48 | Loss 50448979.48\n",
      "\tEpoch 49 | Batch 0 | Loss 17763452.00\n",
      "\tEpoch 49 | Batch 40 | Loss 7524207.00\n",
      "\tEpoch 49 | Batch 80 | Loss 6375995.00\n",
      "\tEpoch 49 | Batch 120 | Loss 63482952.00\n",
      "\tEpoch 49 | Batch 160 | Loss 8197280.00\n",
      "\tEpoch 49 | Batch 200 | Loss 216190528.00\n",
      "\tEpoch 49 | Batch 240 | Loss 28133744.00\n",
      "\tEpoch 49 | Batch 280 | Loss 4056539.00\n",
      "Epoch 49 | Loss 52032601.72\n",
      "\tEpoch 50 | Batch 0 | Loss 3523001.75\n",
      "\tEpoch 50 | Batch 40 | Loss 35256872.00\n",
      "\tEpoch 50 | Batch 80 | Loss 38029636.00\n",
      "\tEpoch 50 | Batch 120 | Loss 59157552.00\n",
      "\tEpoch 50 | Batch 160 | Loss 71529632.00\n",
      "\tEpoch 50 | Batch 200 | Loss 11133339.00\n",
      "\tEpoch 50 | Batch 240 | Loss 10824784.00\n",
      "\tEpoch 50 | Batch 280 | Loss 37549004.00\n",
      "Epoch 50 | Loss 47877248.84\n",
      "\tEpoch 51 | Batch 0 | Loss 34436052.00\n",
      "\tEpoch 51 | Batch 40 | Loss 71977824.00\n",
      "\tEpoch 51 | Batch 80 | Loss 104029888.00\n",
      "\tEpoch 51 | Batch 120 | Loss 5296167.50\n",
      "\tEpoch 51 | Batch 160 | Loss 14433460.00\n",
      "\tEpoch 51 | Batch 200 | Loss 38550808.00\n",
      "\tEpoch 51 | Batch 240 | Loss 33144846.00\n",
      "\tEpoch 51 | Batch 280 | Loss 23507730.00\n",
      "Epoch 51 | Loss 45344696.08\n",
      "\tEpoch 52 | Batch 0 | Loss 5318282.00\n",
      "\tEpoch 52 | Batch 40 | Loss 2249107.50\n",
      "\tEpoch 52 | Batch 80 | Loss 5997255.00\n",
      "\tEpoch 52 | Batch 120 | Loss 4287764.00\n",
      "\tEpoch 52 | Batch 160 | Loss 41949600.00\n",
      "\tEpoch 52 | Batch 200 | Loss 36326876.00\n",
      "\tEpoch 52 | Batch 240 | Loss 7397327.00\n",
      "\tEpoch 52 | Batch 280 | Loss 60481276.00\n",
      "Epoch 52 | Loss 50320462.76\n",
      "\tEpoch 53 | Batch 0 | Loss 20711132.00\n",
      "\tEpoch 53 | Batch 40 | Loss 3738570.00\n",
      "\tEpoch 53 | Batch 80 | Loss 36127712.00\n",
      "\tEpoch 53 | Batch 120 | Loss 4649014.00\n",
      "\tEpoch 53 | Batch 160 | Loss 26472268.00\n",
      "\tEpoch 53 | Batch 200 | Loss 61989556.00\n",
      "\tEpoch 53 | Batch 240 | Loss 26901772.00\n",
      "\tEpoch 53 | Batch 280 | Loss 6331597.00\n",
      "Epoch 53 | Loss 48163166.19\n",
      "\tEpoch 54 | Batch 0 | Loss 19204870.00\n",
      "\tEpoch 54 | Batch 40 | Loss 97996864.00\n",
      "\tEpoch 54 | Batch 80 | Loss 7017914.50\n",
      "\tEpoch 54 | Batch 120 | Loss 5766978.00\n",
      "\tEpoch 54 | Batch 160 | Loss 89031016.00\n",
      "\tEpoch 54 | Batch 200 | Loss 167762240.00\n",
      "\tEpoch 54 | Batch 240 | Loss 3971158.25\n",
      "\tEpoch 54 | Batch 280 | Loss 32147486.00\n",
      "Epoch 54 | Loss 53139924.39\n",
      "\tEpoch 55 | Batch 0 | Loss 354543584.00\n",
      "\tEpoch 55 | Batch 40 | Loss 12039125.00\n",
      "\tEpoch 55 | Batch 80 | Loss 11561314.00\n",
      "\tEpoch 55 | Batch 120 | Loss 7569326.50\n",
      "\tEpoch 55 | Batch 160 | Loss 26522436.00\n",
      "\tEpoch 55 | Batch 200 | Loss 26496068.00\n",
      "\tEpoch 55 | Batch 240 | Loss 5164806.00\n",
      "\tEpoch 55 | Batch 280 | Loss 71719008.00\n",
      "Epoch 55 | Loss 51074546.24\n",
      "\tEpoch 56 | Batch 0 | Loss 7012407.00\n",
      "\tEpoch 56 | Batch 40 | Loss 14880625.00\n",
      "\tEpoch 56 | Batch 80 | Loss 3590164.50\n",
      "\tEpoch 56 | Batch 120 | Loss 44027508.00\n",
      "\tEpoch 56 | Batch 160 | Loss 60840548.00\n",
      "\tEpoch 56 | Batch 200 | Loss 84098024.00\n",
      "\tEpoch 56 | Batch 240 | Loss 26711284.00\n",
      "\tEpoch 56 | Batch 280 | Loss 11568546.00\n",
      "Epoch 56 | Loss 49219179.64\n",
      "\tEpoch 57 | Batch 0 | Loss 30994248.00\n",
      "\tEpoch 57 | Batch 40 | Loss 5203309.50\n",
      "\tEpoch 57 | Batch 80 | Loss 4573476.00\n",
      "\tEpoch 57 | Batch 120 | Loss 18815756.00\n",
      "\tEpoch 57 | Batch 160 | Loss 32634108.00\n",
      "\tEpoch 57 | Batch 200 | Loss 18741138.00\n",
      "\tEpoch 57 | Batch 240 | Loss 4158400.50\n",
      "\tEpoch 57 | Batch 280 | Loss 32972138.00\n",
      "Epoch 57 | Loss 49075831.29\n",
      "\tEpoch 58 | Batch 0 | Loss 11095590.00\n",
      "\tEpoch 58 | Batch 40 | Loss 680457408.00\n",
      "\tEpoch 58 | Batch 80 | Loss 5102128.50\n",
      "\tEpoch 58 | Batch 120 | Loss 1851132.25\n",
      "\tEpoch 58 | Batch 160 | Loss 2558010.25\n",
      "\tEpoch 58 | Batch 200 | Loss 24716472.00\n",
      "\tEpoch 58 | Batch 240 | Loss 5603024.50\n",
      "\tEpoch 58 | Batch 280 | Loss 57252628.00\n",
      "Epoch 58 | Loss 50556739.77\n",
      "\tEpoch 59 | Batch 0 | Loss 3559389.75\n",
      "\tEpoch 59 | Batch 40 | Loss 100304496.00\n",
      "\tEpoch 59 | Batch 80 | Loss 39299536.00\n",
      "\tEpoch 59 | Batch 120 | Loss 71097328.00\n",
      "\tEpoch 59 | Batch 160 | Loss 34062192.00\n",
      "\tEpoch 59 | Batch 200 | Loss 107131920.00\n",
      "\tEpoch 59 | Batch 240 | Loss 29636332.00\n",
      "\tEpoch 59 | Batch 280 | Loss 5011438.00\n",
      "Epoch 59 | Loss 51660791.08\n",
      "\tEpoch 60 | Batch 0 | Loss 31598732.00\n",
      "\tEpoch 60 | Batch 40 | Loss 23366446.00\n",
      "\tEpoch 60 | Batch 80 | Loss 66634000.00\n",
      "\tEpoch 60 | Batch 120 | Loss 69067272.00\n",
      "\tEpoch 60 | Batch 160 | Loss 9306562.00\n",
      "\tEpoch 60 | Batch 200 | Loss 5829411.50\n",
      "\tEpoch 60 | Batch 240 | Loss 7123709.50\n",
      "\tEpoch 60 | Batch 280 | Loss 8278575.00\n",
      "Epoch 60 | Loss 50341905.89\n",
      "\tEpoch 61 | Batch 0 | Loss 4913637.50\n",
      "\tEpoch 61 | Batch 40 | Loss 136975264.00\n",
      "\tEpoch 61 | Batch 80 | Loss 16539411.00\n",
      "\tEpoch 61 | Batch 120 | Loss 19309232.00\n",
      "\tEpoch 61 | Batch 160 | Loss 11606817.00\n",
      "\tEpoch 61 | Batch 200 | Loss 14443314.00\n",
      "\tEpoch 61 | Batch 240 | Loss 6623693.50\n",
      "\tEpoch 61 | Batch 280 | Loss 8029354.50\n",
      "Epoch 61 | Loss 50914514.82\n",
      "\tEpoch 62 | Batch 0 | Loss 10047264.00\n",
      "\tEpoch 62 | Batch 40 | Loss 45008380.00\n",
      "\tEpoch 62 | Batch 80 | Loss 9986742.00\n",
      "\tEpoch 62 | Batch 120 | Loss 52401580.00\n",
      "\tEpoch 62 | Batch 160 | Loss 9027966.00\n",
      "\tEpoch 62 | Batch 200 | Loss 18357656.00\n",
      "\tEpoch 62 | Batch 240 | Loss 8330206.50\n",
      "\tEpoch 62 | Batch 280 | Loss 9825150.00\n",
      "Epoch 62 | Loss 51650262.96\n",
      "\tEpoch 63 | Batch 0 | Loss 16402000.00\n",
      "\tEpoch 63 | Batch 40 | Loss 19587840.00\n",
      "\tEpoch 63 | Batch 80 | Loss 39725904.00\n",
      "\tEpoch 63 | Batch 120 | Loss 68095328.00\n",
      "\tEpoch 63 | Batch 160 | Loss 813936896.00\n",
      "\tEpoch 63 | Batch 200 | Loss 7406878.00\n",
      "\tEpoch 63 | Batch 240 | Loss 2520039.00\n",
      "\tEpoch 63 | Batch 280 | Loss 31965350.00\n",
      "Epoch 63 | Loss 47645511.89\n",
      "\tEpoch 64 | Batch 0 | Loss 65433572.00\n",
      "\tEpoch 64 | Batch 40 | Loss 133671872.00\n",
      "\tEpoch 64 | Batch 80 | Loss 7290904.50\n",
      "\tEpoch 64 | Batch 120 | Loss 15468768.00\n",
      "\tEpoch 64 | Batch 160 | Loss 77827984.00\n",
      "\tEpoch 64 | Batch 200 | Loss 6603995.00\n",
      "\tEpoch 64 | Batch 240 | Loss 44460048.00\n",
      "\tEpoch 64 | Batch 280 | Loss 4367334.50\n",
      "Epoch 64 | Loss 45195872.28\n",
      "\tEpoch 65 | Batch 0 | Loss 4932550.00\n",
      "\tEpoch 65 | Batch 40 | Loss 71193168.00\n",
      "\tEpoch 65 | Batch 80 | Loss 11230801.00\n",
      "\tEpoch 65 | Batch 120 | Loss 5713617.50\n",
      "\tEpoch 65 | Batch 160 | Loss 68024000.00\n",
      "\tEpoch 65 | Batch 200 | Loss 33459340.00\n",
      "\tEpoch 65 | Batch 240 | Loss 23097788.00\n",
      "\tEpoch 65 | Batch 280 | Loss 65771868.00\n",
      "Epoch 65 | Loss 50621065.54\n",
      "\tEpoch 66 | Batch 0 | Loss 23034662.00\n",
      "\tEpoch 66 | Batch 40 | Loss 9793326.00\n",
      "\tEpoch 66 | Batch 80 | Loss 28608774.00\n",
      "\tEpoch 66 | Batch 120 | Loss 32965396.00\n",
      "\tEpoch 66 | Batch 160 | Loss 14729130.00\n",
      "\tEpoch 66 | Batch 200 | Loss 100208448.00\n",
      "\tEpoch 66 | Batch 240 | Loss 22379662.00\n",
      "\tEpoch 66 | Batch 280 | Loss 209316208.00\n",
      "Epoch 66 | Loss 48973348.71\n",
      "\tEpoch 67 | Batch 0 | Loss 119586912.00\n",
      "\tEpoch 67 | Batch 40 | Loss 24953312.00\n",
      "\tEpoch 67 | Batch 80 | Loss 8239092.50\n",
      "\tEpoch 67 | Batch 120 | Loss 77008504.00\n",
      "\tEpoch 67 | Batch 160 | Loss 40708800.00\n",
      "\tEpoch 67 | Batch 200 | Loss 18627996.00\n",
      "\tEpoch 67 | Batch 240 | Loss 3856421.75\n",
      "\tEpoch 67 | Batch 280 | Loss 52977736.00\n",
      "Epoch 67 | Loss 49367605.02\n",
      "\tEpoch 68 | Batch 0 | Loss 11033194.00\n",
      "\tEpoch 68 | Batch 40 | Loss 281903104.00\n",
      "\tEpoch 68 | Batch 80 | Loss 60635632.00\n",
      "\tEpoch 68 | Batch 120 | Loss 34769660.00\n",
      "\tEpoch 68 | Batch 160 | Loss 12206803.00\n",
      "\tEpoch 68 | Batch 200 | Loss 10946218.00\n",
      "\tEpoch 68 | Batch 240 | Loss 137474928.00\n",
      "\tEpoch 68 | Batch 280 | Loss 7586674.50\n",
      "Epoch 68 | Loss 50740031.68\n",
      "\tEpoch 69 | Batch 0 | Loss 61387424.00\n",
      "\tEpoch 69 | Batch 40 | Loss 8773916.00\n",
      "\tEpoch 69 | Batch 80 | Loss 27661484.00\n",
      "\tEpoch 69 | Batch 120 | Loss 169047184.00\n",
      "\tEpoch 69 | Batch 160 | Loss 19400406.00\n",
      "\tEpoch 69 | Batch 200 | Loss 5490810.00\n",
      "\tEpoch 69 | Batch 240 | Loss 15054170.00\n",
      "\tEpoch 69 | Batch 280 | Loss 6484120.00\n",
      "Epoch 69 | Loss 49534242.76\n",
      "\tEpoch 70 | Batch 0 | Loss 224642448.00\n",
      "\tEpoch 70 | Batch 40 | Loss 41473336.00\n",
      "\tEpoch 70 | Batch 80 | Loss 25538568.00\n",
      "\tEpoch 70 | Batch 120 | Loss 129436184.00\n",
      "\tEpoch 70 | Batch 160 | Loss 4255252.00\n",
      "\tEpoch 70 | Batch 200 | Loss 51698484.00\n",
      "\tEpoch 70 | Batch 240 | Loss 10140246.00\n",
      "\tEpoch 70 | Batch 280 | Loss 56574664.00\n",
      "Epoch 70 | Loss 46188852.29\n",
      "\tEpoch 71 | Batch 0 | Loss 154826624.00\n",
      "\tEpoch 71 | Batch 40 | Loss 6272964.00\n",
      "\tEpoch 71 | Batch 80 | Loss 19974468.00\n",
      "\tEpoch 71 | Batch 120 | Loss 5341625.00\n",
      "\tEpoch 71 | Batch 160 | Loss 27929888.00\n",
      "\tEpoch 71 | Batch 200 | Loss 202864016.00\n",
      "\tEpoch 71 | Batch 240 | Loss 27680438.00\n",
      "\tEpoch 71 | Batch 280 | Loss 93577384.00\n",
      "Epoch 71 | Loss 49458338.78\n",
      "\tEpoch 72 | Batch 0 | Loss 141242432.00\n",
      "\tEpoch 72 | Batch 40 | Loss 4834606.00\n",
      "\tEpoch 72 | Batch 80 | Loss 44304508.00\n",
      "\tEpoch 72 | Batch 120 | Loss 37358728.00\n",
      "\tEpoch 72 | Batch 160 | Loss 48086520.00\n",
      "\tEpoch 72 | Batch 200 | Loss 1554797312.00\n",
      "\tEpoch 72 | Batch 240 | Loss 3855375.00\n",
      "\tEpoch 72 | Batch 280 | Loss 19582314.00\n",
      "Epoch 72 | Loss 48208874.15\n",
      "\tEpoch 73 | Batch 0 | Loss 29145166.00\n",
      "\tEpoch 73 | Batch 40 | Loss 189764192.00\n",
      "\tEpoch 73 | Batch 80 | Loss 7127668.50\n",
      "\tEpoch 73 | Batch 120 | Loss 26468304.00\n",
      "\tEpoch 73 | Batch 160 | Loss 73524760.00\n",
      "\tEpoch 73 | Batch 200 | Loss 9178801.00\n",
      "\tEpoch 73 | Batch 240 | Loss 82320816.00\n",
      "\tEpoch 73 | Batch 280 | Loss 54838976.00\n",
      "Epoch 73 | Loss 47529051.67\n",
      "\tEpoch 74 | Batch 0 | Loss 32116632.00\n",
      "\tEpoch 74 | Batch 40 | Loss 28269112.00\n",
      "\tEpoch 74 | Batch 80 | Loss 7719756.50\n",
      "\tEpoch 74 | Batch 120 | Loss 15182390.00\n",
      "\tEpoch 74 | Batch 160 | Loss 8882125.00\n",
      "\tEpoch 74 | Batch 200 | Loss 37437344.00\n",
      "\tEpoch 74 | Batch 240 | Loss 9195696.00\n",
      "\tEpoch 74 | Batch 280 | Loss 5078775.00\n",
      "Epoch 74 | Loss 50704977.29\n",
      "\tEpoch 75 | Batch 0 | Loss 83273032.00\n",
      "\tEpoch 75 | Batch 40 | Loss 91033776.00\n",
      "\tEpoch 75 | Batch 80 | Loss 23762828.00\n",
      "\tEpoch 75 | Batch 120 | Loss 57711004.00\n",
      "\tEpoch 75 | Batch 160 | Loss 19492952.00\n",
      "\tEpoch 75 | Batch 200 | Loss 11813149.00\n",
      "\tEpoch 75 | Batch 240 | Loss 70855264.00\n",
      "\tEpoch 75 | Batch 280 | Loss 5901839.00\n",
      "Epoch 75 | Loss 49594002.74\n",
      "\tEpoch 76 | Batch 0 | Loss 19325256.00\n",
      "\tEpoch 76 | Batch 40 | Loss 8158199.50\n",
      "\tEpoch 76 | Batch 80 | Loss 7752851.50\n",
      "\tEpoch 76 | Batch 120 | Loss 2733144.25\n",
      "\tEpoch 76 | Batch 160 | Loss 52078496.00\n",
      "\tEpoch 76 | Batch 200 | Loss 19370852.00\n",
      "\tEpoch 76 | Batch 240 | Loss 636619328.00\n",
      "\tEpoch 76 | Batch 280 | Loss 14902069.00\n",
      "Epoch 76 | Loss 49067378.31\n",
      "\tEpoch 77 | Batch 0 | Loss 293000768.00\n",
      "\tEpoch 77 | Batch 40 | Loss 22238412.00\n",
      "\tEpoch 77 | Batch 80 | Loss 21090900.00\n",
      "\tEpoch 77 | Batch 120 | Loss 55856944.00\n",
      "\tEpoch 77 | Batch 160 | Loss 4366480.00\n",
      "\tEpoch 77 | Batch 200 | Loss 19864456.00\n",
      "\tEpoch 77 | Batch 240 | Loss 12533828.00\n",
      "\tEpoch 77 | Batch 280 | Loss 18590536.00\n",
      "Epoch 77 | Loss 52296747.70\n",
      "\tEpoch 78 | Batch 0 | Loss 8803032.00\n",
      "\tEpoch 78 | Batch 40 | Loss 4181705.00\n",
      "\tEpoch 78 | Batch 80 | Loss 58072888.00\n",
      "\tEpoch 78 | Batch 120 | Loss 131385504.00\n",
      "\tEpoch 78 | Batch 160 | Loss 59850624.00\n",
      "\tEpoch 78 | Batch 200 | Loss 21616584.00\n",
      "\tEpoch 78 | Batch 240 | Loss 4516058.00\n",
      "\tEpoch 78 | Batch 280 | Loss 3390308.00\n",
      "Epoch 78 | Loss 50029014.61\n",
      "\tEpoch 79 | Batch 0 | Loss 22997066.00\n",
      "\tEpoch 79 | Batch 40 | Loss 8839082.00\n",
      "\tEpoch 79 | Batch 80 | Loss 13705580.00\n",
      "\tEpoch 79 | Batch 120 | Loss 35106284.00\n",
      "\tEpoch 79 | Batch 160 | Loss 32498322.00\n",
      "\tEpoch 79 | Batch 200 | Loss 17132028.00\n",
      "\tEpoch 79 | Batch 240 | Loss 23522998.00\n",
      "\tEpoch 79 | Batch 280 | Loss 66127372.00\n",
      "Epoch 79 | Loss 49172492.99\n",
      "\tEpoch 80 | Batch 0 | Loss 28388520.00\n",
      "\tEpoch 80 | Batch 40 | Loss 3290301.00\n",
      "\tEpoch 80 | Batch 80 | Loss 3789347.00\n",
      "\tEpoch 80 | Batch 120 | Loss 9024110.00\n",
      "\tEpoch 80 | Batch 160 | Loss 15072278.00\n",
      "\tEpoch 80 | Batch 200 | Loss 14664406.00\n",
      "\tEpoch 80 | Batch 240 | Loss 69238080.00\n",
      "\tEpoch 80 | Batch 280 | Loss 36726080.00\n",
      "Epoch 80 | Loss 46944056.05\n",
      "\tEpoch 81 | Batch 0 | Loss 14819229.00\n",
      "\tEpoch 81 | Batch 40 | Loss 30084410.00\n",
      "\tEpoch 81 | Batch 80 | Loss 50109424.00\n",
      "\tEpoch 81 | Batch 120 | Loss 25252500.00\n",
      "\tEpoch 81 | Batch 160 | Loss 63051088.00\n",
      "\tEpoch 81 | Batch 200 | Loss 48810092.00\n",
      "\tEpoch 81 | Batch 240 | Loss 7654094.50\n",
      "\tEpoch 81 | Batch 280 | Loss 4042573.50\n",
      "Epoch 81 | Loss 47690133.97\n",
      "\tEpoch 82 | Batch 0 | Loss 91994944.00\n",
      "\tEpoch 82 | Batch 40 | Loss 40941956.00\n",
      "\tEpoch 82 | Batch 80 | Loss 4090719.00\n",
      "\tEpoch 82 | Batch 120 | Loss 4733040.00\n",
      "\tEpoch 82 | Batch 160 | Loss 7300385.00\n",
      "\tEpoch 82 | Batch 200 | Loss 6268074.50\n",
      "\tEpoch 82 | Batch 240 | Loss 40303968.00\n",
      "\tEpoch 82 | Batch 280 | Loss 27875944.00\n",
      "Epoch 82 | Loss 49769625.23\n",
      "\tEpoch 83 | Batch 0 | Loss 10570572.00\n",
      "\tEpoch 83 | Batch 40 | Loss 234787104.00\n",
      "\tEpoch 83 | Batch 80 | Loss 8627181.00\n",
      "\tEpoch 83 | Batch 120 | Loss 65306440.00\n",
      "\tEpoch 83 | Batch 160 | Loss 48587268.00\n",
      "\tEpoch 83 | Batch 200 | Loss 93379752.00\n",
      "\tEpoch 83 | Batch 240 | Loss 23708420.00\n",
      "\tEpoch 83 | Batch 280 | Loss 3527915.75\n",
      "Epoch 83 | Loss 47967339.95\n",
      "\tEpoch 84 | Batch 0 | Loss 29454558.00\n",
      "\tEpoch 84 | Batch 40 | Loss 47091668.00\n",
      "\tEpoch 84 | Batch 80 | Loss 45878104.00\n",
      "\tEpoch 84 | Batch 120 | Loss 29314704.00\n",
      "\tEpoch 84 | Batch 160 | Loss 4026852.50\n",
      "\tEpoch 84 | Batch 200 | Loss 14737564.00\n",
      "\tEpoch 84 | Batch 240 | Loss 198552848.00\n",
      "\tEpoch 84 | Batch 280 | Loss 112152168.00\n",
      "Epoch 84 | Loss 47896635.19\n",
      "\tEpoch 85 | Batch 0 | Loss 4445241.00\n",
      "\tEpoch 85 | Batch 40 | Loss 89739032.00\n",
      "\tEpoch 85 | Batch 80 | Loss 24361228.00\n",
      "\tEpoch 85 | Batch 120 | Loss 27377436.00\n",
      "\tEpoch 85 | Batch 160 | Loss 5156322.00\n",
      "\tEpoch 85 | Batch 200 | Loss 11467377.00\n",
      "\tEpoch 85 | Batch 240 | Loss 8338930.00\n",
      "\tEpoch 85 | Batch 280 | Loss 31495836.00\n",
      "Epoch 85 | Loss 49498201.84\n",
      "\tEpoch 86 | Batch 0 | Loss 9472466.00\n",
      "\tEpoch 86 | Batch 40 | Loss 21005684.00\n",
      "\tEpoch 86 | Batch 80 | Loss 12032454.00\n",
      "\tEpoch 86 | Batch 120 | Loss 28354260.00\n",
      "\tEpoch 86 | Batch 160 | Loss 10365794.00\n",
      "\tEpoch 86 | Batch 200 | Loss 76980632.00\n",
      "\tEpoch 86 | Batch 240 | Loss 6260951.00\n",
      "\tEpoch 86 | Batch 280 | Loss 4557813.00\n",
      "Epoch 86 | Loss 47755775.15\n",
      "\tEpoch 87 | Batch 0 | Loss 40933952.00\n",
      "\tEpoch 87 | Batch 40 | Loss 42702448.00\n",
      "\tEpoch 87 | Batch 80 | Loss 162398256.00\n",
      "\tEpoch 87 | Batch 120 | Loss 6038170.00\n",
      "\tEpoch 87 | Batch 160 | Loss 8945916.00\n",
      "\tEpoch 87 | Batch 200 | Loss 7493606.50\n",
      "\tEpoch 87 | Batch 240 | Loss 6071814.00\n",
      "\tEpoch 87 | Batch 280 | Loss 6691991.00\n",
      "Epoch 87 | Loss 49650218.65\n",
      "\tEpoch 88 | Batch 0 | Loss 35224656.00\n",
      "\tEpoch 88 | Batch 40 | Loss 7443813.50\n",
      "\tEpoch 88 | Batch 80 | Loss 164620976.00\n",
      "\tEpoch 88 | Batch 120 | Loss 7317477.00\n",
      "\tEpoch 88 | Batch 160 | Loss 5969494.50\n",
      "\tEpoch 88 | Batch 200 | Loss 42701452.00\n",
      "\tEpoch 88 | Batch 240 | Loss 13947020.00\n",
      "\tEpoch 88 | Batch 280 | Loss 106999376.00\n",
      "Epoch 88 | Loss 46138790.77\n",
      "\tEpoch 89 | Batch 0 | Loss 53610712.00\n",
      "\tEpoch 89 | Batch 40 | Loss 11292296.00\n",
      "\tEpoch 89 | Batch 80 | Loss 65471588.00\n",
      "\tEpoch 89 | Batch 120 | Loss 285330496.00\n",
      "\tEpoch 89 | Batch 160 | Loss 5801837.50\n",
      "\tEpoch 89 | Batch 200 | Loss 28012316.00\n",
      "\tEpoch 89 | Batch 240 | Loss 4674956.50\n",
      "\tEpoch 89 | Batch 280 | Loss 52383024.00\n",
      "Epoch 89 | Loss 46749046.09\n",
      "\tEpoch 90 | Batch 0 | Loss 11306846.00\n",
      "\tEpoch 90 | Batch 40 | Loss 35541540.00\n",
      "\tEpoch 90 | Batch 80 | Loss 32319570.00\n",
      "\tEpoch 90 | Batch 120 | Loss 4246002.50\n",
      "\tEpoch 90 | Batch 160 | Loss 5896615.00\n",
      "\tEpoch 90 | Batch 200 | Loss 30154172.00\n",
      "\tEpoch 90 | Batch 240 | Loss 51851708.00\n",
      "\tEpoch 90 | Batch 280 | Loss 3489663.00\n",
      "Epoch 90 | Loss 48046365.64\n",
      "\tEpoch 91 | Batch 0 | Loss 66676972.00\n",
      "\tEpoch 91 | Batch 40 | Loss 5344523.00\n",
      "\tEpoch 91 | Batch 80 | Loss 16378889.00\n",
      "\tEpoch 91 | Batch 120 | Loss 16728714.00\n",
      "\tEpoch 91 | Batch 160 | Loss 7696250.50\n",
      "\tEpoch 91 | Batch 200 | Loss 8527510.00\n",
      "\tEpoch 91 | Batch 240 | Loss 2477076.75\n",
      "\tEpoch 91 | Batch 280 | Loss 13001560.00\n",
      "Epoch 91 | Loss 48164917.32\n",
      "\tEpoch 92 | Batch 0 | Loss 4818241.50\n",
      "\tEpoch 92 | Batch 40 | Loss 6340891.00\n",
      "\tEpoch 92 | Batch 80 | Loss 65389716.00\n",
      "\tEpoch 92 | Batch 120 | Loss 60044916.00\n",
      "\tEpoch 92 | Batch 160 | Loss 12401930.00\n",
      "\tEpoch 92 | Batch 200 | Loss 141810352.00\n",
      "\tEpoch 92 | Batch 240 | Loss 62948088.00\n",
      "\tEpoch 92 | Batch 280 | Loss 5086216.00\n",
      "Epoch 92 | Loss 50483947.46\n",
      "\tEpoch 93 | Batch 0 | Loss 16580696.00\n",
      "\tEpoch 93 | Batch 40 | Loss 4655586.50\n",
      "\tEpoch 93 | Batch 80 | Loss 3559466.75\n",
      "\tEpoch 93 | Batch 120 | Loss 17129816.00\n",
      "\tEpoch 93 | Batch 160 | Loss 19873840.00\n",
      "\tEpoch 93 | Batch 200 | Loss 34640876.00\n",
      "\tEpoch 93 | Batch 240 | Loss 30529588.00\n",
      "\tEpoch 93 | Batch 280 | Loss 24438172.00\n",
      "Epoch 93 | Loss 48255171.46\n",
      "\tEpoch 94 | Batch 0 | Loss 20685656.00\n",
      "\tEpoch 94 | Batch 40 | Loss 25540040.00\n",
      "\tEpoch 94 | Batch 80 | Loss 135621024.00\n",
      "\tEpoch 94 | Batch 120 | Loss 17733272.00\n",
      "\tEpoch 94 | Batch 160 | Loss 182784656.00\n",
      "\tEpoch 94 | Batch 200 | Loss 46224704.00\n",
      "\tEpoch 94 | Batch 240 | Loss 94730032.00\n",
      "\tEpoch 94 | Batch 280 | Loss 35398152.00\n",
      "Epoch 94 | Loss 47856057.80\n",
      "\tEpoch 95 | Batch 0 | Loss 5596298.00\n",
      "\tEpoch 95 | Batch 40 | Loss 4189210.00\n",
      "\tEpoch 95 | Batch 80 | Loss 16962202.00\n",
      "\tEpoch 95 | Batch 120 | Loss 85600160.00\n",
      "\tEpoch 95 | Batch 160 | Loss 117910680.00\n",
      "\tEpoch 95 | Batch 200 | Loss 5603791.50\n",
      "\tEpoch 95 | Batch 240 | Loss 11616773.00\n",
      "\tEpoch 95 | Batch 280 | Loss 9616630.00\n",
      "Epoch 95 | Loss 48012952.07\n",
      "\tEpoch 96 | Batch 0 | Loss 27084892.00\n",
      "\tEpoch 96 | Batch 40 | Loss 76135336.00\n",
      "\tEpoch 96 | Batch 80 | Loss 16767089.00\n",
      "\tEpoch 96 | Batch 120 | Loss 8060741.00\n",
      "\tEpoch 96 | Batch 160 | Loss 8543295.00\n",
      "\tEpoch 96 | Batch 200 | Loss 35920660.00\n",
      "\tEpoch 96 | Batch 240 | Loss 16963614.00\n",
      "\tEpoch 96 | Batch 280 | Loss 5559615.00\n",
      "Epoch 96 | Loss 47414901.37\n",
      "\tEpoch 97 | Batch 0 | Loss 4276230.00\n",
      "\tEpoch 97 | Batch 40 | Loss 4316655.00\n",
      "\tEpoch 97 | Batch 80 | Loss 21846392.00\n",
      "\tEpoch 97 | Batch 120 | Loss 20859544.00\n",
      "\tEpoch 97 | Batch 160 | Loss 17518628.00\n",
      "\tEpoch 97 | Batch 200 | Loss 94171384.00\n",
      "\tEpoch 97 | Batch 240 | Loss 9210776.00\n",
      "\tEpoch 97 | Batch 280 | Loss 1792188.62\n",
      "Epoch 97 | Loss 47056910.40\n",
      "\tEpoch 98 | Batch 0 | Loss 52986468.00\n",
      "\tEpoch 98 | Batch 40 | Loss 54737700.00\n",
      "\tEpoch 98 | Batch 80 | Loss 6873640.50\n",
      "\tEpoch 98 | Batch 120 | Loss 4903482.00\n",
      "\tEpoch 98 | Batch 160 | Loss 41836156.00\n",
      "\tEpoch 98 | Batch 200 | Loss 4615137.00\n",
      "\tEpoch 98 | Batch 240 | Loss 25830662.00\n",
      "\tEpoch 98 | Batch 280 | Loss 34273520.00\n",
      "Epoch 98 | Loss 45295154.27\n",
      "\tEpoch 99 | Batch 0 | Loss 11366122.00\n",
      "\tEpoch 99 | Batch 40 | Loss 27186176.00\n",
      "\tEpoch 99 | Batch 80 | Loss 15409363.00\n",
      "\tEpoch 99 | Batch 120 | Loss 3616925.00\n",
      "\tEpoch 99 | Batch 160 | Loss 28483308.00\n",
      "\tEpoch 99 | Batch 200 | Loss 61692048.00\n",
      "\tEpoch 99 | Batch 240 | Loss 19763326.00\n",
      "\tEpoch 99 | Batch 280 | Loss 55021152.00\n",
      "Epoch 99 | Loss 48524431.59\n",
      "\tEpoch 100 | Batch 0 | Loss 24422562.00\n",
      "\tEpoch 100 | Batch 40 | Loss 3343130.25\n",
      "\tEpoch 100 | Batch 80 | Loss 27635972.00\n",
      "\tEpoch 100 | Batch 120 | Loss 6975014.00\n",
      "\tEpoch 100 | Batch 160 | Loss 81280872.00\n",
      "\tEpoch 100 | Batch 200 | Loss 15799699.00\n",
      "\tEpoch 100 | Batch 240 | Loss 7277480.00\n",
      "\tEpoch 100 | Batch 280 | Loss 48470084.00\n",
      "Epoch 100 | Loss 47334021.51\n",
      "\tEpoch 101 | Batch 0 | Loss 8133846.00\n",
      "\tEpoch 101 | Batch 40 | Loss 32417096.00\n",
      "\tEpoch 101 | Batch 80 | Loss 2850588.50\n",
      "\tEpoch 101 | Batch 120 | Loss 18691654.00\n",
      "\tEpoch 101 | Batch 160 | Loss 8631163.00\n",
      "\tEpoch 101 | Batch 200 | Loss 34435384.00\n",
      "\tEpoch 101 | Batch 240 | Loss 23724224.00\n",
      "\tEpoch 101 | Batch 280 | Loss 211047344.00\n",
      "Epoch 101 | Loss 48156192.84\n",
      "\tEpoch 102 | Batch 0 | Loss 11497530.00\n",
      "\tEpoch 102 | Batch 40 | Loss 35877416.00\n",
      "\tEpoch 102 | Batch 80 | Loss 11457181.00\n",
      "\tEpoch 102 | Batch 120 | Loss 5099820.00\n",
      "\tEpoch 102 | Batch 160 | Loss 72184464.00\n",
      "\tEpoch 102 | Batch 200 | Loss 24423914.00\n",
      "\tEpoch 102 | Batch 240 | Loss 73344728.00\n",
      "\tEpoch 102 | Batch 280 | Loss 4523829.50\n",
      "Epoch 102 | Loss 44975066.50\n",
      "\tEpoch 103 | Batch 0 | Loss 6238524.00\n",
      "\tEpoch 103 | Batch 40 | Loss 86999216.00\n",
      "\tEpoch 103 | Batch 80 | Loss 5691781.00\n",
      "\tEpoch 103 | Batch 120 | Loss 38879716.00\n",
      "\tEpoch 103 | Batch 160 | Loss 4436716.00\n",
      "\tEpoch 103 | Batch 200 | Loss 86294144.00\n",
      "\tEpoch 103 | Batch 240 | Loss 146624960.00\n",
      "\tEpoch 103 | Batch 280 | Loss 13010120.00\n",
      "Epoch 103 | Loss 50605997.01\n",
      "\tEpoch 104 | Batch 0 | Loss 10096564.00\n",
      "\tEpoch 104 | Batch 40 | Loss 2340493.25\n",
      "\tEpoch 104 | Batch 80 | Loss 4617837.00\n",
      "\tEpoch 104 | Batch 120 | Loss 33129398.00\n",
      "\tEpoch 104 | Batch 160 | Loss 9669318.00\n",
      "\tEpoch 104 | Batch 200 | Loss 2813336.00\n",
      "\tEpoch 104 | Batch 240 | Loss 5590363.50\n",
      "\tEpoch 104 | Batch 280 | Loss 17494900.00\n",
      "Epoch 104 | Loss 47663200.34\n",
      "\tEpoch 105 | Batch 0 | Loss 29869090.00\n",
      "\tEpoch 105 | Batch 40 | Loss 87062808.00\n",
      "\tEpoch 105 | Batch 80 | Loss 9157887.00\n",
      "\tEpoch 105 | Batch 120 | Loss 6567728.50\n",
      "\tEpoch 105 | Batch 160 | Loss 8471976.00\n",
      "\tEpoch 105 | Batch 200 | Loss 180348592.00\n",
      "\tEpoch 105 | Batch 240 | Loss 31961596.00\n",
      "\tEpoch 105 | Batch 280 | Loss 4959946.00\n",
      "Epoch 105 | Loss 48356549.71\n",
      "\tEpoch 106 | Batch 0 | Loss 6791088.50\n",
      "\tEpoch 106 | Batch 40 | Loss 222865216.00\n",
      "\tEpoch 106 | Batch 80 | Loss 5519606.00\n",
      "\tEpoch 106 | Batch 120 | Loss 34144812.00\n",
      "\tEpoch 106 | Batch 160 | Loss 4944372.00\n",
      "\tEpoch 106 | Batch 200 | Loss 29698524.00\n",
      "\tEpoch 106 | Batch 240 | Loss 23565060.00\n",
      "\tEpoch 106 | Batch 280 | Loss 382505088.00\n",
      "Epoch 106 | Loss 44470073.72\n",
      "\tEpoch 107 | Batch 0 | Loss 13072748.00\n",
      "\tEpoch 107 | Batch 40 | Loss 8238256.50\n",
      "\tEpoch 107 | Batch 80 | Loss 46181360.00\n",
      "\tEpoch 107 | Batch 120 | Loss 15094898.00\n",
      "\tEpoch 107 | Batch 160 | Loss 8244538.50\n",
      "\tEpoch 107 | Batch 200 | Loss 73717600.00\n",
      "\tEpoch 107 | Batch 240 | Loss 7703094.50\n",
      "\tEpoch 107 | Batch 280 | Loss 8760554.00\n",
      "Epoch 107 | Loss 49199189.42\n",
      "\tEpoch 108 | Batch 0 | Loss 8580504.00\n",
      "\tEpoch 108 | Batch 40 | Loss 9478580.00\n",
      "\tEpoch 108 | Batch 80 | Loss 19560632.00\n",
      "\tEpoch 108 | Batch 120 | Loss 35721024.00\n",
      "\tEpoch 108 | Batch 160 | Loss 16779166.00\n",
      "\tEpoch 108 | Batch 200 | Loss 4399716.00\n",
      "\tEpoch 108 | Batch 240 | Loss 385165440.00\n",
      "\tEpoch 108 | Batch 280 | Loss 14605202.00\n",
      "Epoch 108 | Loss 46989142.00\n",
      "\tEpoch 109 | Batch 0 | Loss 8105227.00\n",
      "\tEpoch 109 | Batch 40 | Loss 3750790.00\n",
      "\tEpoch 109 | Batch 80 | Loss 5603558.00\n",
      "\tEpoch 109 | Batch 120 | Loss 23913502.00\n",
      "\tEpoch 109 | Batch 160 | Loss 12021532.00\n",
      "\tEpoch 109 | Batch 200 | Loss 6117948.50\n",
      "\tEpoch 109 | Batch 240 | Loss 10159604.00\n",
      "\tEpoch 109 | Batch 280 | Loss 51920600.00\n",
      "Epoch 109 | Loss 46971830.36\n",
      "\tEpoch 110 | Batch 0 | Loss 29702472.00\n",
      "\tEpoch 110 | Batch 40 | Loss 12772688.00\n",
      "\tEpoch 110 | Batch 80 | Loss 4129496.50\n",
      "\tEpoch 110 | Batch 120 | Loss 9670478.00\n",
      "\tEpoch 110 | Batch 160 | Loss 7912969.00\n",
      "\tEpoch 110 | Batch 200 | Loss 75928048.00\n",
      "\tEpoch 110 | Batch 240 | Loss 3873672.50\n",
      "\tEpoch 110 | Batch 280 | Loss 5314283.00\n",
      "Epoch 110 | Loss 48304230.73\n",
      "\tEpoch 111 | Batch 0 | Loss 7968777.00\n",
      "\tEpoch 111 | Batch 40 | Loss 17864228.00\n",
      "\tEpoch 111 | Batch 80 | Loss 31290070.00\n",
      "\tEpoch 111 | Batch 120 | Loss 313145600.00\n",
      "\tEpoch 111 | Batch 160 | Loss 11817270.00\n",
      "\tEpoch 111 | Batch 200 | Loss 25710952.00\n",
      "\tEpoch 111 | Batch 240 | Loss 7142575.50\n",
      "\tEpoch 111 | Batch 280 | Loss 3778709.50\n",
      "Epoch 111 | Loss 45381152.10\n",
      "\tEpoch 112 | Batch 0 | Loss 5927495.50\n",
      "\tEpoch 112 | Batch 40 | Loss 7475923.50\n",
      "\tEpoch 112 | Batch 80 | Loss 4190983.50\n",
      "\tEpoch 112 | Batch 120 | Loss 6613595.00\n",
      "\tEpoch 112 | Batch 160 | Loss 67457272.00\n",
      "\tEpoch 112 | Batch 200 | Loss 60716924.00\n",
      "\tEpoch 112 | Batch 240 | Loss 11363790.00\n",
      "\tEpoch 112 | Batch 280 | Loss 68823152.00\n",
      "Epoch 112 | Loss 47960256.72\n",
      "\tEpoch 113 | Batch 0 | Loss 4092524.00\n",
      "\tEpoch 113 | Batch 40 | Loss 85131840.00\n",
      "\tEpoch 113 | Batch 80 | Loss 24958356.00\n",
      "\tEpoch 113 | Batch 120 | Loss 5313595.00\n",
      "\tEpoch 113 | Batch 160 | Loss 8271527.00\n",
      "\tEpoch 113 | Batch 200 | Loss 1065496320.00\n",
      "\tEpoch 113 | Batch 240 | Loss 27509480.00\n",
      "\tEpoch 113 | Batch 280 | Loss 38637120.00\n",
      "Epoch 113 | Loss 48940332.97\n",
      "\tEpoch 114 | Batch 0 | Loss 64756336.00\n",
      "\tEpoch 114 | Batch 40 | Loss 168923184.00\n",
      "\tEpoch 114 | Batch 80 | Loss 3442344.25\n",
      "\tEpoch 114 | Batch 120 | Loss 13678750.00\n",
      "\tEpoch 114 | Batch 160 | Loss 88065672.00\n",
      "\tEpoch 114 | Batch 200 | Loss 82417248.00\n",
      "\tEpoch 114 | Batch 240 | Loss 8136230.00\n",
      "\tEpoch 114 | Batch 280 | Loss 27599406.00\n",
      "Epoch 114 | Loss 49673980.45\n",
      "\tEpoch 115 | Batch 0 | Loss 9130704.00\n",
      "\tEpoch 115 | Batch 40 | Loss 30083220.00\n",
      "\tEpoch 115 | Batch 80 | Loss 3317430.25\n",
      "\tEpoch 115 | Batch 120 | Loss 75237312.00\n",
      "\tEpoch 115 | Batch 160 | Loss 16464950.00\n",
      "\tEpoch 115 | Batch 200 | Loss 325965984.00\n",
      "\tEpoch 115 | Batch 240 | Loss 34011360.00\n",
      "\tEpoch 115 | Batch 280 | Loss 50168088.00\n",
      "Epoch 115 | Loss 48697340.67\n",
      "\tEpoch 116 | Batch 0 | Loss 7016213.00\n",
      "\tEpoch 116 | Batch 40 | Loss 4135199.00\n",
      "\tEpoch 116 | Batch 80 | Loss 35307176.00\n",
      "\tEpoch 116 | Batch 120 | Loss 14427020.00\n",
      "\tEpoch 116 | Batch 160 | Loss 109123600.00\n",
      "\tEpoch 116 | Batch 200 | Loss 21209370.00\n",
      "\tEpoch 116 | Batch 240 | Loss 35357056.00\n",
      "\tEpoch 116 | Batch 280 | Loss 674650240.00\n",
      "Epoch 116 | Loss 46619162.93\n",
      "\tEpoch 117 | Batch 0 | Loss 2359269.50\n",
      "\tEpoch 117 | Batch 40 | Loss 8836531.00\n",
      "\tEpoch 117 | Batch 80 | Loss 81319160.00\n",
      "\tEpoch 117 | Batch 120 | Loss 29618656.00\n",
      "\tEpoch 117 | Batch 160 | Loss 22678154.00\n",
      "\tEpoch 117 | Batch 200 | Loss 3394347.50\n",
      "\tEpoch 117 | Batch 240 | Loss 35126456.00\n",
      "\tEpoch 117 | Batch 280 | Loss 8535425.00\n",
      "Epoch 117 | Loss 51632738.66\n",
      "\tEpoch 118 | Batch 0 | Loss 8332849.50\n",
      "\tEpoch 118 | Batch 40 | Loss 249867472.00\n",
      "\tEpoch 118 | Batch 80 | Loss 13128752.00\n",
      "\tEpoch 118 | Batch 120 | Loss 77792840.00\n",
      "\tEpoch 118 | Batch 160 | Loss 8825487.00\n",
      "\tEpoch 118 | Batch 200 | Loss 34075544.00\n",
      "\tEpoch 118 | Batch 240 | Loss 29227744.00\n",
      "\tEpoch 118 | Batch 280 | Loss 5767023.00\n",
      "Epoch 118 | Loss 50359806.61\n",
      "\tEpoch 119 | Batch 0 | Loss 14106966.00\n",
      "\tEpoch 119 | Batch 40 | Loss 39904476.00\n",
      "\tEpoch 119 | Batch 80 | Loss 5543856.50\n",
      "\tEpoch 119 | Batch 120 | Loss 10734254.00\n",
      "\tEpoch 119 | Batch 160 | Loss 8911854.00\n",
      "\tEpoch 119 | Batch 200 | Loss 5806343.50\n",
      "\tEpoch 119 | Batch 240 | Loss 4833234.00\n",
      "\tEpoch 119 | Batch 280 | Loss 18063158.00\n",
      "Epoch 119 | Loss 48714484.80\n",
      "\tEpoch 120 | Batch 0 | Loss 3793845.00\n",
      "\tEpoch 120 | Batch 40 | Loss 21821214.00\n",
      "\tEpoch 120 | Batch 80 | Loss 26747484.00\n",
      "\tEpoch 120 | Batch 120 | Loss 7539680.00\n",
      "\tEpoch 120 | Batch 160 | Loss 5574295.50\n",
      "\tEpoch 120 | Batch 200 | Loss 15732734.00\n",
      "\tEpoch 120 | Batch 240 | Loss 39185480.00\n",
      "\tEpoch 120 | Batch 280 | Loss 12167421.00\n",
      "Epoch 120 | Loss 47201773.02\n",
      "\tEpoch 121 | Batch 0 | Loss 245183808.00\n",
      "\tEpoch 121 | Batch 40 | Loss 156378176.00\n",
      "\tEpoch 121 | Batch 80 | Loss 29808516.00\n",
      "\tEpoch 121 | Batch 120 | Loss 12467993.00\n",
      "\tEpoch 121 | Batch 160 | Loss 40026100.00\n",
      "\tEpoch 121 | Batch 200 | Loss 50054008.00\n",
      "\tEpoch 121 | Batch 240 | Loss 75416216.00\n",
      "\tEpoch 121 | Batch 280 | Loss 5436050.00\n",
      "Epoch 121 | Loss 45207442.60\n",
      "\tEpoch 122 | Batch 0 | Loss 6806523.00\n",
      "\tEpoch 122 | Batch 40 | Loss 40778472.00\n",
      "\tEpoch 122 | Batch 80 | Loss 8795006.00\n",
      "\tEpoch 122 | Batch 120 | Loss 4119281.00\n",
      "\tEpoch 122 | Batch 160 | Loss 88310008.00\n",
      "\tEpoch 122 | Batch 200 | Loss 16604951.00\n",
      "\tEpoch 122 | Batch 240 | Loss 5307224.50\n",
      "\tEpoch 122 | Batch 280 | Loss 8408349.00\n",
      "Epoch 122 | Loss 48087945.01\n",
      "\tEpoch 123 | Batch 0 | Loss 32861786.00\n",
      "\tEpoch 123 | Batch 40 | Loss 5704436.50\n",
      "\tEpoch 123 | Batch 80 | Loss 6775472.50\n",
      "\tEpoch 123 | Batch 120 | Loss 5374279.50\n",
      "\tEpoch 123 | Batch 160 | Loss 31984082.00\n",
      "\tEpoch 123 | Batch 200 | Loss 8255810.00\n",
      "\tEpoch 123 | Batch 240 | Loss 16457723.00\n",
      "\tEpoch 123 | Batch 280 | Loss 97092000.00\n",
      "Epoch 123 | Loss 46807225.42\n",
      "\tEpoch 124 | Batch 0 | Loss 3080406.25\n",
      "\tEpoch 124 | Batch 40 | Loss 84632720.00\n",
      "\tEpoch 124 | Batch 80 | Loss 23262854.00\n",
      "\tEpoch 124 | Batch 120 | Loss 204575744.00\n",
      "\tEpoch 124 | Batch 160 | Loss 10844216.00\n",
      "\tEpoch 124 | Batch 200 | Loss 17601156.00\n",
      "\tEpoch 124 | Batch 240 | Loss 35550216.00\n",
      "\tEpoch 124 | Batch 280 | Loss 4645797.00\n",
      "Epoch 124 | Loss 46108463.61\n",
      "\tEpoch 125 | Batch 0 | Loss 34266272.00\n",
      "\tEpoch 125 | Batch 40 | Loss 17294968.00\n",
      "\tEpoch 125 | Batch 80 | Loss 17457374.00\n",
      "\tEpoch 125 | Batch 120 | Loss 60566376.00\n",
      "\tEpoch 125 | Batch 160 | Loss 11780414.00\n",
      "\tEpoch 125 | Batch 200 | Loss 21402932.00\n",
      "\tEpoch 125 | Batch 240 | Loss 17376890.00\n",
      "\tEpoch 125 | Batch 280 | Loss 7782220.50\n",
      "Epoch 125 | Loss 48544746.04\n",
      "\tEpoch 126 | Batch 0 | Loss 16957760.00\n",
      "\tEpoch 126 | Batch 40 | Loss 7546326.00\n",
      "\tEpoch 126 | Batch 80 | Loss 59183008.00\n",
      "\tEpoch 126 | Batch 120 | Loss 25461114.00\n",
      "\tEpoch 126 | Batch 160 | Loss 16547798.00\n",
      "\tEpoch 126 | Batch 200 | Loss 8048881.00\n",
      "\tEpoch 126 | Batch 240 | Loss 6260035.00\n",
      "\tEpoch 126 | Batch 280 | Loss 8625569.00\n",
      "Epoch 126 | Loss 47149836.00\n",
      "\tEpoch 127 | Batch 0 | Loss 21416330.00\n",
      "\tEpoch 127 | Batch 40 | Loss 5239376.50\n",
      "\tEpoch 127 | Batch 80 | Loss 349155552.00\n",
      "\tEpoch 127 | Batch 120 | Loss 6688341.00\n",
      "\tEpoch 127 | Batch 160 | Loss 5097290.00\n",
      "\tEpoch 127 | Batch 200 | Loss 3884435.75\n",
      "\tEpoch 127 | Batch 240 | Loss 13942604.00\n",
      "\tEpoch 127 | Batch 280 | Loss 6956996.50\n",
      "Epoch 127 | Loss 45562591.37\n",
      "\tEpoch 128 | Batch 0 | Loss 27464930.00\n",
      "\tEpoch 128 | Batch 40 | Loss 63690312.00\n",
      "\tEpoch 128 | Batch 80 | Loss 6658572.50\n",
      "\tEpoch 128 | Batch 120 | Loss 12874152.00\n",
      "\tEpoch 128 | Batch 160 | Loss 55587644.00\n",
      "\tEpoch 128 | Batch 200 | Loss 27573564.00\n",
      "\tEpoch 128 | Batch 240 | Loss 22538848.00\n",
      "\tEpoch 128 | Batch 280 | Loss 152690176.00\n",
      "Epoch 128 | Loss 48991939.02\n",
      "\tEpoch 129 | Batch 0 | Loss 5810449.00\n",
      "\tEpoch 129 | Batch 40 | Loss 8635696.00\n",
      "\tEpoch 129 | Batch 80 | Loss 13675720.00\n",
      "\tEpoch 129 | Batch 120 | Loss 38393008.00\n",
      "\tEpoch 129 | Batch 160 | Loss 18218756.00\n",
      "\tEpoch 129 | Batch 200 | Loss 83014816.00\n",
      "\tEpoch 129 | Batch 240 | Loss 22390958.00\n",
      "\tEpoch 129 | Batch 280 | Loss 22783566.00\n",
      "Epoch 129 | Loss 50063447.18\n",
      "\tEpoch 130 | Batch 0 | Loss 17457842.00\n",
      "\tEpoch 130 | Batch 40 | Loss 311537120.00\n",
      "\tEpoch 130 | Batch 80 | Loss 7642184.00\n",
      "\tEpoch 130 | Batch 120 | Loss 28293570.00\n",
      "\tEpoch 130 | Batch 160 | Loss 25922262.00\n",
      "\tEpoch 130 | Batch 200 | Loss 8229045.00\n",
      "\tEpoch 130 | Batch 240 | Loss 5599565.00\n",
      "\tEpoch 130 | Batch 280 | Loss 7574352.50\n",
      "Epoch 130 | Loss 48281045.78\n",
      "\tEpoch 131 | Batch 0 | Loss 179421888.00\n",
      "\tEpoch 131 | Batch 40 | Loss 7227664.00\n",
      "\tEpoch 131 | Batch 80 | Loss 27405044.00\n",
      "\tEpoch 131 | Batch 120 | Loss 25317160.00\n",
      "\tEpoch 131 | Batch 160 | Loss 26956192.00\n",
      "\tEpoch 131 | Batch 200 | Loss 5607503.50\n",
      "\tEpoch 131 | Batch 240 | Loss 66834240.00\n",
      "\tEpoch 131 | Batch 280 | Loss 54807188.00\n",
      "Epoch 131 | Loss 47304958.78\n",
      "\tEpoch 132 | Batch 0 | Loss 10887069.00\n",
      "\tEpoch 132 | Batch 40 | Loss 5619843.00\n",
      "\tEpoch 132 | Batch 80 | Loss 7729093.50\n",
      "\tEpoch 132 | Batch 120 | Loss 36978532.00\n",
      "\tEpoch 132 | Batch 160 | Loss 2982036.25\n",
      "\tEpoch 132 | Batch 200 | Loss 21851014.00\n",
      "\tEpoch 132 | Batch 240 | Loss 10656561.00\n",
      "\tEpoch 132 | Batch 280 | Loss 327907296.00\n",
      "Epoch 132 | Loss 45836688.04\n",
      "\tEpoch 133 | Batch 0 | Loss 4250566.50\n",
      "\tEpoch 133 | Batch 40 | Loss 7458904.00\n",
      "\tEpoch 133 | Batch 80 | Loss 45113240.00\n",
      "\tEpoch 133 | Batch 120 | Loss 5408490.00\n",
      "\tEpoch 133 | Batch 160 | Loss 323868864.00\n",
      "\tEpoch 133 | Batch 200 | Loss 60831608.00\n",
      "\tEpoch 133 | Batch 240 | Loss 6977539.00\n",
      "\tEpoch 133 | Batch 280 | Loss 7275580.00\n",
      "Epoch 133 | Loss 46873611.42\n",
      "\tEpoch 134 | Batch 0 | Loss 13704281.00\n",
      "\tEpoch 134 | Batch 40 | Loss 8721910.00\n",
      "\tEpoch 134 | Batch 80 | Loss 186697056.00\n",
      "\tEpoch 134 | Batch 120 | Loss 34365648.00\n",
      "\tEpoch 134 | Batch 160 | Loss 27762584.00\n",
      "\tEpoch 134 | Batch 200 | Loss 28812198.00\n",
      "\tEpoch 134 | Batch 240 | Loss 15780894.00\n",
      "\tEpoch 134 | Batch 280 | Loss 4306357.50\n",
      "Epoch 134 | Loss 50561183.33\n",
      "\tEpoch 135 | Batch 0 | Loss 14590960.00\n",
      "\tEpoch 135 | Batch 40 | Loss 33873708.00\n",
      "\tEpoch 135 | Batch 80 | Loss 5903696.00\n",
      "\tEpoch 135 | Batch 120 | Loss 12326008.00\n",
      "\tEpoch 135 | Batch 160 | Loss 28218612.00\n",
      "\tEpoch 135 | Batch 200 | Loss 3800420.25\n",
      "\tEpoch 135 | Batch 240 | Loss 169343136.00\n",
      "\tEpoch 135 | Batch 280 | Loss 72014680.00\n",
      "Epoch 135 | Loss 45721780.96\n",
      "\tEpoch 136 | Batch 0 | Loss 7754964.00\n",
      "\tEpoch 136 | Batch 40 | Loss 53782924.00\n",
      "\tEpoch 136 | Batch 80 | Loss 35803252.00\n",
      "\tEpoch 136 | Batch 120 | Loss 4138493.00\n",
      "\tEpoch 136 | Batch 160 | Loss 25511848.00\n",
      "\tEpoch 136 | Batch 200 | Loss 46712392.00\n",
      "\tEpoch 136 | Batch 240 | Loss 6382669.00\n",
      "\tEpoch 136 | Batch 280 | Loss 6367896.00\n",
      "Epoch 136 | Loss 48765905.81\n",
      "\tEpoch 137 | Batch 0 | Loss 5946820.50\n",
      "\tEpoch 137 | Batch 40 | Loss 45807988.00\n",
      "\tEpoch 137 | Batch 80 | Loss 2578215.75\n",
      "\tEpoch 137 | Batch 120 | Loss 5975222.50\n",
      "\tEpoch 137 | Batch 160 | Loss 80105904.00\n",
      "\tEpoch 137 | Batch 200 | Loss 91698512.00\n",
      "\tEpoch 137 | Batch 240 | Loss 51853872.00\n",
      "\tEpoch 137 | Batch 280 | Loss 340605600.00\n",
      "Epoch 137 | Loss 48475038.61\n",
      "\tEpoch 138 | Batch 0 | Loss 11127500.00\n",
      "\tEpoch 138 | Batch 40 | Loss 4470784.00\n",
      "\tEpoch 138 | Batch 80 | Loss 30199424.00\n",
      "\tEpoch 138 | Batch 120 | Loss 30822558.00\n",
      "\tEpoch 138 | Batch 160 | Loss 42716000.00\n",
      "\tEpoch 138 | Batch 200 | Loss 24587978.00\n",
      "\tEpoch 138 | Batch 240 | Loss 57631444.00\n",
      "\tEpoch 138 | Batch 280 | Loss 54760064.00\n",
      "Epoch 138 | Loss 47280188.33\n",
      "\tEpoch 139 | Batch 0 | Loss 42721496.00\n",
      "\tEpoch 139 | Batch 40 | Loss 18224060.00\n",
      "\tEpoch 139 | Batch 80 | Loss 37018324.00\n",
      "\tEpoch 139 | Batch 120 | Loss 37191848.00\n",
      "\tEpoch 139 | Batch 160 | Loss 59619268.00\n",
      "\tEpoch 139 | Batch 200 | Loss 22658870.00\n",
      "\tEpoch 139 | Batch 240 | Loss 26156012.00\n",
      "\tEpoch 139 | Batch 280 | Loss 30950134.00\n",
      "Epoch 139 | Loss 47614361.51\n",
      "\tEpoch 140 | Batch 0 | Loss 15368660.00\n",
      "\tEpoch 140 | Batch 40 | Loss 18678126.00\n",
      "\tEpoch 140 | Batch 80 | Loss 98419032.00\n",
      "\tEpoch 140 | Batch 120 | Loss 354319712.00\n",
      "\tEpoch 140 | Batch 160 | Loss 17432248.00\n",
      "\tEpoch 140 | Batch 200 | Loss 43810064.00\n",
      "\tEpoch 140 | Batch 240 | Loss 8981973.00\n",
      "\tEpoch 140 | Batch 280 | Loss 27588708.00\n",
      "Epoch 140 | Loss 45948633.37\n",
      "\tEpoch 141 | Batch 0 | Loss 3276264.50\n",
      "\tEpoch 141 | Batch 40 | Loss 124931344.00\n",
      "\tEpoch 141 | Batch 80 | Loss 23090134.00\n",
      "\tEpoch 141 | Batch 120 | Loss 133821080.00\n",
      "\tEpoch 141 | Batch 160 | Loss 4158492.50\n",
      "\tEpoch 141 | Batch 200 | Loss 50617644.00\n",
      "\tEpoch 141 | Batch 240 | Loss 4702148.00\n",
      "\tEpoch 141 | Batch 280 | Loss 15891604.00\n",
      "Epoch 141 | Loss 48199069.14\n",
      "\tEpoch 142 | Batch 0 | Loss 4341399.50\n",
      "\tEpoch 142 | Batch 40 | Loss 29712932.00\n",
      "\tEpoch 142 | Batch 80 | Loss 6816961.00\n",
      "\tEpoch 142 | Batch 120 | Loss 6817034.50\n",
      "\tEpoch 142 | Batch 160 | Loss 3977070.50\n",
      "\tEpoch 142 | Batch 200 | Loss 5198888.00\n",
      "\tEpoch 142 | Batch 240 | Loss 4409196.50\n",
      "\tEpoch 142 | Batch 280 | Loss 7793642.50\n",
      "Epoch 142 | Loss 47343802.87\n",
      "\tEpoch 143 | Batch 0 | Loss 9094952.00\n",
      "\tEpoch 143 | Batch 40 | Loss 5714185.00\n",
      "\tEpoch 143 | Batch 80 | Loss 14690831.00\n",
      "\tEpoch 143 | Batch 120 | Loss 18781446.00\n",
      "\tEpoch 143 | Batch 160 | Loss 21681366.00\n",
      "\tEpoch 143 | Batch 200 | Loss 9185584.00\n",
      "\tEpoch 143 | Batch 240 | Loss 41911944.00\n",
      "\tEpoch 143 | Batch 280 | Loss 32157232.00\n",
      "Epoch 143 | Loss 46990325.11\n",
      "\tEpoch 144 | Batch 0 | Loss 4415552.00\n",
      "\tEpoch 144 | Batch 40 | Loss 61286132.00\n",
      "\tEpoch 144 | Batch 80 | Loss 19110352.00\n",
      "\tEpoch 144 | Batch 120 | Loss 5565667.00\n",
      "\tEpoch 144 | Batch 160 | Loss 35019100.00\n",
      "\tEpoch 144 | Batch 200 | Loss 20037432.00\n",
      "\tEpoch 144 | Batch 240 | Loss 5703599.00\n",
      "\tEpoch 144 | Batch 280 | Loss 74114176.00\n",
      "Epoch 144 | Loss 45745652.79\n",
      "\tEpoch 145 | Batch 0 | Loss 12479210.00\n",
      "\tEpoch 145 | Batch 40 | Loss 62836880.00\n",
      "\tEpoch 145 | Batch 80 | Loss 7266054.00\n",
      "\tEpoch 145 | Batch 120 | Loss 8619748.00\n",
      "\tEpoch 145 | Batch 160 | Loss 24280552.00\n",
      "\tEpoch 145 | Batch 200 | Loss 2081702.00\n",
      "\tEpoch 145 | Batch 240 | Loss 43093072.00\n",
      "\tEpoch 145 | Batch 280 | Loss 18867198.00\n",
      "Epoch 145 | Loss 44858055.80\n",
      "\tEpoch 146 | Batch 0 | Loss 134462944.00\n",
      "\tEpoch 146 | Batch 40 | Loss 4356188.00\n",
      "\tEpoch 146 | Batch 80 | Loss 6469537.50\n",
      "\tEpoch 146 | Batch 120 | Loss 3228410.00\n",
      "\tEpoch 146 | Batch 160 | Loss 4817066.00\n",
      "\tEpoch 146 | Batch 200 | Loss 13921951.00\n",
      "\tEpoch 146 | Batch 240 | Loss 183908304.00\n",
      "\tEpoch 146 | Batch 280 | Loss 2127363.75\n",
      "Epoch 146 | Loss 49850928.12\n",
      "\tEpoch 147 | Batch 0 | Loss 12172158.00\n",
      "\tEpoch 147 | Batch 40 | Loss 10462535.00\n",
      "\tEpoch 147 | Batch 80 | Loss 10128338.00\n",
      "\tEpoch 147 | Batch 120 | Loss 121871728.00\n",
      "\tEpoch 147 | Batch 160 | Loss 23506356.00\n",
      "\tEpoch 147 | Batch 200 | Loss 7250753.00\n",
      "\tEpoch 147 | Batch 240 | Loss 9998567.00\n",
      "\tEpoch 147 | Batch 280 | Loss 169331472.00\n",
      "Epoch 147 | Loss 45788430.52\n",
      "\tEpoch 148 | Batch 0 | Loss 33655208.00\n",
      "\tEpoch 148 | Batch 40 | Loss 170508624.00\n",
      "\tEpoch 148 | Batch 80 | Loss 274220320.00\n",
      "\tEpoch 148 | Batch 120 | Loss 7169368.00\n",
      "\tEpoch 148 | Batch 160 | Loss 12088033.00\n",
      "\tEpoch 148 | Batch 200 | Loss 11942956.00\n",
      "\tEpoch 148 | Batch 240 | Loss 1250096000.00\n",
      "\tEpoch 148 | Batch 280 | Loss 445393472.00\n",
      "Epoch 148 | Loss 49445089.65\n",
      "\tEpoch 149 | Batch 0 | Loss 5044669.00\n",
      "\tEpoch 149 | Batch 40 | Loss 415537792.00\n",
      "\tEpoch 149 | Batch 80 | Loss 34284796.00\n",
      "\tEpoch 149 | Batch 120 | Loss 112666992.00\n",
      "\tEpoch 149 | Batch 160 | Loss 23875654.00\n",
      "\tEpoch 149 | Batch 200 | Loss 26396248.00\n",
      "\tEpoch 149 | Batch 240 | Loss 18921274.00\n",
      "\tEpoch 149 | Batch 280 | Loss 39488584.00\n",
      "Epoch 149 | Loss 47717292.76\n",
      "\tEpoch 150 | Batch 0 | Loss 3222193.25\n",
      "\tEpoch 150 | Batch 40 | Loss 5407707.50\n",
      "\tEpoch 150 | Batch 80 | Loss 30189100.00\n",
      "\tEpoch 150 | Batch 120 | Loss 56883212.00\n",
      "\tEpoch 150 | Batch 160 | Loss 45619788.00\n",
      "\tEpoch 150 | Batch 200 | Loss 44086024.00\n",
      "\tEpoch 150 | Batch 240 | Loss 3987746.50\n",
      "\tEpoch 150 | Batch 280 | Loss 6829180.00\n",
      "Epoch 150 | Loss 45856144.29\n",
      "\tEpoch 151 | Batch 0 | Loss 12008018.00\n",
      "\tEpoch 151 | Batch 40 | Loss 3498997.25\n",
      "\tEpoch 151 | Batch 80 | Loss 205244640.00\n",
      "\tEpoch 151 | Batch 120 | Loss 28309228.00\n",
      "\tEpoch 151 | Batch 160 | Loss 42270932.00\n",
      "\tEpoch 151 | Batch 200 | Loss 45411376.00\n",
      "\tEpoch 151 | Batch 240 | Loss 16318175.00\n",
      "\tEpoch 151 | Batch 280 | Loss 11081626.00\n",
      "Epoch 151 | Loss 49128930.71\n",
      "\tEpoch 152 | Batch 0 | Loss 26948030.00\n",
      "\tEpoch 152 | Batch 40 | Loss 3605361.25\n",
      "\tEpoch 152 | Batch 80 | Loss 17622478.00\n",
      "\tEpoch 152 | Batch 120 | Loss 8116429.00\n",
      "\tEpoch 152 | Batch 160 | Loss 91914144.00\n",
      "\tEpoch 152 | Batch 200 | Loss 206204016.00\n",
      "\tEpoch 152 | Batch 240 | Loss 7348603.00\n",
      "\tEpoch 152 | Batch 280 | Loss 4371235.00\n",
      "Epoch 152 | Loss 47320001.87\n",
      "\tEpoch 153 | Batch 0 | Loss 57095420.00\n",
      "\tEpoch 153 | Batch 40 | Loss 15878919.00\n",
      "\tEpoch 153 | Batch 80 | Loss 6881338.00\n",
      "\tEpoch 153 | Batch 120 | Loss 3895401.25\n",
      "\tEpoch 153 | Batch 160 | Loss 23457400.00\n",
      "\tEpoch 153 | Batch 200 | Loss 8568718.00\n",
      "\tEpoch 153 | Batch 240 | Loss 29531728.00\n",
      "\tEpoch 153 | Batch 280 | Loss 5641843.00\n",
      "Epoch 153 | Loss 47588588.26\n",
      "\tEpoch 154 | Batch 0 | Loss 80270536.00\n",
      "\tEpoch 154 | Batch 40 | Loss 32757182.00\n",
      "\tEpoch 154 | Batch 80 | Loss 27421556.00\n",
      "\tEpoch 154 | Batch 120 | Loss 11796312.00\n",
      "\tEpoch 154 | Batch 160 | Loss 12293982.00\n",
      "\tEpoch 154 | Batch 200 | Loss 33118164.00\n",
      "\tEpoch 154 | Batch 240 | Loss 66370352.00\n",
      "\tEpoch 154 | Batch 280 | Loss 4605395.50\n",
      "Epoch 154 | Loss 46602089.37\n",
      "\tEpoch 155 | Batch 0 | Loss 331109664.00\n",
      "\tEpoch 155 | Batch 40 | Loss 17093650.00\n",
      "\tEpoch 155 | Batch 80 | Loss 18784976.00\n",
      "\tEpoch 155 | Batch 120 | Loss 3853948.75\n",
      "\tEpoch 155 | Batch 160 | Loss 27108768.00\n",
      "\tEpoch 155 | Batch 200 | Loss 5490714.00\n",
      "\tEpoch 155 | Batch 240 | Loss 12952366.00\n",
      "\tEpoch 155 | Batch 280 | Loss 10035331.00\n",
      "Epoch 155 | Loss 47885108.55\n",
      "\tEpoch 156 | Batch 0 | Loss 14140869.00\n",
      "\tEpoch 156 | Batch 40 | Loss 7337874.50\n",
      "\tEpoch 156 | Batch 80 | Loss 155890160.00\n",
      "\tEpoch 156 | Batch 120 | Loss 27766836.00\n",
      "\tEpoch 156 | Batch 160 | Loss 5230903.00\n",
      "\tEpoch 156 | Batch 200 | Loss 15893645.00\n",
      "\tEpoch 156 | Batch 240 | Loss 5268744.00\n",
      "\tEpoch 156 | Batch 280 | Loss 10035692.00\n",
      "Epoch 156 | Loss 45170825.23\n",
      "\tEpoch 157 | Batch 0 | Loss 23109712.00\n",
      "\tEpoch 157 | Batch 40 | Loss 39742088.00\n",
      "\tEpoch 157 | Batch 80 | Loss 6757964.00\n",
      "\tEpoch 157 | Batch 120 | Loss 68310072.00\n",
      "\tEpoch 157 | Batch 160 | Loss 14220384.00\n",
      "\tEpoch 157 | Batch 200 | Loss 7514970.00\n",
      "\tEpoch 157 | Batch 240 | Loss 34976556.00\n",
      "\tEpoch 157 | Batch 280 | Loss 36719988.00\n",
      "Epoch 157 | Loss 43662386.20\n",
      "\tEpoch 158 | Batch 0 | Loss 67415864.00\n",
      "\tEpoch 158 | Batch 40 | Loss 27991200.00\n",
      "\tEpoch 158 | Batch 80 | Loss 5129270.00\n",
      "\tEpoch 158 | Batch 120 | Loss 6770337.00\n",
      "\tEpoch 158 | Batch 160 | Loss 38720656.00\n",
      "\tEpoch 158 | Batch 200 | Loss 5721216.50\n",
      "\tEpoch 158 | Batch 240 | Loss 5769104.00\n",
      "\tEpoch 158 | Batch 280 | Loss 4888505.00\n",
      "Epoch 158 | Loss 48023719.72\n",
      "\tEpoch 159 | Batch 0 | Loss 6150333.00\n",
      "\tEpoch 159 | Batch 40 | Loss 6660787.00\n",
      "\tEpoch 159 | Batch 80 | Loss 6394913.00\n",
      "\tEpoch 159 | Batch 120 | Loss 6226465.00\n",
      "\tEpoch 159 | Batch 160 | Loss 126838032.00\n",
      "\tEpoch 159 | Batch 200 | Loss 9388752.00\n",
      "\tEpoch 159 | Batch 240 | Loss 3550092.25\n",
      "\tEpoch 159 | Batch 280 | Loss 5440892.50\n",
      "Epoch 159 | Loss 51866838.39\n",
      "\tEpoch 160 | Batch 0 | Loss 61678336.00\n",
      "\tEpoch 160 | Batch 40 | Loss 23179598.00\n",
      "\tEpoch 160 | Batch 80 | Loss 154191296.00\n",
      "\tEpoch 160 | Batch 120 | Loss 150014576.00\n",
      "\tEpoch 160 | Batch 160 | Loss 15828974.00\n",
      "\tEpoch 160 | Batch 200 | Loss 24102768.00\n",
      "\tEpoch 160 | Batch 240 | Loss 75897192.00\n",
      "\tEpoch 160 | Batch 280 | Loss 9675382.00\n",
      "Epoch 160 | Loss 46757223.91\n",
      "\tEpoch 161 | Batch 0 | Loss 4512672.00\n",
      "\tEpoch 161 | Batch 40 | Loss 7860755.00\n",
      "\tEpoch 161 | Batch 80 | Loss 7396599.00\n",
      "\tEpoch 161 | Batch 120 | Loss 26199284.00\n",
      "\tEpoch 161 | Batch 160 | Loss 4108799.50\n",
      "\tEpoch 161 | Batch 200 | Loss 74381488.00\n",
      "\tEpoch 161 | Batch 240 | Loss 13034303.00\n",
      "\tEpoch 161 | Batch 280 | Loss 17051430.00\n",
      "Epoch 161 | Loss 45239637.19\n",
      "\tEpoch 162 | Batch 0 | Loss 16998698.00\n",
      "\tEpoch 162 | Batch 40 | Loss 42139224.00\n",
      "\tEpoch 162 | Batch 80 | Loss 4389854.50\n",
      "\tEpoch 162 | Batch 120 | Loss 8769236.00\n",
      "\tEpoch 162 | Batch 160 | Loss 4843134.00\n",
      "\tEpoch 162 | Batch 200 | Loss 31184256.00\n",
      "\tEpoch 162 | Batch 240 | Loss 13637936.00\n",
      "\tEpoch 162 | Batch 280 | Loss 5226181.00\n",
      "Epoch 162 | Loss 44673428.51\n",
      "\tEpoch 163 | Batch 0 | Loss 4860384.00\n",
      "\tEpoch 163 | Batch 40 | Loss 8164228.00\n",
      "\tEpoch 163 | Batch 80 | Loss 82110096.00\n",
      "\tEpoch 163 | Batch 120 | Loss 144868064.00\n",
      "\tEpoch 163 | Batch 160 | Loss 22601472.00\n",
      "\tEpoch 163 | Batch 200 | Loss 8504518.00\n",
      "\tEpoch 163 | Batch 240 | Loss 18599056.00\n",
      "\tEpoch 163 | Batch 280 | Loss 202181088.00\n",
      "Epoch 163 | Loss 45385874.82\n",
      "\tEpoch 164 | Batch 0 | Loss 7186219.00\n",
      "\tEpoch 164 | Batch 40 | Loss 3424932.50\n",
      "\tEpoch 164 | Batch 80 | Loss 8659629.00\n",
      "\tEpoch 164 | Batch 120 | Loss 58628848.00\n",
      "\tEpoch 164 | Batch 160 | Loss 18485256.00\n",
      "\tEpoch 164 | Batch 200 | Loss 34757180.00\n",
      "\tEpoch 164 | Batch 240 | Loss 34561720.00\n",
      "\tEpoch 164 | Batch 280 | Loss 12235420.00\n",
      "Epoch 164 | Loss 49719245.37\n",
      "\tEpoch 165 | Batch 0 | Loss 11076412.00\n",
      "\tEpoch 165 | Batch 40 | Loss 10020318.00\n",
      "\tEpoch 165 | Batch 80 | Loss 154160336.00\n",
      "\tEpoch 165 | Batch 120 | Loss 76456872.00\n",
      "\tEpoch 165 | Batch 160 | Loss 15054710.00\n",
      "\tEpoch 165 | Batch 200 | Loss 17578276.00\n",
      "\tEpoch 165 | Batch 240 | Loss 3780669.25\n",
      "\tEpoch 165 | Batch 280 | Loss 40271508.00\n",
      "Epoch 165 | Loss 45790174.28\n",
      "\tEpoch 166 | Batch 0 | Loss 3983792.00\n",
      "\tEpoch 166 | Batch 40 | Loss 369917504.00\n",
      "\tEpoch 166 | Batch 80 | Loss 30682076.00\n",
      "\tEpoch 166 | Batch 120 | Loss 12680145.00\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, (batch_input, batch_label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_input = batch_input.to(device, torch.float32)\n",
    "        batch_label = batch_label.to(device,torch.float32)\n",
    "        output = model(batch_input)\n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
